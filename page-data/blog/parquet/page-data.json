{"componentChunkName":"component---src-templates-blog-js","path":"/blog/parquet","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Performance Benchmarks: Parquet","date":"2020-03-26"},"html":"<p>A Parquet file is a popular column-oriented storage format for Hadoop. For more information about column-oriented stores, refer to my <a href=\"/blog/columnar/\">previous post</a>. A Parquet file is used for fast analytics that often reads and writes columns, rather than rows. Originally, Parquet files were designed to be used in MapReduce problems. Meaning, most of its development went towards <a href=\"https://github.com/apache/parquet-mr\" target=\"_blank\" rel=\"nofollow\">parquet-mr</a>, which is a Java implementation.</p>\n<p>As of 2020, there has been development towards <a href=\"https://github.com/apache/parquet-cpp\" target=\"_blank\" rel=\"nofollow\">parquet-cpp</a>, which is a native C++ implementation of Parquet. Eventually, this implementation of parquet will provide native read and write support for pandas DataFrames, which will improve the performance of:</p>\n<ul>\n<li>Reading Parquet files into DataFrames</li>\n<li>Writing DataFrames to Parquet files</li>\n</ul>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#the-benefits-of-parquet\">The Benefits of Parquet</a></li>\n<li><a href=\"#compression-and-io-optimization\">Compression and I/O Optimization in Parquet</a></li>\n<li><a href=\"#the-format-of-a-parquet-file\">The Format of a Parquet File</a></li>\n<li><a href=\"#setting-up-performance-tests\">Setting Up Performance Tests</a></li>\n<li><a href=\"#performance-of-parquet-engines\">Performance of Parquet Engines</a></li>\n</ul>\n<h2>The Benefits of Parquet</h2>\n<p>The Apache Parquet project was originally initiated to create an open-standard columnar file format. In the beginning, Parquet files were only used in the Hadoop ecosystem. Today, they are used in Apache Spark and by cloud vendors to fill many data warehousing needs.</p>\n<p>A parquet file is used for storing a columnar to disk. Meaning, it focuses on data compression, which refers to reducing the size of a file. In Parquet, data compression is performed column-by-column. This enables encoding schemes to be used for different data types. As a result, parquet files are able to reduce the time for each query by reducing the overall I/O, such as reading data for each column in a compressed format.</p>\n<h2>Compression and I/O Optimization</h2>\n<p>Since an entire column is stored on blocks, compression can be optimized by deducing the exact number of bits for each data value. For example, a column of integers could be compressed into a smaller data type by inferring the maximum integer value. So, if a column consist of integers that range from 0 and 100, then the column doesn't need to be any larger than int8.</p>\n<p>I/O is optimized by focusing on projection pushdown and predicate pushdown. Here, a predicate refers to a filter with a <code class=\"language-text\">where</code> clause, and a projection refers to selected columns using a <code class=\"language-text\">select</code> clause. Projection pushdown involves column pruning. This happens automatically, since Parquet is formatted as a columnar file.</p>\n<p>In parquet, predicate pushdown involves moving any filtering to an earlier phase of query execution. Then, it maintains statistics for groups of rows to improve the performance of predicate evaluation. In summary, predicate pushdown in Parquet provides significant performance improvements. For more details about predicate pushdown in Parquet, refer to <a href=\"https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html#concept_pgs_plb_mgb\" target=\"_blank\" rel=\"nofollow\">this article</a>.</p>\n<p><img src=\"/c60617b3604322fc8d79a880f807475d/parquetpushdown.svg\" alt=\"parquetpushdown\"></p>\n<h2>The Format of a Parquet File</h2>\n<p>A Parquet file is organized into three general sections:</p>\n<ul>\n<li>Header</li>\n<li>Data Blocks</li>\n<li>Footer</li>\n</ul>\n<p>Each Parquet file has one header, one or many data blocks, and one footer. Within these components, a Parquet file stores two different types of information: metadata and data. Specifically, the metadata is stored in the header and footer, whereas the data is stored in the data blocks.</p>\n<p><img src=\"/b7c46cd4f740fd192cc36f11f030bcab/parquetlayout.svg\" alt=\"parquetgenerallayout\"></p>\n<p>In particular, the header contains metadata in the form of a 4-byte magic number in the header, which represents its file is in Parquet format. Remaining metadata about the file is stored in the footer section. It contains metadata about:</p>\n<ul>\n<li>Row groups</li>\n<li>Columns</li>\n<li>Version of its Parquet format</li>\n<li>4-byte magic number</li>\n</ul>\n<p>In a Parquet file, each data block is stored as a collection of row groups. These row groups are stored as a collection of column chunks. A row group corresponds to a set of rows, whereas a column chunk corresponds to an individual column in the dataset. The data in column chunks are organized into pages, which correspond to column values.</p>\n<p>At a high-level, the graphic below illustrates sample data formatted as a Parquet file. For more details about the layout of a Parquet file, refer to the Apache Parquet <a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" rel=\"nofollow\">documentation</a>.</p>\n<p><img src=\"/22ea2f576c7f67964c31b28fab3691f2/parquetexample.svg\" alt=\"parquetformat\"></p>\n<h2>Use of Parquet in Pandas</h2>\n<p>As of June 2020, the pandas library provides wrapper functions that use a Parquet engine for reading and writing Parquet files. These two functions are <code class=\"language-text\">pandas.read_parquet</code> and <code class=\"language-text\">pandas.to_parquet</code>. As of June 2020, there are two choices of Parquet engines used for reading in Parquet files.</p>\n<p>According to the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_parquet.html#pandas.read_parquet\" target=\"_blank\" rel=\"nofollow\">pandas documentation</a>, an engine parameter can be specified, which refers to the Parquet library to use. Its default behavior is to try ‘pyarrow’, falling back to ‘fastparquet’ if ‘pyarrow’ is unavailable.</p>\n<p>Seeing as the development towards PyArrow and parquet-cpp is still progressing, we may be interested in performance benchmarks for reading from and writing to Parquet files, while using the above functions in their current state. For a more detailed analysis of performance benchmarks, refer to <a href=\"https://wesmckinney.com/blog/python-parquet-update/\" target=\"_blank\" rel=\"nofollow\">this article</a>.</p>\n<h2>Setting Up Performance Tests</h2>\n<p>The New York City Taxi &#x26; Limousine Commission records each taxi and limousine trip in NYC. They report these trips to the public each month, and include information about pick-up and drop-off destinations and times. This data is used in many Data Engineering projects for reasons mentioned <a href=\"https://uwekorn.com/2019/08/22/why-the-nyc-trd-is-a-nice-training-dataset.html\" target=\"_blank\" rel=\"nofollow\">here</a>.</p>\n<p>This exercise will only use the dataset containing trips from January of 2019 completed in yellow taxis. More details about the commission and their datasets can be found on <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\" rel=\"nofollow\">the site</a>. This particular dataset is 687.1 MB and contains 7.6 million rows. Before performing any benchmarks, let's include any setup code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> timeit <span class=\"token keyword\">import</span> timeit\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> itr <span class=\"token operator\">=</span> <span class=\"token number\">100</span></code></pre></div>\n<p>The <code class=\"language-text\">timeit</code> library is commonly used for testing performance of code segments in Python. Specifically, it returns the total seconds taken to run a given code segment, excluding the execution of any specified setup code. The <code class=\"language-text\">iter</code> variable is included to test each segment 100 times. After running the <code class=\"language-text\">timeit()</code> function, the total seconds is divided by the number of runs, ultimately to determine average performance per test.</p>\n<h2>Performance of Parquet Engines</h2>\n<p>Now, let's test the performance of reading in the same dataset in the following formats:</p>\n<ul>\n<li>A csv file</li>\n<li>An hdf file</li>\n<li>A parquet file using the <code class=\"language-text\">fastparquet</code> engine</li>\n<li>A parquet file using the <code class=\"language-text\">pyarrow</code> engine</li>\n</ul>\n<p>Prior to executing the tests below, the HDF and Parquet files were converted to a csv file. Then, the <code class=\"language-text\">pandas.DataFrame.to_hdf()</code> and <code class=\"language-text\">pandas.DataFrame.to_parquet()</code> functions were used to store each file into their respective format. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Read csv</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"pd.read_csv('taxi.csv')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token string\">\"import pandas as pd\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">12.92</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Read hdf</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"pd.read_hdf('taxi.h5')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token string\">\"import pandas as pd\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">6.57</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Read parquet using fastparquet</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"pd.read_parquet('taxi.parquet', engine='fastparquet')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token string\">\"import pandas as pd\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">6.64</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Read parquet using pyarrow</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"pd.read_parquet('taxi.parquet', engine='pyarrow')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token string\">\"import pandas as pd\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">3.62</span></code></pre></div>\n<p>After reading in each file in the various formats, the final test delivered the best performance. Using the PyArrow Parquet engine, the taxi trips dataset, formatted as a Parquet file, only took an average of 3.62 seconds for reading in the entire dataset.</p>\n<p>Now, let's test the performance of writing to similar files.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Write hdf</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"df.to_hdf('taxi.h5', key='df')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"\n... import pandas as pd\n... df = pd.read_csv('taxi.csv')\n... \"\"\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">6.18</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Write parquet using fastparquet</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"df.to_parquet('taxi.parquet', engine='fastparquet')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"\n... import pandas as pd\n... df = pd.read_csv('taxi.csv')\n... \"\"\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">9.12</span>\n\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Write parquet using pyarrow</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> stmnt <span class=\"token operator\">=</span> <span class=\"token string\">\"df.to_parquet('taxi.parquet', engine='pyarrow')\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> setup <span class=\"token operator\">=</span> <span class=\"token triple-quoted-string string\">\"\"\"\n... import pandas as pd\n... df = pd.read_csv('taxi.csv')\n... \"\"\"</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> s <span class=\"token operator\">=</span> timeit<span class=\"token punctuation\">(</span>stmnt<span class=\"token punctuation\">,</span> setup<span class=\"token punctuation\">,</span> number<span class=\"token operator\">=</span>itr<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">round</span><span class=\"token punctuation\">(</span>s<span class=\"token operator\">/</span>itr<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n<span class=\"token number\">4.96</span></code></pre></div>\n<p>This example was run locally on my laptop without testing other types of datasets using various styles of compression styles, such as uncompressed, snappy, and gzip. Thus, the performance benchmarks should not be taken precisely. Regardless, there clearly seems to be a huge performance boost when using the PyArrow engine. To learn more about the other use cases for PyArrow, refer to <a href=\"/blog/pyarrow/\">my next post</a>.</p>"}},"pageContext":{"slug":"parquet"}},"staticQueryHashes":["2961437231","3159585216"]}