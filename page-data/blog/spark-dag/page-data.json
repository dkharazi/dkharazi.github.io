{"componentChunkName":"component---src-templates-blog-js","path":"/blog/spark-dag","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Visualizing DAGs in Spark","date":"2019-02-27"},"html":"<p>The goal of this post is to provide a general introduction to the <code class=\"language-text\">RDD</code> API. Each example has a snippet of PySpark code with explanations. Another goal is to provide a general introduction to Spark's web UI. Certain examples have <code class=\"language-text\">DAG</code> visualizations for jobs and stages. Spark starts a web UI for each <code class=\"language-text\">SparkContext</code> that is initialized.</p>\n<p>This will only include rudimentary examples of methods in the <code class=\"language-text\">RDD</code> API. For more detailed illustrations and explanations of these concepts, refer to this <a href=\"https://stackoverflow.com/a/37529233/12777044\" target=\"_blank\" rel=\"nofollow\">post</a> and this <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\" target=\"_blank\" rel=\"nofollow\">article</a>. In the coming posts, we'll dive deeper into more generic objects in the Spark API. Then, we'll explore low-level concepts, including the Spark internals.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#setting-up-a-sparksession\">Setting up a SparkSession</a></li>\n<li><a href=\"#using-an-rdd-with-python\">Using an RDD with Python</a></li>\n<li><a href=\"#counting-words-from-files\">Counting Words from Files</a></li>\n<li><a href=\"#visualizing-the-dag\">Visualizing the DAG</a></li>\n</ul>\n<h2>Setting up a SparkSession</h2>\n<p>Before walking through any code examples, we need to create a <code class=\"language-text\">SparkSession</code>. Each <code class=\"language-text\">SparkSession</code> acts as an entry point into Spark programming with <code class=\"language-text\">RDDs</code>. After executing the setup code, we'll be able to use the session in our examples below.</p>\n<p>The <code class=\"language-text\">builder</code> object of a SparkSession provides access to the associated application name, associated master URL, and configuration options. It also provides access to a <code class=\"language-text\">getOrCreate</code> method, which initializes a SparkSession after setting options in the <code class=\"language-text\">builder</code> object.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> pyspark<span class=\"token punctuation\">.</span>sql <span class=\"token keyword\">import</span> SparkSession\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> spark <span class=\"token operator\">=</span> SparkSession \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>         <span class=\"token punctuation\">.</span>builder \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>         <span class=\"token punctuation\">.</span>appName<span class=\"token punctuation\">(</span><span class=\"token string\">'TxtRdr'</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>         <span class=\"token punctuation\">.</span>getOrCreate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> sc <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sparkContext</code></pre></div>\n<p>Some examples use a set of files named <code class=\"language-text\">hello.txt</code> and <code class=\"language-text\">order.txt</code>. A SparkContext will read these files using the <code class=\"language-text\">textFile</code> method. After creating an <code class=\"language-text\">RDD</code>, we'll call methods to transform and filter the data in the files listed below.</p>\n<p><code class=\"language-text\">hello.txt</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Hello world\nThis is a file</code></pre></div>\n<p><code class=\"language-text\">order.txt</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Hello friend\nThis file is my order\nBurger, fries, soda</code></pre></div>\n<p>Notice, there are two similar words found in both files. Specifically, these words are <em>Hello</em> and <em>file</em>. In the upcoming examples, we'll run some standard Python code and Spark code to find these words. Then, we'll count the number of each word using Spark.</p>\n<h2>Using an <code class=\"language-text\">RDD</code> with Python</h2>\n<p>Before performing any transformations, the data needs to be read using the <code class=\"language-text\">textFile</code> method. Specifically, the SparkContext will read the <code class=\"language-text\">hello.txt</code> and <code class=\"language-text\">orders.txt</code> files using this method, which returns an <code class=\"language-text\">RDD</code> object. After calling this method, the returned <code class=\"language-text\">RDD</code> essentially contains a list of strings, where each string represents a line from the file.</p>\n<p>The <code class=\"language-text\">flatMap</code> method maps a function that is defined using the lambda keyword to our <code class=\"language-text\">RDD</code>. In this example, our function separates each line into individual strings based on any spaces. Although it isn't used here, the <code class=\"language-text\">map</code> method is very similar to the <code class=\"language-text\">flatMap</code> method. Specifically, the <code class=\"language-text\">map</code> method performs an extra step, which involves storing these individual strings into a list for each line.</p>\n<p>Ultimately, the <code class=\"language-text\">collect</code> method is called, which returns all the elements from the <code class=\"language-text\">RDD</code> as a list to the driver program. This method is an action. In other words, the transformations prior to the <code class=\"language-text\">collect</code> method aren't computed until the <code class=\"language-text\">collect</code> method is called. The code snippet below creates an <code class=\"language-text\">RDD</code> and performs the described operations on the <code class=\"language-text\">hello.txt</code> file.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> hello <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'hello.txt'</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>           <span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> c<span class=\"token punctuation\">:</span> c<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>           <span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>For this example, there's still a second file with words that should be counted by the same Spark program.\nThrough a similar process as before, the <code class=\"language-text\">collect</code> method is called for reading in the <code class=\"language-text\">order.txt</code> file. Additionally, the other methods are called for function mapping and data collection.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> order <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'order.txt'</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>           <span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> c<span class=\"token punctuation\">:</span> c<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>           <span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>After executing these two code snippets, the resulting <code class=\"language-text\">RDD</code> includes a long list of each word in both files. Since the <code class=\"language-text\">collect</code> method outputs a list from the <code class=\"language-text\">RDD</code>, we can perform standard Python functions on it. As an example, we can use Python to find the common words between the two <code class=\"language-text\">RDDs</code>.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>hello<span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;</span> <span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>order<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">[</span><span class=\"token string\">'Hello'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'file'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<h2>Counting Words from Files</h2>\n<p>Now, we may prefer to compute some of these operations in Spark. By performing filters in CPython, we will lose the benefit of distributed computation in Spark. If <code class=\"language-text\">hello.txt</code> and <code class=\"language-text\">orders.txt</code> are  much larger files, or if we instead count the words of thousands of files, filtering in Spark becomes much faster compared to filtering in ordinary CPython on a single machine.</p>\n<p>To illustrate this point, let's go through an example that is very similar to the previous one, but ultimately performs counting and filtering.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> hello <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'hello.txt'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> order <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'order.txt'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> counts <span class=\"token operator\">=</span> hello<span class=\"token punctuation\">.</span>union<span class=\"token punctuation\">(</span>order<span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>               <span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> w<span class=\"token punctuation\">:</span> w<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">' '</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>               <span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> c<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">(</span>c<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>               <span class=\"token punctuation\">.</span>reduceByKey<span class=\"token punctuation\">(</span>add<span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>               <span class=\"token punctuation\">.</span><span class=\"token builtin\">filter</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> t<span class=\"token punctuation\">:</span> t<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">></span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span> \\\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>               <span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Again, calling the <code class=\"language-text\">flatMap</code> method returns an <code class=\"language-text\">RDD</code> containing individual words as strings. The <code class=\"language-text\">map</code> method converts each string into a tuple containing words and their frequency. Then, the <code class=\"language-text\">reduceByKey</code> operation loops through each key-value pair and adds up the values for any repeated keys. Lastly, the <code class=\"language-text\">filter</code> method will only include key-value pairs, where the value is greater than one.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> counts\n<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Hello'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">'file'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>As expected, the program outputs two key-value pairs. The first key-value pair is <code class=\"language-text\">(&#39;Hello&#39;, 2)</code>, since there are two of these keys in both files. Similarly, the second key-value pair is <code class=\"language-text\">(&#39;file&#39;, 2)</code>, since there are two of these keys in both files.</p>\n<h2>Visualizing the DAG</h2>\n<p>The number of jobs for an application equals the number of <code class=\"language-text\">RDD</code> actions. In our example, we can see there is a single job. This is because there is only one action, which is <code class=\"language-text\">collect</code>. Instead of having a single job, we would have two jobs with this code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> hello <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'hello.txt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> order <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'order.txt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The number of additional stages equals the number of wide transformations in an application. In our example, we can see there are two stages in total, but only a single additional stage. This is because there is only one wide transformation, which is <code class=\"language-text\">reduceByKey</code>. Notice, the web UI in Spark provides a nice visualization of the <code class=\"language-text\">DAG</code>.</p>\n<p><img src=\"/fc9c48f25fb6bb781bc41b4773ca7fa5/pysparkwordcounts.svg\" alt=\"wordcounts\"></p>\n<p>Recall, the number of tasks within a stage equals the number of partitions in an <code class=\"language-text\">RDD</code>. By default, Spark assigns the number of paritions to be two. Meaning, the default number of tasks per stage is two. The number of partitions is assigned to an <code class=\"language-text\">RDD</code> when it is initialized. Thus, this parameter can be adjusted when calling <code class=\"language-text\">textFile(file, num_partitions)</code>. Therefore, our stage will have six tasks (instead of four) if we change this line of code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> hello <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">'hello.txt'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Since Spark assigns the number of paritions of an <code class=\"language-text\">RDD</code> during initialization, the number of tasks are determined after shuffling as well. This is because Spark creates a new <code class=\"language-text\">RDD</code> after shuffling. This object is called a <code class=\"language-text\">ShuffleRDD</code>. For a more detailed description of how tasks are separated and organized in Spark, refer to this <a href=\"https://stackoverflow.com/a/37759913/12777044\" target=\"_blank\" rel=\"nofollow\">post</a>.</p>"}},"pageContext":{"slug":"spark-dag"}},"staticQueryHashes":["2961437231","3159585216"]}