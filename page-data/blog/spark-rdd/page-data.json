{"componentChunkName":"component---src-templates-blog-js","path":"/blog/spark-rdd","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Spark RDD Fundamentals","date":"2019-02-15"},"html":"<p>This post provides a high-level introduction to the RDD object in the Spark API. In the coming posts, we'll dive deeper into more generic objects in the Spark API. Then, we'll explore low-level concepts, including the Spark internals.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#resilient-distributed-datasets\">Resilient Distributed Datasets</a></li>\n<li><a href=\"#defining-a-dagscheduler\">Defining a DAGScheduler</a></li>\n<li><a href=\"#types-of-transformations\">Types of Transformations</a></li>\n<li><a href=\"#lifecycle-of-a-spark-program\">Lifecycle of a Spark Program</a></li>\n</ul>\n<h2>Resilient Distributed Datasets</h2>\n<p>Most likely, we've all worked with pandas <code class=\"language-text\">DataFrames</code> before. They're in-memory, <em>single-server</em> data structures that offer many user-friendly functions for data processing. Functionally, Spark provides a data structure that is very similar in this sense, but can be used across multiple servers. This data structure is called a <em>resilient distributed dataset</em>, or an <code class=\"language-text\">RDD</code> for short. In short, an <code class=\"language-text\">RDD</code> is an in-memory data structure that is distributed across many servers within a Spark cluster.</p>\n<p>Roughly, we can think of an <code class=\"language-text\">RDD</code> as a distributed version of a pandas <code class=\"language-text\">DataFrame</code>. I'm making this comparison because RDDs offer many pandas-like functions that are focused around data processing. These functions are called <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\" target=\"_blank\" rel=\"nofollow\">Transformations</a> and <a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\" target=\"_blank\" rel=\"nofollow\">Actions</a>. Specifically, transformations create a new dataset from an existing one. Contrastingly, actions return non-dataset values, which generally relate to some aggregation.</p>\n<p><img src=\"/b6e026fce430f4b5b89e1bab73bac406/sparkaction.svg\" alt=\"diagram of rdd to transf to action\"></p>\n<p>Transformations are <em>lazy</em>. Meaning, an <code class=\"language-text\">RDD</code> isn't computed until it receives an action. Actions always return values to the driver program. Spark receives a performance boost from any lazy evaluations. However, this could become a problem if users continuously recompute that same transformation. As a result, Spark allows us to persist an <code class=\"language-text\">RDD</code> to memory using the <code class=\"language-text\">persist</code> method. To summarize, transformations and actions have the following properties:</p>\n<ul>\n<li>Transformations are lazy by default</li>\n<li>Actions aren't lazy</li>\n<li>Transformations return a new <code class=\"language-text\">RDD</code></li>\n<li>Actions return an aggregated value of the <code class=\"language-text\">RDD</code></li>\n</ul>\n<p>As stated previously, an <code class=\"language-text\">RDD</code> receives a huge boost in performance by keeping data in memory. However, Spark supports data persistance to disk as well. Spark also supports data persistence to databases. To summarize, an <code class=\"language-text\">RDD</code> has the following properties:</p>\n<ul>\n<li>Distributed</li>\n<li>Fault-tolerant</li>\n<li>Flexible functions such as <code class=\"language-text\">map(func)</code></li>\n<li>Optionally in-memory on the Driver's JVM</li>\n<li>Parallelizable using <code class=\"language-text\">sc.parallelize(data)</code></li>\n<li>Immutable (more on this later)</li>\n</ul>\n<h2>Defining a <code class=\"language-text\">DAGScheduler</code></h2>\n<p>Recall, a transformation is a type of special <code class=\"language-text\">RDD</code> method that returns another <code class=\"language-text\">RDD</code> object. Again, these methods aren't computed until the <code class=\"language-text\">RDD</code> receives an action, indicating that <code class=\"language-text\">RDD</code> objects are immutable. Since <code class=\"language-text\">RDD</code> objects can't <em>change</em> once they are created, Spark creates a new object called a <code class=\"language-text\">DAG</code> when an action is called. In a <code class=\"language-text\">DAG</code>, each node is an <code class=\"language-text\">RDD</code> partition, and an edge is a transformation.</p>\n<p>Spark breaks the <code class=\"language-text\">RDD</code> into smaller chunks of data called <em>partitions</em>. In Spark, a partition is a chunk of data on a node within the cluster. At a high level, Spark breaks transformations and actions into <code class=\"language-text\">Tasks</code>, which are mapped to partitions. Essentially, a <code class=\"language-text\">Task</code> represents a unit of work on a partition of a distributed dataset.</p>\n<p>Assuming nonsequential dependence, <code class=\"language-text\">Tasks</code> are executed in parallel on partitions. Thus, the number of partitions made up by an <code class=\"language-text\">RDD</code> should equal the number of CPU cores within a cluster. Theoretically, increasing the number of partitions would increase the amount of parallelism for a system, assuming there are available CPU cores. If an <code class=\"language-text\">RDD</code> can't fit an entire set of data into memory, then that data will be stored to and read from disk.</p>\n<p>Now, let's return to our previous discussion about the <code class=\"language-text\">DAG</code> object. When an action is called, each <code class=\"language-text\">DAG</code> is submitted to a <code class=\"language-text\">DAGScheduler</code> object for execution. A <code class=\"language-text\">DAGScheduler</code> organizes operations into <code class=\"language-text\">Stages</code>, and a <code class=\"language-text\">Stage</code> is organized into <code class=\"language-text\">Tasks</code>. Each <code class=\"language-text\">Task</code> is scheduled separately. It represents a unit of work on a partition of an <code class=\"language-text\">RDD</code>, and is executed as a thread in an executor's JVM. The <code class=\"language-text\">DAGScheduler</code> returns a <code class=\"language-text\">TaskSet</code> object, which is passed to a <code class=\"language-text\">TaskScheduler</code>. The <code class=\"language-text\">TaskScheduler</code> launches tasks in the a cluster manager.</p>\n<p><img src=\"/28e6454b64974a357969e1e53fbe616c/sparktasks.svg\" alt=\"SparkTaskLifecycle\"></p>\n<p>Multiple tasks can be executed in parallel for any stage. Specifically, any two stages can be executed in parallel if they aren't sequentially dependent on each other. Implying, tasks from one stage can be executed in parallel with tasks from a separate stage, if they aren't sequentially dependent on each other. Refer to <a href=\"https://stackoverflow.com/a/41340858/12777044\" target=\"_blank\" rel=\"nofollow\">this post</a> for an illustration of how <code class=\"language-text\">Tasks</code> and stages run in parallel.</p>\n<p>The number of tasks is equal to the number of partitions. The number of stages is equal to the number of wide transformations. For examples that may help illustrate these concepts, refer to my <a href=\"/blog/spark-dag/\">next post</a>.</p>\n<h2>Types of Transformations</h2>\n<p>There are two types of transformations that can be applied to <code class=\"language-text\">RDDs</code>: narrow transformations and wide transformations. Narrow transformations refer to transformations where each partition contributes to one stage only. These include transformations like <code class=\"language-text\">map</code>, <code class=\"language-text\">filter</code>, etc. Wide transformations refer to transformations where each partition contributes to many stages. In Spark, this concept is called <em>shuffling</em>.</p>\n<p>Shuffling is used for regrouping data between partitions. Shuffling is necessary for situations requiring information from each partition. Wider transformations are more expensive than narrow transformations in comparison. For example, the <code class=\"language-text\">map</code> transformation doesn't require shuffling, since it applies element-wise transformations to each partition. This technique is called pipelining. In other words, an element in one partition doesn't need any information from other partitions. On the other hand, the <code class=\"language-text\">groupByKey</code> wide transformation needs information from each partition. Specifically, a narrow transformation keeps its results in memory, whereas a wide transformation writes its results to disk. This <a href=\"https://0x0fff.com/spark-architecture-shuffle/\" target=\"_blank\" rel=\"nofollow\">post</a> defines optimized shuffling algorithms in detail.</p>\n<p><img src=\"/dc7f75a874f8b22bdacf7fbcffe7758e/sparktransformation.svg\" alt=\"SparkNarrowAndWideTransformation\"></p>\n<h2>Lifecycle of a Spark Program</h2>\n<p>Now, we have a high-level level understanding of the core Spark data structures. This <a href=\"https://www.youtube.com/watch?v=7ooZ4S7Ay6Y\" target=\"_blank\" rel=\"nofollow\">lecture</a> defines the lifecycle of a Spark program in detail.  Generally, a common lifecycle of a spark program looks like the following:</p>\n<ol>\n<li>Create some input RDDs from external data</li>\n<li>Lazily transform them to define new RDDs using transformations</li>\n<li>Ask Spark to <code class=\"language-text\">cache()</code> any intermediate RDDs that will be reused</li>\n<li>Launch actions to start parallel computation</li>\n<li>Spark optimizes and executes its computations</li>\n</ol>"}},"pageContext":{"slug":"spark-rdd"}},"staticQueryHashes":["2961437231","3159585216"]}