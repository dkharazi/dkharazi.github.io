{"componentChunkName":"component---src-templates-blog-js","path":"/blog/spark-standalone","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Testing Spark Applications in Standalone","date":"2019-06-06"},"html":"<p>This post walks through an example of a cluster running in standalone mode. In the coming posts, we'll explore other examples, including clusters running a <a href=\"/blog/spark-yarn/\">YARN</a> cluster manager and <a href=\"/blog/spark-mesos/\">Mesos</a> cluster manager.</p>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#setting-up-a-sparksession\">Setting up a SparkSession</a></li>\n<li><a href=\"#launching-daemons\">Launching Daemons</a></li>\n<li><a href=\"#accessing-web-ui-for-daemons\">Accessing Web UI for Daemons</a></li>\n<li><a href=\"#caveat-about-pyspark-applications\">Caveat about PySpark Applications</a></li>\n<li><a href=\"#launching-applications-in-client-mode\">Launching Applications in Client Mode</a></li>\n<li><a href=\"#launching-applications-in-cluster-mode\">Launching Applications in Cluster Mode</a></li>\n</ul>\n<h2>Setting up a SparkSession</h2>\n<ol>\n<li>Download <a href=\"https://apache.claz.org/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\" target=\"_blank\" rel=\"nofollow\">Spark 2.4.6</a></li>\n<li>Create the file <code class=\"language-text\">./conf/spark-defaults.conf</code>:</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">spark.master=spark://localhost:7077\nspark.eventLog.enabled=true\nspark.eventLog.dir=./tmp/spark-events/\nspark.history.fs.logDirectory=./tmp/spark-events/\nspark.driver.memory=5g</code></pre></div>\n<ol start=\"3\">\n<li>Create a Spark application:</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># test.py</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">from</span> pyspark <span class=\"token keyword\">import</span> SparkContext\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token builtin\">file</span> <span class=\"token operator\">=</span> <span class=\"token string\">\"~/data.txt\"</span>  <span class=\"token comment\"># path of data</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> masterurl <span class=\"token operator\">=</span> <span class=\"token string\">'spark://localhost:7077'</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> sc <span class=\"token operator\">=</span> SparkContext<span class=\"token punctuation\">(</span>masterurl<span class=\"token punctuation\">,</span> <span class=\"token string\">'myapp'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> data <span class=\"token operator\">=</span> sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> num_a <span class=\"token operator\">=</span> data<span class=\"token punctuation\">.</span><span class=\"token builtin\">filter</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> s<span class=\"token punctuation\">:</span> <span class=\"token string\">'a'</span> <span class=\"token keyword\">in</span> s<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>num_a<span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> sc<span class=\"token punctuation\">.</span>stop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Launching Daemons</h2>\n<ol>\n<li>\n<p>Start a master daemon in standalone mode</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./sbin/start-master.sh </code></pre></div>\n</li>\n<li>\n<p>Start a worker daemon</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./sbin/start-slave.sh spark://localhost:7077</code></pre></div>\n</li>\n<li>\n<p>Start a history daemon</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./sbin/start-history-server.sh</code></pre></div>\n</li>\n<li>\n<p>Start a Spark application</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./bin/spark-submit <span class=\"token punctuation\">\\</span>\n--master spark://localhost:7077 <span class=\"token punctuation\">\\</span>\ntest.py</code></pre></div>\n</li>\n<li>\n<p>Stopping the daemons</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./sbin/stop-master.sh\n$ ./sbin/stop-slave.sh\n$ ./sbin/stop-history-server.sh </code></pre></div>\n</li>\n</ol>\n<h2>Accessing Web UI for Daemons</h2>\n<p>Spark provides a web UI for each initialized daemon. By default, Spark creates a web UI for the master on port <code class=\"language-text\">8080</code>. The workers can take on different portsand can be accessed via the master web UI. The history server can be accessed on port <code class=\"language-text\">18080</code> by default. The table below summarizes the default locations for each web UI.</p>\n<table>\n<thead>\n<tr>\n<th>Daemon</th>\n<th>Port</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Master</td>\n<td><code class=\"language-text\">8080</code></td>\n</tr>\n<tr>\n<td>Worker</td>\n<td><code class=\"language-text\">8081</code></td>\n</tr>\n<tr>\n<td>History</td>\n<td><code class=\"language-text\">18080</code></td>\n</tr>\n</tbody>\n</table>\n<h2>Caveat about PySpark Applications</h2>\n<p>Notice, launching an application in client mode doesn't seem to trigger a driver according to the master's web UI. This doesn't mean a driver isn't launched in client mode. The driver is still launched within the spark-submit process. However, the master's web UI omits driver information if the application is running in client mode.</p>\n<p>So, we may want to launch an application in cluster mode now. However, running an application in cluster mode would give us the following error:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token operator\">$ </span>./bin/spark-submit <span class=\"token punctuation\">\\</span>\n    --master spark://localhost:7077 <span class=\"token punctuation\">\\</span>\n    --deploy-mode cluster\n    test.py\nException <span class=\"token keyword\">in</span> thread <span class=\"token string\">\"main\"</span> org.apache.spark.SparkException: Cluster deploy mode is currently not supported <span class=\"token keyword\">for</span> python applications on standalone clusters.</code></pre></div>\n<p>As of Spark 2.4.6, we can't run python applications in cluster mode when running a standalone cluster manager. This is a good opportunity for us to experiment with other resource managers in the <a href=\"\">next post</a>. For now, we will run <code class=\"language-text\">JavaSparkPi.java</code> found in the examples directory.</p>\n<h2>Launching Applications in Client Mode</h2>\n<p>In a <a href=\"\">previous post</a>, we defined the components associated with a driver program and cluster, while illustrating the interaction between a driver program and cluster components. Specifically, we defined this interaction when applications are launched in client mode. Now, we can execute an application and verify these steps using the logs.</p>\n<p>Note, the timestamps and logged messages were slightly modified for clarification. However, the order and substance of each message still remains the same.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">14:43:01 INFO\nSparkContext: Submitted application: Spark Pi\n\n14:43:02 INFO\nUtils: Successfully started service &#39;sparkDriver&#39;\n\n14:43:03 INFO\nStandaloneAppClient: Connecting to master\n\n14:43:04 INFO\nStandaloneSchedulerBackend: Connected to Spark cluster\n\n14:43:05 INFO\nMaster: Registered app Spark Pi\n\n14:43:06 INFO\nMaster: Launching executor on worker\n\n14:43:07 INFO\nWorker: Asked to launch executor\n\n14:43:08 INFO\nExecutorRunner: Launched\n\n14:43:09 INFO\nStandaloneAppClient: Executor added on worker\n\n14:43:10 INFO\nStandaloneSchedulerBackend: Granted executor ID\n\n14:43:11 INFO\nStandaloneAppClient: Executor is now RUNNING\n\n14:43:12 INFO\nSparkContext: Starting job\n\n...\n\n14:43:13 INFO\nDAGScheduler: Job finished\n\n14:43:14 INFO\nStandaloneSchedulerBackend: Shutting down all executors\n\n14:43:15 INFO\nWorker: Asked to kill executor\n\n14:43:16 INFO\nExecutorRunner: Killing process!\n\n14:43:17 INFO\nMaster: Removing app\n\n14:43:18 INFO\nSparkContext: Successfully stopped SparkContext</code></pre></div>\n<h2>Launching Applications in Cluster Mode</h2>\n<p>In a <a href=\"\">previous post</a>, we defined the interaction between a driver program and cluster components, while applications are launched in cluster mode. Now, we can execute an application in cluster mode to verify these steps using the logs.</p>\n<p>Note, the timestamps and logged messages were slightly modified for clarification. However, the order and substance of each message still remains the same.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">14:43:01 INFO\nMaster: Driver submitted\n\n14:43:02 INFO\nMaster: Launching driver\n\n14:43:03 INFO\nWorker: Asked to launch driver\n\n14:43:04 INFO\nDriverRunner: Launched\n\n14:43:05 INFO\nUtils: Successfully started service &#39;driverClient&#39;\n\n14:43:06 INFO\nClientEndpoint: Driver successfully submitted\n\n14:43:07 INFO\nSparkContext: Submitted application: Spark Pi\n\n14:43:08 INFO\nUtils: Successfully started service &#39;sparkDriver&#39;\n\n14:43:11 INFO\nStandaloneAppClient: Connecting to master\n\n14:43:10 INFO\nStandaloneSchedulerBackend: Connected to Spark cluster\n\n14:43:12 INFO\nMaster: Registered app Spark Pi\n\n14:43:13 INFO\nMaster: Launching executor on worker\n\n14:43:14 INFO\nWorker: Asked to launch executor\n\n14:43:15 INFO\nExecutorRunner: Launched\n\n14:43:16 INFO\nStandaloneAppClient: Executor added on worker\n\n14:43:17 INFO\nStandaloneSchedulerBackend: Granted executor ID\n\n14:43:18 INFO\nStandaloneAppClient: Executor is now RUNNING\n\n14:43:19 INFO\nSparkContext: Starting job\n\n...\n\n14:43:20 INFO\nDAGScheduler: Job finished\n\n14:43:21 INFO\nStandaloneSchedulerBackend: Shutting down all executors\n\n14:43:22 INFO\nWorker: Asked to kill executor\n\n14:43:23 INFO\nExecutorRunner: Killing process!\n\n14:43:24 INFO\nWorker: Driver exited successfully\n\n14:43:25 INFO\nMaster: Removing app\n\n14:43:26 INFO\nSparkContext: Successfully stopped SparkContext</code></pre></div>"}},"pageContext":{"slug":"spark-standalone"}},"staticQueryHashes":["2961437231","3159585216"]}