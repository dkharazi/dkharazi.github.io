{"componentChunkName":"component---src-templates-entry-js","path":"/notes/de/azure/datafactory","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Azure Data Factory"},"html":"<h3>Loading and Moving Internal Data using Azure Data Factory</h3>\n<ol>\n<li>Locate the SQL Data Warehouse instance in the Azure portal</li>\n<li>Select Load Data under the Common Tasks tab</li>\n<li>Select Azure Data Factory</li>\n<li>Create a Data Factory</li>\n<li>\n<p>Specify the following details:</p>\n<ul>\n<li>Data Factory name</li>\n<li>Subscription</li>\n<li>Select resource group</li>\n<li>Select region</li>\n<li>Select load data</li>\n</ul>\n</li>\n<li>\n<p>Specify the following configurations:</p>\n<ul>\n<li>Task name</li>\n<li>Task description</li>\n<li>Task cadence</li>\n<li>Expiration time</li>\n</ul>\n</li>\n<li>Select the data source</li>\n<li>Select the destination</li>\n</ol>\n<h3>Loading and Moving External Data using Polybase</h3>\n<ol>\n<li>Create the following queries for the database within either Azure SQL Data Warehouse or Azure Data Studio</li>\n<li>Create an external Hadoop data source</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> EXTERNAL <span class=\"token keyword\">DATA</span> SOURCE LabAzureStorage\n<span class=\"token keyword\">WITH</span>\n<span class=\"token punctuation\">(</span>\n\t<span class=\"token keyword\">TYPE</span> <span class=\"token operator\">=</span> Hadoop<span class=\"token punctuation\">,</span>\n\tLOCATION <span class=\"token operator\">=</span> <span class=\"token string\">'wasbs://labdata@&lt;Name_Of_Storage_Account>.blob.core.windows.net/'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<ol start=\"3\">\n<li>Define the external file format</li>\n</ol>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> EXTERNAL <span class=\"token keyword\">FILE</span> FORMAT TextFileFormat\n<span class=\"token keyword\">WITH</span>\n<span class=\"token punctuation\">(</span>\nFORMAT_TYPE <span class=\"token operator\">=</span> DELIMITEDTEXT<span class=\"token punctuation\">,</span>\n\tFORMAT_OPTIONS <span class=\"token punctuation\">(</span>\n\t\tFIELD_TERMINATOR <span class=\"token operator\">=</span> <span class=\"token string\">','</span><span class=\"token punctuation\">,</span>\n\t\tSTRING_DELIMITER <span class=\"token operator\">=</span> <span class=\"token string\">''</span><span class=\"token punctuation\">,</span>\n\t\tDATE_FORMAT <span class=\"token operator\">=</span> <span class=\"token string\">'yyyy-MM-dd HH:mm:ss.fff'</span><span class=\"token punctuation\">,</span>\n\t\tUSE_TYPE_DEFAULT <span class=\"token operator\">=</span> <span class=\"token boolean\">FALSE</span>\n\t<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h3>Integrating Data Factory with Databricks</h3>\n<ol>\n<li>Create an Azure storage account</li>\n<li>Create a Data Factory instance</li>\n<li>\n<p>Create a data workflow pipeline</p>\n<ul>\n<li>This involves copying data from our source by using a copy activity in Data Factory</li>\n<li>A copy activity allows us to copy data from different on-premises and cloud services</li>\n</ul>\n</li>\n<li>Add a Databricks notebook to the pipeline</li>\n<li>Analyze the data</li>\n</ol>\n<h3>Defining Best Practices</h3>\n<ul>\n<li>We should pause the SQL Data Warehouse instance when we don't need to run any queries if we want to save in compute costs</li>\n<li>Saving data in a format like Parquet is the recommended way to save data if we plan to run several queries against one SQL Data Warehouse Table, since each query can extract a large amount of data to Blob storage</li>\n<li>Linked services define the connection information needed for Data Factory to connect to external resources</li>\n<li>In Azure Databricks, a target cluster will start automatically if the cluster isn't already running by Data Factory</li>\n<li>We can connect our Spark cluster in Databricks to Azure Blob storage by mounting the cluster</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/sql-data-warehouse/design-elt-data-loading\" target=\"_blank\" rel=\"nofollow\">Documentation for Azure SQL Data Warehouse</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver15\" target=\"_blank\" rel=\"nofollow\">Documentation for Polybase</a></li>\n<li><a href=\"https://cloudblogs.microsoft.com/sqlserver/2014/07/30/transitioning-from-smp-to-mpp-the-why-and-the-how/\" target=\"_blank\" rel=\"nofollow\">Article Describing SQL Server</a></li>\n<li><a href=\"https://www.flydata.com/blog/introduction-to-massively-parallel-processing/\" target=\"_blank\" rel=\"nofollow\">Describing Data Warhousing and Parallel Processing</a></li>\n<li><a href=\"https://hevodata.com/blog/redshift-architecture/\" target=\"_blank\" rel=\"nofollow\">Defining the Redshift Architecture</a></li>\n<li><a href=\"https://0x0fff.com/hadoop-vs-mpp/\" target=\"_blank\" rel=\"nofollow\">Article about Hadoop and Data Warehousing</a></li>\n<li><a href=\"https://stackoverflow.com/questions/21900185/what-are-oltp-and-olap-what-is-the-difference-between-them\" target=\"_blank\" rel=\"nofollow\">Comparing OLTP and OLAP</a></li>\n</ul>"}},"pageContext":{"slug":"de/azure/datafactory","previousSlug":"de/azure/sqldb","nextSlug":null,"previousTitle":"Azure SQL Data Warehouse","nextTitle":null}},"staticQueryHashes":[]}