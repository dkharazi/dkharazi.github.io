{"componentChunkName":"component---src-templates-entry-js","path":"/notes/de/etl/spark","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Spark"},"html":"<h3>Describing Apache Spark</h3>\n<ul>\n<li>Spark is a distributed computing engine</li>\n<li>Its performance boost mostly comes from Spark storing data in memory</li>\n<li>Spark is run on server-side clusters</li>\n<li>\n<p>It provides high-level APIs in:</p>\n<ul>\n<li>Java</li>\n<li>Scala</li>\n<li>Python</li>\n<li>R</li>\n</ul>\n</li>\n<li>We are able to run Spark on server-side clusters</li>\n</ul>\n<h3>Describing Higher-Level APIs in Spark</h3>\n<ul>\n<li>\n<p>Spark also supports higher-level APIs:</p>\n<ul>\n<li>Spark SQL</li>\n<li>MLlib</li>\n<li>GraphX</li>\n<li>Spark Streaming</li>\n</ul>\n</li>\n<li>Spark SQL is used for SQL and structured data processing</li>\n<li>MLlib is used for machine learning</li>\n<li>GraphX is used for graph processing</li>\n<li>Spark Streaming is used for streaming data</li>\n</ul>\n<h3>Describing Features of Spark SQL</h3>\n<ul>\n<li>Integrates querying inside Spark programs</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Mix SQL queries with DataFrame API</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> results <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">'SELECT * FROM people'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> names <span class=\"token operator\">=</span> results<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> p<span class=\"token punctuation\">:</span> p<span class=\"token punctuation\">.</span>name<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>Connects to any data source the same way</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token comment\"># Uniform data access with DataFrame API</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">(</span><span class=\"token string\">'s3n://...'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>registerTempTable<span class=\"token punctuation\">(</span><span class=\"token string\">'json'</span><span class=\"token punctuation\">)</span>\n<span class=\"token operator\">>></span><span class=\"token operator\">></span> results <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">'SELECT * FROM people JOIN json'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>Integrates with Hive</li>\n</ul>\n<p><img src=\"/a6ffda7cd5b17da6b519913f31b11dc0/sparksqlhive.png\" alt=\"sparksql_hive\"></p>\n<ul>\n<li>Connects through JDBC or ODBC</li>\n</ul>\n<p><img src=\"/1a04c28db261aff824ca542ffafb78ee/sparksqlconnect.png\" alt=\"sparksql_connect\"></p>\n<h3>Motivating the Comparison between Hive and Spark</h3>\n<ul>\n<li>\n<p>Consider two types of tools used in Hadoop:</p>\n<ul>\n<li>Execution engines</li>\n<li>Query optimizers</li>\n</ul>\n</li>\n<li>These are both individual software frameworks</li>\n<li>An execution engine is required in Hadoop</li>\n<li>A query optimizer is optional</li>\n<li>An execution engine processes jobs related to the data</li>\n<li>\n<p>A query optimizer optimizes queries before they are processed</p>\n<ul>\n<li>Sometimes, they optimize queries during processing too</li>\n</ul>\n</li>\n</ul>\n<h3>Comparing Apache Hive and Spark</h3>\n<ul>\n<li>\n<p>Hive is mostly referred to as a query optimizer</p>\n<ul>\n<li>This is because Hive is essentially a metastore</li>\n</ul>\n</li>\n<li>Spark is mostly referred to as an execution engine</li>\n<li>Hive uses MapReduce as its execution engine by default</li>\n<li>\n<p>Spark uses its own execution engine</p>\n<ul>\n<li>It is an alternative of MapReduce</li>\n</ul>\n</li>\n<li>\n<p>Spark offers query optimizers as well</p>\n<ul>\n<li>It uses catalyst optimizers</li>\n<li>This uses rule-based and cost-based optimization</li>\n</ul>\n</li>\n<li>Spark and Hive can be used together</li>\n<li>\n<p>Specifically, we can include:</p>\n<ul>\n<li>HDFS as our storage layer</li>\n<li>Hive's metastore for query optimization</li>\n<li>Spark's query optimization</li>\n<li>Either Spark or MapReduce as an execution engine</li>\n</ul>\n</li>\n</ul>\n<h3>Describing Features of MLlib</h3>\n<ul>\n<li>Again, MLlib is the machine learning API in Spark</li>\n<li>It supports Java, Scala, Python, and R</li>\n<li>\n<p>MLlib provides functions for:</p>\n<ul>\n<li>Kmeans</li>\n<li>LogisticRegression</li>\n<li>etc.</li>\n</ul>\n</li>\n<li>\n<p>This allows data scientists to:</p>\n<ul>\n<li>Perform clustering</li>\n<li>Perform classification</li>\n<li>Transform data</li>\n<li>Process data</li>\n<li>Run algorithms</li>\n</ul>\n</li>\n<li>Specifically, these algorithms are run nearly <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>100</mn></mrow><annotation encoding=\"application/x-tex\">100</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span></span></span></span> times faster than MapReduce jobs</li>\n<li>MLlib supports Java, Scala, Python, and R APIs</li>\n</ul>\n<h3>Describing Features of Spark Streaming</h3>\n<ul>\n<li>MLlib is used for writing streamed jobs the same way as writing batch jobs</li>\n<li>Streamed jobs are processed in real-time</li>\n<li>\n<p>Spark Streaming can read data from:</p>\n<ul>\n<li>HDFS</li>\n<li>Flume</li>\n<li>Kafka</li>\n<li>Twitter</li>\n<li>Custom data sources</li>\n</ul>\n</li>\n<li>It uses ZooKeeper and HDFS for deploying streaming applications</li>\n<li>Spark Streaming supports Java, Scala, Python, and R APIs</li>\n</ul>\n<h3>Describing Features of GraphX</h3>\n<ul>\n<li>GraphX is used for performing graph-parallel computation to display and work with graphics</li>\n<li>\n<p>It unifies the following within a single API:</p>\n<ul>\n<li>ETL</li>\n<li>Exploratory analysis</li>\n<li>Iterative graph computations</li>\n</ul>\n</li>\n<li>It allows data to be viewed as graphs and collections</li>\n<li>Collections are transformed and joined graphs</li>\n<li>It offers integration with the Pregal API</li>\n<li>It offers comparable performance to the fastest specialized graph processing systems</li>\n<li>These include GraphLab and Giraph</li>\n</ul>\n<h3>Defining Resilient Distributed Datasets</h3>\n<ul>\n<li>Spark revolves around resilient distributed datasets (RDD)</li>\n<li>These are a fault-tolerant collection of elements that can be operated in parallel</li>\n<li>\n<p>There are two ways of creating RDDs:</p>\n<ol>\n<li>Parallelizing an existing collection in the driver program</li>\n<li>Referencing a dataset in an external storage system</li>\n</ol>\n</li>\n<li>\n<p>External storage systems include:</p>\n<ul>\n<li>HDFS</li>\n<li>HBase</li>\n<li>Other data sources offering a Hadoop InputFormat</li>\n</ul>\n</li>\n</ul>\n<h3>Benefits of Spark over MapReduce</h3>\n<ul>\n<li>Spark executes much faster by saving data in memory</li>\n<li>This happens across multiple parallel operations</li>\n<li>On the other hand, MapReduce reads and writes from disk</li>\n<li>Spark runs multi-threaded tasks inside of JVM processes</li>\n<li>MapReduce runs heavy-weight tasks inside of JVM processes</li>\n<li>\n<p>As a result, this gives Spark the following boost:</p>\n<ul>\n<li>Faster startup</li>\n<li>Efficient parallelism</li>\n<li>Better CPI utilization</li>\n</ul>\n</li>\n<li>Spark provides a richer functional programming model</li>\n<li>MapReduce doesn't do this</li>\n<li>Spark is especially useful for parallel processing of distributed data with iterative algorithms</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://mapr.com/blog/spark-101-what-it-what-it-does-and-why-it-matters/\" target=\"_blank\" rel=\"nofollow\">Defining the Architecture of Apache Spark</a></li>\n<li><a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\" rel=\"nofollow\">Documentation for Apache Spark</a></li>\n<li><a href=\"https://spark.apache.org/sql/\" target=\"_blank\" rel=\"nofollow\">Documentation for Spark SQL</a></li>\n<li><a href=\"https://spark.apache.org/streaming/\" target=\"_blank\" rel=\"nofollow\">Documentation for Spark Streaming</a></li>\n<li><a href=\"https://spark.apache.org/mllib/\" target=\"_blank\" rel=\"nofollow\">Documentation for Spark MLlib</a></li>\n<li><a href=\"https://spark.apache.org/graphx/\" target=\"_blank\" rel=\"nofollow\">Documentation for Spark GraphX</a></li>\n</ul>"}},"pageContext":{"slug":"de/etl/spark","previousSlug":"de/etl/zookeeper","nextSlug":null,"previousTitle":"ZooKeeper","nextTitle":null}},"staticQueryHashes":[]}