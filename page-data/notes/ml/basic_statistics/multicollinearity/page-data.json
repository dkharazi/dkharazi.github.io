{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/basic_statistics/multicollinearity","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Multicollinearity"},"html":"<h3>Describing Multicollinearity</h3>\n<ul>\n<li>Collinearity is a condition in which some of the independent variables are highly correlated with the other independent variables</li>\n<li>Collinearity tends to be a problem because it tends to inflate the variance of at least one estimated regression coefficient</li>\n<li>Specifically it will look like we have <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">p</span></span></span></span> different predictor variables, but really some of those predictors are linear combinations of another predictor</li>\n<li>Therefore, they don't add any additional information</li>\n<li>In summary, multicollinearity is a form of collinearity involving a perfectly linear relationship between more than two predictor variables</li>\n</ul>\n<h3>Diagnosing and Dealing with Collinearity</h3>\n<ul>\n<li>The most common method of diagnosing collinearity in our model involves creating pairs plots between each predictor</li>\n<li>The most common method of dealing with collinearity involves removing one of the variables from our model</li>\n<li>When removing collinear predictor variables, we need to be careful about which predictor variable we should remove</li>\n<li>For example, keeping a GPA predictor variable in our model may make more sense than keeping a categorical-level grade predictor variable in our model</li>\n</ul>\n<h3>Diagnosing and Dealing with Multicollinearity</h3>\n<ul>\n<li>Diagnosing multicollinearity can be more difficult (compared to diagnosing collinearity)</li>\n<li>A multicollinear relationship may involve three or more variables that could be totally invisible on a pairs plot</li>\n<li>Some turn to the variance inflation factors (VIF) to detect multicollinearity, but this can also be manipulated to miss multicollinearity\n- If the predictors are correlated with each other, the standard errors of the coefficient estimates will be bigger than if the predictors were uncorrelated</li>\n<li>Ridge regression typically works well in the presence of multicollinearity</li>\n<li>Also, one substantial advantage of ridge regression is that we don't have to make any decisions about which variables to remove, and can match (to extremely high accuracy) what we'd get after dropping variables</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf\" target=\"_blank\" rel=\"nofollow\">Truth about Linear Regression</a></li>\n<li><a href=\"http://www.stat.tamu.edu/~hart/652/collinear.pdf\" target=\"_blank\" rel=\"nofollow\">Collinearity</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/104779/why-does-ridge-regression-work-well-in-the-presence-of-multicollinearity\" target=\"_blank\" rel=\"nofollow\">Regression working with Multicollinearity</a></li>\n</ul>"}},"pageContext":{"slug":"ml/basic_statistics/multicollinearity","previousSlug":"ml/basic_statistics/causal_inference","nextSlug":"ml/basic_statistics/interactions","previousTitle":"Causal Inference","nextTitle":"Interactions"}},"staticQueryHashes":[]}