{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/bayesian_inference/bayesianism_and_frequentism","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Bayesianism and Frequentism"},"html":"<h3>Motivating Bayesian Inference</h3>\n<ul>\n<li>Bayesian inference typically involves parameter estimation and density estimation using probabalistic methods, such as Monte Carlo methods, Markov chains, or MCMC algorithms</li>\n<li>We typically use Bayes Theorem to estimate the posterior probability of observing some parameters (given our data)</li>\n<li>In other words, the poster probabilities associated with a parameter represent that parameter's estimates</li>\n<li>Roughly speaking, the posterior probability represents the likelihoods we're used to seeing, with an additional element of personal uncertainty about our parameters</li>\n<li>This element of subjectivity is known as a prior belief</li>\n<li>Bayesians typically represent this prior belief as a distribution that describes our beliefs about a parameter</li>\n<li>In other words, Bayesians (and Frequentists) believe the value of a parameter is a fixed single value from the start</li>\n<li>However, Bayesians represent any personal uncertainty about these parameters as a distribution</li>\n<li>\n<p>Frequentists, on the other hand, don't represent uncertainty within their estimates, and instead represent uncertainty after the parameters have already been estimated</p>\n<ul>\n<li>Specifically, Frequentists use tools like confidence intervals to represent uncertainty</li>\n</ul>\n</li>\n<li>Bayesian inference uses bayes theorem at a very broad level when calculating posterior probabiltiies as the following:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mi mathvariant=\"normal\">∣</mi><mi>D</mi><mo stretchy=\"false\">)</mo><mo>∝</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo><mi>P</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mi mathvariant=\"normal\">∣</mi><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(H|D) \\propto P(H)P(D|H)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∝</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span></span>\n<ul>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mi mathvariant=\"normal\">∣</mi><mi>D</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(H|D)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span></span></span></span> is the posterior distribution, which is a combination of the data for some parameter and our prior beliefs</li>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(H)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span> is the prior distribution, which represents our belief about the parameter</li>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>D</mi><mi mathvariant=\"normal\">∣</mi><mi>H</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P(D|H)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose\">)</span></span></span></span> is the likelihood distribution, which represents the data</li>\n</ul>\n<h3>Semi-Bayesian Inference</h3>\n<ul>\n<li>MAP estimation is a semi-bayesian technique used for parameter estimation</li>\n<li>\n<p>MAP estimation involves optimizing parameters, whereas MCMC is a sampling method (of the posterior distribution of parameters)</p>\n<ul>\n<li>This is because MAP estimates are point estimates, whereas Bayesian methods are characterized by their use of distributions to summarize data and draw inferences</li>\n<li>Thus, Bayesian methods tend to report the posterior mean or median along with credible intervals</li>\n</ul>\n</li>\n<li>\n<p>MAP estimates can be computed using conjugate priors</p>\n<ul>\n<li>However, the posterior, prior, and likelihood distributions will not follow a closed-form pdf if they fail any assumptions</li>\n<li>In this case, we would turn to MCMC techniques instead of MAP estimates</li>\n</ul>\n</li>\n</ul>\n<h3>Bayesian versus Frequentist Inference</h3>\n<ul>\n<li>Essentially, frequentists are concerned with uncertainty in the data, whereas Bayesians are concerned with uncertainty in the parameters</li>\n<li>This is because Bayesians represent uncertainty of parameters using distributions, whereas frequentists only represent uncertainty of data using distributions</li>\n<li>\n<p>Frequentists believe the following:</p>\n<ol>\n<li>\n<p>Probability is thought of in terms of frequencies and can't include any personal uncertainty</p>\n<ul>\n<li>Specifically, the probability of the event is the amount of times it happened over the total amount of times it could have happened</li>\n</ul>\n</li>\n<li>\n<p>Observed data is represented as a distribution, and parameters are represented as fixed values</p>\n<ul>\n<li>We consider our observed data to be a random variable</li>\n<li>We consider our parameters to be some unobservable fixed value</li>\n<li>In other words, only the data is represented using distributions</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>Bayesians believe the following:</p>\n<ol>\n<li>\n<p>Probability is belief or certainty about an event</p>\n<ul>\n<li>This belief exists because of the use of priors in Bayesian Inference</li>\n<li>If the prior is a uniform distribution, then the posterior probabilities will be the same as frequentist probabilities for some parameter</li>\n</ul>\n</li>\n<li>\n<p>Observed data is represented as fixed values, and parameters are represented as distributions</p>\n<ul>\n<li>We consider our observed data to be fixed values</li>\n<li>We consider our parameters to be some unobservable fixed value, but represent them as a distribution (with added uncertainty)</li>\n<li>In other words, only the parameters are represented using distributions</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3>Advantages Bayesian Inference</h3>\n<ol>\n<li>\n<p>Able to insert prior knowledge (i.e prior distribution) when there is a lack of data</p>\n<ul>\n<li>These priors will typically become irrelevant when the data better reflects the population</li>\n<li>This can happen when there is a small sample size or large sample size</li>\n<li>However, there is typically a smaller chance of having data reflecting the population when we have a small sample size, in comparison to when we have a large sample size</li>\n</ul>\n</li>\n<li>\n<p>Able to include an added level of uncertainty in our estimates</p>\n<ul>\n<li>This can be arguably better for reflecting the randomness that occurs in real world observations</li>\n</ul>\n</li>\n</ol>\n<h3>Different Methods of Inference</h3>\n<ul>\n<li>Example of Bayesian statistics: MCMC simulation</li>\n<li>Example of Frequentist statistics: ML estimation</li>\n<li>Example of Semi-Bayesian statistics: MAP estimation</li>\n</ul>\n<h3>Relevance of MCMC in Dynamics</h3>\n<ul>\n<li>\n<p>The likelihood distribution represents the transition matrix</p>\n<ul>\n<li>As a reminder, the transition matrix is a frequency matrix of the distinct current states (columns) and previous states (rows)</li>\n<li>As we increase the iterations of the transition matrix, the probabilities will eventually converge</li>\n</ul>\n</li>\n<li>\n<p>The prior distribution represents the state vector</p>\n<ul>\n<li>We are able to insert our own weights to filter any probabilities given by the transition matrix</li>\n</ul>\n</li>\n<li>\n<p>The posterior distribution represents the steady-state vector</p>\n<ul>\n<li>We are able to receive our converged distribution if we take many samples</li>\n</ul>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.sciencedirect.com/topics/computer-science/posterior-probability\" target=\"_blank\" rel=\"nofollow\">Paper on Bayesian Parameter Estimation Approaches</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval/2287#2287\" target=\"_blank\" rel=\"nofollow\">Difference between Confidence Interval and Credible Intervals</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english\" target=\"_blank\" rel=\"nofollow\">Intuition behind Bayesianism and Frequentism</a></li>\n<li><a href=\"https://frnsys.com/ai_notes/foundations/bayesian_statistics.html\" target=\"_blank\" rel=\"nofollow\">Examples of Bayesian Inference</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/200982/do-bayesian-priors-become-irrelevant-with-large-sample-size\" target=\"_blank\" rel=\"nofollow\">Do Bayesian Priors become Irrelevant with a Large Sample Size</a></li>\n<li><a href=\"http://www.svcl.ucsd.edu/courses/ece271A/handouts/BayesIntro.pdf\" target=\"_blank\" rel=\"nofollow\">Examples of Bayesian Inference Lecture Slides</a></li>\n</ul>"}},"pageContext":{"slug":"ml/bayesian_inference/bayesianism_and_frequentism","previousSlug":null,"nextSlug":"ml/bayesian_inference/map","previousTitle":null,"nextTitle":"Maximum a Posteriori Estimation"}},"staticQueryHashes":[]}