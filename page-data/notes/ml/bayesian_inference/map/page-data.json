{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/bayesian_inference/map","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Maximum a Posteriori Estimation"},"html":"<h3>Describing MAP Estimation</h3>\n<ul>\n<li>The MAP estimator is a semi-Bayesian technique for finding the maximum probability of the posterior probability distribution</li>\n<li>In other words, the MAP estimate is an estimate of the true mode of the posterior distribution</li>\n<li>\n<p>The MAP estimator with different prior distributions lead to different regulizers and estimators</p>\n<ul>\n<li>A MAP estimator with a zero-mean Gaussian prior equals the cost function associated with L1 regularization of OLS estimation(i.e. LASSO)</li>\n<li>A MAP estimator with a zero-mean Laplacean prior equals the cost function associated with L2 regularization of OLS estimation (i.e. Ridge)</li>\n<li>A MAP estimator with a uniform prior equals the MLE</li>\n</ul>\n</li>\n</ul>\n<h3>Computing MAP Estimates</h3>\n<ul>\n<li>Typically, we calculate MAP estimates analytically</li>\n<li>Meaning, the posterior distribution follows a closed-form distribution (i.e. Normal, Poisson, etc.)</li>\n<li>Therefore, we can use conjugate priors to estimate the mode of the posterior distribution</li>\n</ul>\n<h3>MLE, MAP, and Bayesian Inference</h3>\n<ul>\n<li>Bayesian estimation techniques and MAP both involve computing likelihoods and priors, but in different ways</li>\n<li>MLE only involves computing the likelihoods (without some prior belief)</li>\n<li>Bayesian estimation techniques and MAP both return some information about the parameters of the posterior distribution, prior distribution, and likelihood function</li>\n<li>MLE only returns some information about the parameters of the likelihood function</li>\n<li>Bayesian inference typically returns some information about a parameter through simulation</li>\n<li>MAP and MLE typically return some information about a parameter through analytical computations of assumed closed-form distributional expressions</li>\n<li>\n<p>MAP returns some information about a parameter through analytical computation</p>\n<ul>\n<li>Specifically, the analytical computation involves maximizing the likelihood of observing the parameter given some data</li>\n<li>Assumes the posterior follows a closed-form distribution</li>\n</ul>\n</li>\n<li>\n<p>MLE returns some information about a parameter through analytical computation</p>\n<ul>\n<li>Specifically, the analytical computation involves maximizing the likelihood of observing the data given some parameter</li>\n<li>Assumes the data follows a closed-form distribution</li>\n</ul>\n</li>\n<li>Bayesian inference usually involves returning the entire posterior probability distribution, whereas MAP involves returning a single estimate of a parameter</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/\" target=\"_blank\" rel=\"nofollow\">Regularization in MAP Estimation</a></li>\n<li><a href=\"https://towardsdatascience.com/mle-map-and-bayesian-inference-3407b2d6d4d9\" target=\"_blank\" rel=\"nofollow\">Differences between MLE, MAP, and Bayesian Inference</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\" target=\"_blank\" rel=\"nofollow\">Maximum a Posteriori Estimation Wiki</a></li>\n</ul>"}},"pageContext":{"slug":"ml/bayesian_inference/map","previousSlug":"ml/bayesian_inference/bayesianism_and_frequentism","nextSlug":"ml/bayesian_inference/credible_intervals","previousTitle":"Bayesianism and Frequentism","nextTitle":"Credible Intervals"}},"staticQueryHashes":[]}