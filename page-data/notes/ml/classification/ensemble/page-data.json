{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/classification/ensemble","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Ensemble Methods"},"html":"<h3>Motivating Ensemble Methods</h3>\n<ul>\n<li>An ensemble method is a technique that combines several base models together in order to produce one optimal predictive model</li>\n<li>Ensemble methods are used to improve predictions by decreasing variance or bias</li>\n<li>Ensemble methods produce <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> learners by generating additional data in the training stage</li>\n<li>These <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> new training datasets are produced by random sampling with replacement from the original dataset</li>\n</ul>\n<h3>Describing Bagging</h3>\n<ul>\n<li>In the case of bagging, any observation has the same probability of appearing in the new dataset</li>\n<li>The training stage is parallel for bagging</li>\n<li>Meaning, each model is build independently</li>\n<li>In bagging, the result is obtained by averaging the responses of the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> learners (or majority vote)</li>\n</ul>\n<h3>Bagging Algorithm</h3>\n<ol>\n<li>Draw a random sample with replacement from the training set</li>\n<li>Train a model on that random sample</li>\n<li>Keep repeating the above steps until we're satisfied with the number of models we have</li>\n<li>Perform classification on all of the models, and choose the class with the highest number of votes out of all the models (i.e. majority vote)</li>\n</ol>\n<h3>Advantages of Bagging</h3>\n<ul>\n<li>Attempts to reduce variance</li>\n<li>Reduces overfitting</li>\n<li>Handles higher dimensionality of data well</li>\n<li>Maintains accuracy for missing data</li>\n</ul>\n<h3>Disadvantages of Bagging</h3>\n<ul>\n<li>Bagging will rarely produce better bias</li>\n<li>Since the final prediction is based on the mean of the predictions from the subset trees, it won't give precise values for the classification and regression model</li>\n</ul>\n<h3>Describing Boosting</h3>\n<ul>\n<li>In the case of boosting, observations have a distinct probability of appearing in the new dataset</li>\n<li>Boosting builds the new learnings in a sequential way</li>\n<li>Boosting is an iterative ensemble technique that adjusts the weight of an observation based on its previous classification's success</li>\n<li>Specifically, boosting will increase the weight of an observation if the observation was incorrectly classified</li>\n<li>In boosting, the result is obtained by taking a weighted average of the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> learners</li>\n<li>Specifically, the algorithm allocates weights to each resulting model</li>\n<li>A learner with a good classification result on the training data will be assigned a higher weight compared to learners with a poor classification results</li>\n<li>So, boosting needs to keep track of learners' errors, too</li>\n</ul>\n<h3>Boosting Algorithm</h3>\n<ol>\n<li>Draw a random sample without replacement from the training set</li>\n<li>Train a weak learner on that random sample</li>\n<li>Draw another random sample without replacement from the training set, and add <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">50%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">0</span></span></span></span> of the observations that were incorrectly classified from the previous sample</li>\n<li>Train a weak learner on our new random sample</li>\n<li>Draw another random sample without replacement from the training set, and add <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>50</mn></mrow><annotation encoding=\"application/x-tex\">50%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span><span class=\"mord\">0</span></span></span></span> of the observations that were incorrectly classified from the previous two samples</li>\n<li>Keep repeating the above steps until we're satisfied with the number of weak learners we have</li>\n<li>Perform classification on all of the models, and choose the class with the highest number of votes out of all the models (i.e. majority vote)</li>\n</ol>\n<h3>Advantages of Boosting</h3>\n<ul>\n<li>\n<p>Supports different loss functions</p>\n<ul>\n<li>By default, we typically use binary:logistic loss function</li>\n</ul>\n</li>\n<li>Works well with interactions</li>\n<li>Attemps to reduce bias</li>\n</ul>\n<h3>Disadvantages of Boosting</h3>\n<ul>\n<li>Prone to overfitting</li>\n<li>Requires careful tuning of different hyper-parameters</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\" target=\"_blank\" rel=\"nofollow\">Illustrating Differences between Ensemble Methods</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Boosting_(machine_learning)\" target=\"_blank\" rel=\"nofollow\">Boosting Wiki</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Bootstrap_aggregating\" target=\"_blank\" rel=\"nofollow\">Bagging Wiki</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning\" target=\"_blank\" rel=\"nofollow\">Overview of Different Ensemble Methods</a></li>\n<li><a href=\"https://blog.statsbot.co/ensemble-learning-d1dcd548e936\" target=\"_blank\" rel=\"nofollow\">Blog Post about Ensemble Methods</a></li>\n<li><a href=\"https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/\" target=\"_blank\" rel=\"nofollow\">Blog Post about Bagging and Boosting</a></li>\n</ul>"}},"pageContext":{"slug":"ml/classification/ensemble","previousSlug":"ml/classification/information_gain","nextSlug":"ml/classification/decision_trees","previousTitle":"Information Gain","nextTitle":"Decision Trees"}},"staticQueryHashes":[]}