{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/classification/lda","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Linear Discriminant Analysis"},"html":"<h3>Describing Linear Discriminant Analysis</h3>\n<ul>\n<li>Linear Discriminant Analysis (LDA) is a supervised classification technique that is solved using either SVD (i.e. dimensionality reduction) or bayes theorem (i.e. Bayesian techniques)</li>\n<li>If we assume LDA uses dimensionality reduction when predicting an observation's class, then LDA involves mapping the data from a high dimensional space to a lower dimensional space</li>\n<li>The data is transformed to a lower dimensional space by finding the axes that maximize the seperatibility between classes (in the lower dimensional space)</li>\n<li>Said another way, LDA maps the data from a high dimensional space to a lower dimensional space by performing a linear transformation on the data in its original form (i.e. in the high dimensional space)</li>\n<li>More specifically, the linear transformation includes a change of basis (using the SVD formula) that finds the axes that best separate the classes</li>\n<li>LDA uses linear decision boundaries to determine the class of an observation in the newly mapped space</li>\n</ul>\n<h3>Assumptions</h3>\n<ul>\n<li>Observations within each class are drawn from a multivariate Gaussian distribution</li>\n<li>Each class has its own unique mean vector, but each class needs to have equal variance/covariance</li>\n</ul>\n<h3>The LDA Algorithm</h3>\n<ol>\n<li>Perform a change of basis on the data that finds the axes that best separate the classes</li>\n<li>Receive coefficients for (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">k-1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>) number of linear discriminants (LDA axes) based on the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span></span></span></span> number of classes (from the response)</li>\n<li>Use these coefficients to map the data on the new vector space (LDA axes)</li>\n<li>Determine the class for an observation by observing where the mapped observation lands with respect to the (linear) decision boundaries</li>\n</ol>\n<h3>Preparing for LDA</h3>\n<ul>\n<li>\n<p>Check assumptions</p>\n<ul>\n<li>Gaussian distribution - Use log and root transformations to ensure Gaussian distributions are maintained</li>\n<li>Same variance - standarize data to ensure equal variance is maintained across distribution</li>\n</ul>\n</li>\n<li>Possibly remove outliers</li>\n</ul>\n<h3>Advantages over Logistic Regression</h3>\n<ul>\n<li>The preferred method of classification is logistic regression when the response has exactly 2 classes</li>\n<li>The preferred method of classification is linear discriminant analysis when the response has more than 2 classes</li>\n<li>Logistic regression parameter estimates can become poor/unstable when the two classes are well-separated, whereas LDA does not suffer from this</li>\n<li>Logistic regression parameter estimates can become poor/unstable if the sample size is small, whereas LDA is more stable (assuming normally distributed predictors)</li>\n</ul>\n<h3>Difference between Logistic Regression and LDA</h3>\n<ul>\n<li>Logistic regression and LDA both use MLE for parameter estimation (or Bayesian techniques)</li>\n<li>Logistic regression involves directly modeling <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Pr(Y=1|X=x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">1</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span> using the logistic function</li>\n<li>LDA involves directly modeling <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy=\"false\">(</mo><mi>Y</mi><mo>=</mo><mi>k</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Pr(Y=k|X=x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></li>\n</ul>\n<h3>Difference between PCA and LDA</h3>\n<ul>\n<li>PCA is an unsupervised learning method that involves performing linear transformations (dimensionality reduction) on the data to find the features that make up the most variability</li>\n<li>LDA is a supervised learning method that involves performing linear transformations (dimensionality reduction) on the data to maximize the distance between classes and minimize the distance within classes</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://sebastianraschka.com/Articles/2014_python_lda.html#principal-component-analysis-vs-linear-discriminant-analysis\" target=\"_blank\" rel=\"nofollow\">Difference between PCA and LDA</a></li>\n<li><a href=\"https://www.cs.princeton.edu/courses/archive/fall08/cos429/CourseMaterials/lecture2/PCA_handout.pdf\" target=\"_blank\" rel=\"nofollow\">Dimensionality Reduction of LDA</a></li>\n<li><a href=\"http://www.stats.ox.ac.uk/~sejdinov/teaching/sdmml15/materials/HT15_lecture6-nup.pdf\" target=\"_blank\" rel=\"nofollow\">Discriminant Analysis Example</a></li>\n<li><a href=\"http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/\" target=\"_blank\" rel=\"nofollow\">Comparison between PCA, LDA, and QDA</a></li>\n</ul>"}},"pageContext":{"slug":"ml/classification/lda","previousSlug":"ml/classification/logistic_regression","nextSlug":"ml/classification/qda","previousTitle":"Logistic Regression","nextTitle":"Quadratic Discriminant Analysis"}},"staticQueryHashes":[]}