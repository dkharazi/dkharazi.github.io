{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/classification/random_forests","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Random Forests"},"html":"<h3>Motivating Random Forests</h3>\n<ul>\n<li>Decision trees are easy to build and easy to interpret</li>\n<li>However, decision trees are typically less accurate than other learning methods in practice</li>\n<li>In other words, they work great with the data used to create them (i.e. training data), but are not flexible when it comes to classifying new samples (i.e. testing data)</li>\n<li>The good news is that random forests combine the simplicity of decision trees with flexibility</li>\n<li>As a result, random forests receive a great improvement in accuracy compared to ordinary decision trees</li>\n</ul>\n<h3>The Random Forest Algorithm</h3>\n<ol>\n<li>\n<p>Create a bootstrapped dataset from out training data</p>\n<ul>\n<li>Sample from the training data with replacement in order to create a bootstrapped dataset</li>\n</ul>\n</li>\n<li>\n<p>Create a decision tree using the bootstrapped dataset</p>\n<ul>\n<li>Only use a random subset of variables</li>\n<li>This is to prevent any bias or overfitting in the eventual random forest model</li>\n</ul>\n</li>\n<li>Repeat steps 1 and 2 a bunch of time so we have many different types of trees with different columns trained on different data</li>\n<li>\n<p>Predict any new observations (i.e. testing data) by running its attributes through each and every decision tree</p>\n<ul>\n<li>This step can refer to the model evalutation step or the prediction step</li>\n<li>We typically want to perform model evaulation directly after collecting all of our decision trees</li>\n<li>This process involves fitting our collection of decision trees on testing data to verify a high accuracy</li>\n<li>We typically will predict after our model evaluation step</li>\n</ul>\n</li>\n<li>\n<p>Sum up all of those predictions and choose the option (or class) that received the most votes</p>\n<ul>\n<li>We refer to this process as bagging</li>\n<li>Specifically, bagging refers aggregating predictions on bootstrapped data</li>\n</ul>\n</li>\n</ol>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://www.stat.cmu.edu/~cshalizi/dm/19/lectures/25/lecture-25.html\" target=\"_blank\" rel=\"nofollow\">Random Forests and Other Ensembles</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&#x26;vl=en\" target=\"_blank\" rel=\"nofollow\">Building Random Forests StatQuest Video</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=nyxTdL_4Q-Q&#x26;t=408s\" target=\"_blank\" rel=\"nofollow\">Clustering Random Forests StatQuest Video</a></li>\n</ul>"}},"pageContext":{"slug":"ml/classification/random_forests","previousSlug":"ml/classification/decision_trees","nextSlug":"ml/classification/boosting","previousTitle":"Decision Trees","nextTitle":"Gradient Boosting"}},"staticQueryHashes":[]}