{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/cnn/fc","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Benefits of Convolution"},"html":"<h3>More on Fully-Connected Layers</h3>\n<ul>\n<li>A fully-connected layer refers to the layers of a neural network that we're familiar with using</li>\n<li>As a reminder, a fully-connected layer refers to a number of neurons that are connected with all of the activations passed from a previous layer</li>\n<li>In other words, a fully-connected layer is used throughout traditional multilayer perceptron neural networks</li>\n</ul>\n<h3>Motivating of Convolutional Layers</h3>\n<ul>\n<li>Convolutional layers have a benefits over fully-connected layers</li>\n<li>We're interested in the benefits of convolutional layers because the number of parameters within a fully-connected layer tends to grow infeasibly large</li>\n<li>For example, we would need to learn <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>14</mn></mrow><annotation encoding=\"application/x-tex\">14</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">4</span></span></span></span> million parameters from a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>32</mn><mo>×</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">32 \\times 32</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">3</span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span><span class=\"mord\">2</span></span></span></span> image in a fully-connected layer</li>\n<li>Specifically, we can learn this amount of parameters</li>\n<li>However, we're typically interested in larger images</li>\n<li>So, learning the parameters from a <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1000</mn><mo>×</mo><mn>1000</mn></mrow><annotation encoding=\"application/x-tex\">1000 \\times 1000</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">0</span><span class=\"mord\">0</span></span></span></span> image would become infeasible</li>\n<li>Therefore, we need to rely on convolutional layers for parameter learning, at least until we reduce the dimensionality enough</li>\n</ul>\n<h3>Benefits of Convolutional Layers</h3>\n<ol>\n<li>\n<p>Parameter Sharing</p>\n<ul>\n<li>This refers to contraining our image to only using the same small set of weights and biases</li>\n<li>Therefore, parameter sharing only works because we may one reasonable assumption</li>\n<li>Specifically, we assume that if one filter (or set of parameters) is useful to compute at some region in our image, then it should also be useful to compute at other regions in our image</li>\n<li>This is why convolutional layers are sometimes referred to as a <em>locally-connected layer</em></li>\n<li>Obviously, the major benefit of parameter sharing is that we're able to reduce the number of parameters</li>\n</ul>\n</li>\n<li>\n<p>Sparsity of Connections</p>\n<ul>\n<li>In each layer, each output value depends only on a small number of inputs</li>\n<li>Here, we're referring to the values of our output image</li>\n<li>In other words, each value in our output image only depends on small regions within our input image</li>\n<li>The major benefit of sparsity of connections is that we're able to reduce the number of parameters</li>\n<li>Another major benefit is that we're less prone to overfitting</li>\n<li>Again, this is because each output value is only based on a small percentage of the values of the input image</li>\n</ul>\n</li>\n<li>\n<p>Translation Invariance</p>\n<ul>\n<li>This refers to the ability of a convolutional layer being able to produce the same output if an image is shifted marginally</li>\n<li>For example, a convolutional layer will tend to produce the same output if an image is shifted a few pixels </li>\n<li>Meaning, convolutional layers are very robust</li>\n</ul>\n</li>\n</ol>\n<h3>Training Convolutional Layers</h3>\n<ul>\n<li>We need to learn the parameters within any convolutional, pooling, or fully-connected layers</li>\n<li>Therefore, we can just use gradient descent to optimize parameters to reduce our cost function <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>J</mi></mrow><annotation encoding=\"application/x-tex\">J</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span></span></span></span></li>\n<li>We can also use other optimization algorithms (e.g. Adam, etc.)</li>\n</ul>\n<hr>\n<h3>tldr</h3>\n<ul>\n<li>A fully-connected layer refers to the layers of a neural network that we're familiar with using</li>\n<li>As a reminder, a fully-connected layer refers to a number of neurons that are connected with all of the activations passed from a previous layer</li>\n<li>In other words, a fully-connected layer is used throughout traditional multilayer perceptron neural networks</li>\n<li>\n<p>The following are the major benefits of convolutional layers:</p>\n<ul>\n<li>Parameter sharing</li>\n<li>Sparsity of connections</li>\n<li>Translation invariance</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=ay3zYUeuyhU&#x26;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&#x26;index=11\" target=\"_blank\" rel=\"nofollow\">Convolutional and Fully-Connected Layers</a></li>\n<li><a href=\"https://cs231n.github.io/convolutional-networks/#conv\" target=\"_blank\" rel=\"nofollow\">Parameter Sharing in Convolutional Layers</a></li>\n</ul>"}},"pageContext":{"slug":"ml/cnn/fc","previousSlug":"ml/cnn/lenet","nextSlug":"ml/cnn/classic_cnn","previousTitle":"LeNet-5 Implementation","nextTitle":"Common Case Studies"}},"staticQueryHashes":[]}