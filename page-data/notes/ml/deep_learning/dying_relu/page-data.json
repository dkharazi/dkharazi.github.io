{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/deep_learning/dying_relu","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Dying ReLU"},"html":"<h3>Motivating the Dying ReLU Problem</h3>\n<ul>\n<li>ReLU doesn't suffer from the vanishing gradient problem as much as other activation functions</li>\n<li>However, the relu function still has a vanishing gradient problem, but only on one side</li>\n<li>Therefore, we call it something else</li>\n<li>We call it the dying relu problem instead</li>\n</ul>\n<h3>Describing the Dying ReLU Problem</h3>\n<ul>\n<li>Since the relu function returns <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> for all negative inputs, the gradient of negative sums is also <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span></li>\n<li>Indicating, a neuron stops learning once it becomes negative</li>\n<li>This usually happens because of a very large negative bias <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span></li>\n<li>Since the gradient is always <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, then a neuron is unlikely to recover</li>\n<li>Therefore, the weights will not adjust in gradient descent</li>\n<li>This is good if we are at a global minimum</li>\n<li>However, we'll frequently get stuck at local minimums and plateaus because of the dying relu problem</li>\n</ul>\n<h3>The Vanishing Gradient and Dying ReLU Problem</h3>\n<ul>\n<li>The derivatives of many activation functions (e.g. tanh, sigmoid, etc.) are very close to <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span></li>\n<li>In other words, if the gradient becomes smaller, then the slower and harder it is to return into the <em>good</em> zone</li>\n<li>Roughly speaking, this demonstrates a major effect of the vanishing gradient problem</li>\n<li>The gradient of the relu function doesn't become smaller in the positive direction</li>\n<li>Therefore, the relu function doesn't suffer from the vanishing gradient problem in the positive direction</li>\n</ul>\n<h3>Introducing the Leaky ReLU</h3>\n<ul>\n<li>The leaky relu attempts to solve the dying relu problem</li>\n<li>Specifically, the leaky relu does this by providing a very small gradient for negative values</li>\n<li>This represents an attempt to allow neurons to recover</li>\n<li>We can define the leaky relu function as the following:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>leakyrelu</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.3599999999999999em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mn>0.01</mn><mi>x</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{leakyrelu}(x) = \\begin{cases} 0.01x &amp;\\text{if } x \\le 0 \\cr x &amp;\\text{if } x &gt; 0 \\end{cases}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">leakyrelu</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0000299999999998em;vertical-align:-1.25003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">0</span><span class=\"mord\">1</span><span class=\"mord mathnormal\">x</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:1em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if </span></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">≤</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">0</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if </span></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p><img src=\"/a4acaa99f51f0453ce96748383a1ed07/leakyrelu.svg\" alt=\"leakyrelu\"></p>\n<ul>\n<li>Unfortunately, the leaky relu doesn't perform as well as the relu</li>\n<li>Also, there isn't much of an accuracy boost in most circumstances</li>\n</ul>\n<hr>\n<h3>tldr</h3>\n<ul>\n<li>ReLU doesn't suffer from the vanishing gradient problem as much as other activation functions</li>\n<li>However, the relu function still has a vanishing gradient problem, but only on one side</li>\n<li>Therefore, we call it something else</li>\n<li>We call it the dying relu problem instead</li>\n<li>The leaky relu attempts to solve the dying relu problem</li>\n</ul>\n<hr>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks\" target=\"_blank\" rel=\"nofollow\">What is the Dying ReLU Problem</a></li>\n<li><a href=\"https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b\" target=\"_blank\" rel=\"nofollow\">The Vanishing Gradient and Dying ReLU Problem</a></li>\n<li><a href=\"https://www.quora.com/How-does-the-ReLu-solve-the-vanishing-gradient-problem\" target=\"_blank\" rel=\"nofollow\">How ReLU Solves the Vanishing Gradient Problem</a></li>\n<li><a href=\"https://datascience.stackexchange.com/questions/11591/relu-does-have-0-gradient-by-definition-then-why-gradient-vanish-is-not-a-probl\" target=\"_blank\" rel=\"nofollow\">Relating the Vanishing Gradient to the Dying ReLU Problem</a></li>\n<li><a href=\"https://www.quora.com/What-is-the-dying-ReLU-problem-in-neural-networks\" target=\"_blank\" rel=\"nofollow\">What is the Dying ReLU Problem</a></li>\n</ul>"}},"pageContext":{"slug":"ml/deep_learning/dying_relu","previousSlug":"ml/deep_learning/vanishing_gradient","nextSlug":"ml/deep_learning/weight_initialization","previousTitle":"Vanishing Gradient","nextTitle":"Weight Initialization"}},"staticQueryHashes":[]}