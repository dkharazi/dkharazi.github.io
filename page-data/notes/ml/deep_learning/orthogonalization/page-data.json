{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/deep_learning/orthogonalization","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Orthogonalization"},"html":"<h3>Introducing a Conceptual Framework for Deep Learning</h3>\n<ul>\n<li>\n<p>We can represent the deep learning process in a few general steps</p>\n<ol>\n<li>\n<p>Optimize a cost function <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>J</mi></mrow><annotation encoding=\"application/x-tex\">J</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span></span></span></span> using any of the following:</p>\n<ul>\n<li>Gradient descent</li>\n<li>Momentum</li>\n<li>RMSprop</li>\n<li>Adam</li>\n<li>etc.</li>\n</ul>\n</li>\n<li>\n<p>Reduce overfitting using any of the following:</p>\n<ul>\n<li>Regularization</li>\n<li>Getting more data</li>\n<li>etc.</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>Most of the complication around deep learning stems from tuning the hyperparameters for each algorithm</li>\n<li>\n<p>We do this because our goal is to find two algorithms:</p>\n<ul>\n<li>One algorithm that best optimizes the cost function</li>\n<li>Another algorithm that best prevents overfitting</li>\n</ul>\n</li>\n</ul>\n<h3>Optimizing a Cost Function</h3>\n<ul>\n<li>As stated previously, optimizing a cost function <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>J</mi></mrow><annotation encoding=\"application/x-tex\">J</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span></span></span></span> is the first step in our framework</li>\n<li>For this step, our only goal is to find the parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> that minimize <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>J</mi></mrow><annotation encoding=\"application/x-tex\">J</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span></span></span></span></li>\n<li>Common methods for optimizing a cost function include gradient descent and Adam</li>\n<li>However, we can use other approaches, such as RMSprop, Momentum, etc.</li>\n</ul>\n<h3>Reducing Overfitting</h3>\n<ul>\n<li>As stated previously, reducing overfitting is the second step in our framework</li>\n<li>For this step, our only goal is to reduce variance</li>\n<li>Common methods for reducing overfitting include regularization and getting more data</li>\n<li>However, we can use other approaches, such as early stopping, data segmentation, etc.</li>\n</ul>\n<h3>Defining Orthogonalization</h3>\n<ul>\n<li>Orthogonalization is not specific to deep learning</li>\n<li>Orthogonalization is a general concept that carries a different meaning in mathetmatics, computer science, and debate</li>\n<li>Roughly speaking, orthogonalization refers to components acting independently of each other</li>\n<li>Said another way, two things are orthogonal if they act in isolation of each other</li>\n</ul>\n<h3>Relating Orthogonalization to our Framework</h3>\n<ul>\n<li>Here, orthogonalization refers to thinking of one task at a time</li>\n<li>Generally speaking, organizing the deep learning process into two orthogonal tasks can help reduce a lot of the complication around deep learning</li>\n<li>Specifically, organizing the deep learning process into two orthogonal tasks can help us with our development time and general intuition</li>\n<li>In other words, we should be executing these two tasks independently when designing a neural network</li>\n</ul>\n<hr>\n<h3>tldr</h3>\n<ul>\n<li>Orthogonalization is not specific to deep learning</li>\n<li>Roughly speaking, orthogonalization refers to components acting independently of each other</li>\n<li>Said another way, two things are orthogonal if they act in isolation of each other</li>\n<li>\n<p>The deep learning process can be generalized into two orthogonal tasks:</p>\n<ol>\n<li>Optimizing a cost function <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>J</mi></mrow><annotation encoding=\"application/x-tex\">J</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.09618em;\">J</span></span></span></span></li>\n<li>\n<h2>Reducing overfitting</h2>\n</li>\n</ol>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.youtube.com/watch?v=BOCLq2gpcGU&#x26;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&#x26;index=8\" target=\"_blank\" rel=\"nofollow\">Orthogonalization and Overfitting</a></li>\n<li><a href=\"https://english.stackexchange.com/questions/12219/usage-of-the-word-orthogonal-outside-of-mathematics\" target=\"_blank\" rel=\"nofollow\">Usage of Orthogonalization outside of Mathematics</a></li>\n</ul>"}},"pageContext":{"slug":"ml/deep_learning/orthogonalization","previousSlug":"ml/deep_learning/dropout_regularization","nextSlug":"ml/deep_learning/normalization","previousTitle":"Dropout Regularization","nextTitle":"Normalizing Inputs"}},"staticQueryHashes":[]}