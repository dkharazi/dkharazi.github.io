{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/deep_learning/stochastic_gradient_descent","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Stochastic Gradient Descent"},"html":"<h3>Motivating other Optimization Algorithms</h3>\n<ul>\n<li>Gradient descent can vary in terms of the number of iterations it takes to converge to parameters values</li>\n<li>The number of iterations typically determines the stability of a gradient used to update the parameters</li>\n<li>\n<p>There are typically three types of gradient descent</p>\n<ul>\n<li>\n<p>Batch gradient descent</p>\n<ul>\n<li>This is what we've been using up until this point</li>\n</ul>\n</li>\n<li>\n<p>Stochastic gradient descent</p>\n<ul>\n<li>This is a different type of gradient descent</li>\n</ul>\n</li>\n<li>\n<p>Mini batch gradient descent</p>\n<ul>\n<li>This is a hybrid between the above two</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Motivating Stochastic Gradient Descent</h3>\n<ul>\n<li>Gradient descent can lead to slow traning on very large datasets</li>\n<li>This is because one iteration requires a prediction for each instance in the training dataset</li>\n<li>When we have very large data, we could use stochastic gradient descent</li>\n</ul>\n<h3>Describing Stochastic Gradient Descent</h3>\n<ul>\n<li>Stochastic gradient descent (SGD) is a variation of gradient descent</li>\n<li>In this variation, the gradient descent procedure updates parameters for each training instance, rather than at the end of each iteration</li>\n<li>Specifically, we calculate the error and update the parameters accordingly for each training observation</li>\n</ul>\n<h3>Upsides</h3>\n<ul>\n<li>\n<p>SGD can quickly give us insight about the performance and rate improvement of our network</p>\n<ul>\n<li>This is because parameters are updated so frequently</li>\n</ul>\n</li>\n<li>\n<p>SGD is sometimes useful for beginners</p>\n<ul>\n<li>This is sometimes simpler to understand than other variants of gradient descent</li>\n</ul>\n</li>\n<li>\n<p>SGD can sometimes lead to faster learning</p>\n<ul>\n<li>The increased frequency of parameter updates can result in faster learning</li>\n</ul>\n</li>\n<li>\n<p>SGD can lead to premature parameter convergence</p>\n<ul>\n<li>The noisy update process can lead to networks avoiding the local minima</li>\n</ul>\n</li>\n</ul>\n<h3>Downsides</h3>\n<ul>\n<li>\n<p>SGD can be more computationally expensive compared to other optimization algorithms</p>\n<ul>\n<li>This is because the parameters are compared and updated so frequently</li>\n</ul>\n</li>\n<li>\n<p>SGD can result in updates leading to noisy gradient</p>\n<ul>\n<li>The gradients have a high variance over training epochs</li>\n</ul>\n</li>\n<li>\n<p>SGD can sometimes lead to worse accuracy</p>\n<ul>\n<li>The noisy learning process can make it hard for the algorithm to settle on a minimum of the cost function</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3>tldr</h3>\n<ul>\n<li>Stochastic gradient descent (SGD) is a variation of gradient descent</li>\n<li>In this variation, the gradient descent procedure updates parameters for each training instance, rather than at the end of each iteration</li>\n<li>Specifically, we calculate the error and update the parameters accordingly for each training observation</li>\n</ul>\n<hr>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://www.deeplearningbook.org/contents/numerical.html\" target=\"_blank\" rel=\"nofollow\">4.3 Gradient-Based Optimization</a></li>\n<li><a href=\"http://www.deeplearningbook.org/contents/ml.html\" target=\"_blank\" rel=\"nofollow\">5.9 Stochastic Gradient Descent</a></li>\n<li><a href=\"http://www.deeplearningbook.org/contents/mlp.html#pf6\" target=\"_blank\" rel=\"nofollow\">6.2 Gradient-Based Learning</a></li>\n<li><a href=\"https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\" target=\"_blank\" rel=\"nofollow\">Introduction to Mini Batch</a></li>\n<li><a href=\"https://machinelearningmastery.com/gradient-descent-for-machine-learning/\" target=\"_blank\" rel=\"nofollow\">Gradient Descent for Machine Learning</a></li>\n</ul>"}},"pageContext":{"slug":"ml/deep_learning/stochastic_gradient_descent","previousSlug":"ml/deep_learning/gradient_checking","nextSlug":"ml/deep_learning/mini_batch","previousTitle":"Gradient Checking","nextTitle":"Mini-Batch Gradient Descent"}},"staticQueryHashes":[]}