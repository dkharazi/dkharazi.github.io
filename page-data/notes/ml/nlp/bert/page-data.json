{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/bert","result":{"data":{"markdownRemark":{"frontmatter":{"title":"BERT"},"html":"<h3>History of BERT and Other NLP Models</h3>\n<ul>\n<li>In the early stages of NLP, we simply wanted to predict the next word in a sentence</li>\n<li>\n<p>To do this, we used a continuous bag of words (CBOW) model</p>\n<ul>\n<li>This model is limited to classification based on the input words within a fixed-length sliding window</li>\n<li>Unfortunately, this model excludes the use of many useful context words and relationships with other words in the sentence</li>\n</ul>\n</li>\n<li>\n<p>Then, ELMo was created in 2018 by researchers at the Allen Institute</p>\n<ul>\n<li>This model is a bidirectional LSTM</li>\n<li>Implying, words from the left and right are considered</li>\n<li>This model is able to entirely capture context words and relationships</li>\n<li>However, it still suffered from capturing context in longer sentences</li>\n</ul>\n</li>\n<li>\n<p>Later in 2018, OpenAI introduced the GPT model</p>\n<ul>\n<li>There are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span> versions of GPT: GPT-1, GPT-2, and GPT-3</li>\n<li>All three models are a transformer model</li>\n<li>This model only includes a decoder (no encoders included)</li>\n<li>This model only uses causal attention</li>\n<li>Unfortunately, each GPT model is only unidirectional</li>\n<li>Thus, we can't capture context both leftward and rightward of our target word in a sentences</li>\n</ul>\n</li>\n<li>\n<p>In 2019, Google released the BERT model</p>\n<ul>\n<li>This model is a bidirectional transformer</li>\n<li>Implying, words from the left and right are considered</li>\n<li>This model is able to entirely capture context words and relationships</li>\n<li>This model only includes an encoder (no decoders included)</li>\n<li>This model doesn't suffer from capturing context in longer sentences</li>\n<li>\n<p>This model can do the following tasks:</p>\n<ul>\n<li>Next sentence prediction</li>\n<li>Multi-mask language modeling:</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>... on the _ side _ history ... </mtext><mo>→</mo><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></mstyle></mstyle></mstyle></menclose><mo>→</mo><mtext> right, of</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{... on the \\_ side \\_ history ... } \\to \\boxed{Model} \\to \\text{ right, of}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.00444em;vertical-align:-0.31em;\"></span><span class=\"mord text\"><span class=\"mord\">... on the _ side _ history ... </span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.3744399999999999em;vertical-align:-0.3400000000000001em;\"></span><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0344399999999998em;\"><span style=\"top:-3.37444em;\"><span class=\"pstrut\" style=\"height:3.37444em;\"></span><span class=\"boxpad\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span style=\"top:-3.03444em;\"><span class=\"pstrut\" style=\"height:3.37444em;\"></span><span class=\"stretchy fbox\" style=\"height:1.3744399999999999em;border-style:solid;border-width:0.04em;\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3400000000000001em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\"> right, of</span></span></span></span></span></span>\n<ul>\n<li>\n<p>Around the start of 2020, T5 was introduced by Google</p>\n<ul>\n<li>This model is a bidirectional transformer</li>\n<li>Implying, words from the left and right are considered</li>\n<li>This model is able to entirely capture context words and relationships</li>\n<li>This model includes both an encoder and decoder</li>\n<li>This model doesn't suffer from capturing context in longer sentences</li>\n<li>\n<p>This model can do multi-task learning:</p>\n<ul>\n<li>This includes doing multiple different tasks on the same model</li>\n<li>Meaning, we can have a model perform text classification and question answering based on a given label input by the user</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Defining Bidirectional Encoder Representations</h3>\n<ul>\n<li>This model is also referred to as BERT</li>\n<li>As stated previous, it is a transformer model</li>\n<li>Specifically, it uses bidirectional attention for its attention mechanism</li>\n<li>It also uses positional embeddings to collect information about the positions between words</li>\n<li>Since it has already been pre-trained by Google, we can use BERT for transfer learning</li>\n<li>\n<p>At a high-level, the architecture is designed around the following steps:</p>\n<ol>\n<li>Input word embeddings into BERT</li>\n<li>Pass embeddings through to pre-trained encoder-decoder transformer blocks</li>\n<li>Receive output words as predictions</li>\n</ol>\n</li>\n<li>\n<p>The default BERT architecture has the following traits:</p>\n<ul>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">12</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span> layers (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">12</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span> transformers blocks)</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">12</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">2</span></span></span></span> attention heads</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>110</mn></mrow><annotation encoding=\"application/x-tex\">110</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mord\">0</span></span></span></span> million parameters</li>\n</ul>\n</li>\n</ul>\n<h3>Applications of the BERT Network</h3>\n<ul>\n<li>\n<p>Google pre-trained BERT using the following tasks:</p>\n<ul>\n<li>Masked language modeling</li>\n<li>Next sentence prediction</li>\n</ul>\n</li>\n<li>\n<p>Thus, if we're only interested in using the pre-trained network, we can do the following:</p>\n<ul>\n<li>Masked language modeling</li>\n<li>Next sentence prediction</li>\n</ul>\n</li>\n<li>\n<p>However, if we're interested in fine-tuning the pre-trained network, we can also do the following:</p>\n<ul>\n<li>Text classification (e.g. sentiment analysis)</li>\n<li>Named-entity recognition (NER)</li>\n<li>Multi-genre natural language inference (MNLI)</li>\n<li>Question answering (SQuAD)</li>\n<li>Sentence paraphrasing</li>\n<li>Text summarization (e.g. article summaries)</li>\n</ul>\n</li>\n<li>\n<p>We can see fine-tuning BERT has the following benefits:</p>\n<ul>\n<li>Less time required for training the fitted model</li>\n<li>Can effectively adjust the task using a smaller dataset</li>\n<li>Only requires minimal task-specific adjustments to use a wider variety of tasks</li>\n</ul>\n</li>\n</ul>\n<h3>Describing the Pre-Training Strategy for BERT</h3>\n<ul>\n<li>\n<p>BERT has been pre-trained on:</p>\n<ul>\n<li>\n<p>BooksCorpus data set (<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>11038</mn></mrow><annotation encoding=\"application/x-tex\">11038</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">3</span><span class=\"mord\">8</span></span></span></span> unpublished books)</p>\n<ul>\n<li>Containing <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>800</mn></mrow><annotation encoding=\"application/x-tex\">800</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">8</span><span class=\"mord\">0</span><span class=\"mord\">0</span></span></span></span> million words</li>\n</ul>\n</li>\n<li>\n<p>English Wikipedia data set</p>\n<ul>\n<li>Containing <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2500</mn></mrow><annotation encoding=\"application/x-tex\">2500</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span><span class=\"mord\">5</span><span class=\"mord\">0</span><span class=\"mord\">0</span></span></span></span> million words</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Pre-training BERT is composed of two tasks:</p>\n<ul>\n<li>\n<p>Masked language modeling</p>\n<ul>\n<li>This encodes bidirectional context for representing words</li>\n</ul>\n</li>\n<li>\n<p>Next sentence prediction</p>\n<ul>\n<li>This models the logical relationship between text pairs</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>By pre-training using the tasks above, the model attains a general sense of the language</li>\n<li>\n<p>When pre-training, masked language modeling involves the following:</p>\n<ul>\n<li>\n<p>Choose <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>15</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">15 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">1</span><span class=\"mord\">5</span><span class=\"mord\">%</span></span></span></span> of the tokens at random</p>\n<ul>\n<li>Mask them <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>80</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">80 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">8</span><span class=\"mord\">0</span><span class=\"mord\">%</span></span></span></span> of the time</li>\n<li>Replace them with a random token <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">10 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">%</span></span></span></span> of the time</li>\n<li>Keep the token as-is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10</mn><mi mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">10 \\%</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.80556em;vertical-align:-0.05556em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">%</span></span></span></span> of the time</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>There can be multiple masked spans in a sentence</li>\n</ul>\n<h3>Describing Input Representations for BERT</h3>\n<ul>\n<li>\n<p>The input of a BERT model depends on the task of interest</p>\n<ul>\n<li>\n<p>Tasks like masked language and next sentance modeling were pre-trained on BERT</p>\n<ul>\n<li>Thus, there isn't any fine-tuning necessary for training or testing our own next-sentance model with our own inputs</li>\n</ul>\n</li>\n<li>Tasks like sentiment analysis require some fine-tuning</li>\n<li>Regardless of task, the embeddings of the input sequence processed by BERT are the sum of the token embeddings, segment embeddings, and positional embeddings</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msubsup><mi>e</mi><mtext>bert</mtext><mrow><mo>&lt;</mo><mi>i</mi><mo>&gt;</mo></mrow></msubsup><mo>=</mo><msubsup><mi>e</mi><mtext>token</mtext><mrow><mo>&lt;</mo><mi>i</mi><mo>&gt;</mo></mrow></msubsup><mo>+</mo><msubsup><mi>e</mi><mtext>segment</mtext><mrow><mo>&lt;</mo><mi>i</mi><mo>&gt;</mo></mrow></msubsup><mo>+</mo><msubsup><mi>e</mi><mtext>positional</mtext><mrow><mo>&lt;</mo><mi>i</mi><mo>&gt;</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">e_{\\text{bert}}^{&lt;i&gt;} = e_{\\text{token}}^{&lt;i&gt;} + e_{\\text{segment}}^{&lt;i&gt;} + e_{\\text{positional}}^{&lt;i&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.135142em;vertical-align:-0.260478em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.874664em;\"><span style=\"top:-2.439522em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">bert</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&lt;</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">&gt;</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.260478em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.135142em;vertical-align:-0.260478em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.874664em;\"><span style=\"top:-2.439522em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">token</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&lt;</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">&gt;</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.260478em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2577720000000001em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.874664em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">segment</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&lt;</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">&gt;</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2712499999999998em;vertical-align:-0.396586em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-2.439522em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">positional</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&lt;</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mrel mtight\">&gt;</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.396586em;\"><span></span></span></span></span></span></span></span></span></span></span>\n</li>\n</ul>\n<h3>Defining Input Representations for BERT</h3>\n<ul>\n<li>\n<p>For tasks like masked language and next sentence modeling:</p>\n<ul>\n<li>Training can run on the pre-trained model without fine-tuning</li>\n<li>\n<p>The input is an array of tokenized words from two different sentences:</p>\n<ul>\n<li>Starting with a <code class=\"language-text\">&lt;cls&gt;</code> token</li>\n<li>Sentance <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> is separated by sentence <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> using a <code class=\"language-text\">&lt;sep&gt;</code> token</li>\n<li>Ending with a <code class=\"language-text\">&lt;sep&gt;</code> token</li>\n</ul>\n</li>\n<li>The following is an example of this input:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>&lt;cls&gt; </mtext><msub><mi>t</mi><mrow><mi>a</mi><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mrow><mi>a</mi><mn>2</mn></mrow></msub><mtext> &lt;sep&gt; </mtext><msub><mi>t</mi><mrow><mi>b</mi><mn>1</mn></mrow></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mrow><mi>b</mi><mn>2</mn></mrow></msub><mtext> &lt;sep&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;cls&gt; } t_{a1}, t_{a2} \\text{ &lt;sep&gt; } t_{b1}, t_{b2} \\text{ &lt;sep&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;cls&gt; </span></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord text\"><span class=\"mord\"> &lt;sep&gt; </span></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord text\"><span class=\"mord\"> &lt;sep&gt;</span></span></span></span></span></span>\n<ul>\n<li>Here, tokens from sentence <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">a</span></span></span></span> are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>t</mi><mrow><mi>a</mi><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{ai}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76508em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">a</span><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>And, tokens from sentence <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>t</mi><mrow><mi>b</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">t_{bj}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9011879999999999em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">b</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/9e1190b34d982d8bda9bfefedbf67d31/bertmaskedinput.svg\" alt=\"bertinput\"></p>\n<ul>\n<li>\n<p>For tasks like text classification and sentiment analysis:</p>\n<ul>\n<li>Training must run on the pre-trained model with fine-tuning</li>\n<li>\n<p>The input is an array of tokenized words from one sentence and its corresponding sentiment:</p>\n<ul>\n<li>Starting with a <code class=\"language-text\">&lt;cls&gt;</code> token</li>\n<li>A sentance is separated by its sentiment using a <code class=\"language-text\">&lt;sep&gt;</code> token</li>\n<li>Ending with a <code class=\"language-text\">&lt;sep&gt;</code> token</li>\n</ul>\n</li>\n<li>The following is an example of this input:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>&lt;cls&gt; </mtext><msub><mi>t</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>t</mi><mn>2</mn></msub><mtext> &lt;sep&gt; </mtext><msub><mi>s</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>s</mi><mn>2</mn></msub><mtext> &lt;sep&gt;</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{&lt;cls&gt; } t_{1}, t_{2} \\text{ &lt;sep&gt; } s_{1}, s_{2} \\text{ &lt;sep&gt;}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">&lt;cls&gt; </span></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord text\"><span class=\"mord\"> &lt;sep&gt; </span></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord text\"><span class=\"mord\"> &lt;sep&gt;</span></span></span></span></span></span>\n<ul>\n<li>Here, tokens from our sentence are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">t_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76508em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">t</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n<li>And, sentiments from our sentence are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></li>\n</ul>\n</li>\n</ul>\n<p><img src=\"/f36b3fd4e3bd49cc31df9ce9ea26bc4c/berttuneinput.svg\" alt=\"bertinput\"></p>\n<h3>Describing the Objective of BERT</h3>\n<ul>\n<li>For the multi-mask language model, a cross entropy loss function is used to predict the masked word or words</li>\n<li>For the next-sentence prediction model, a binary loss function is used to predict whether a given sentence should follow a target sentence</li>\n<li>Both of these loss outputs are added to each other to produce a final loss output</li>\n</ul>\n<h3>Defining the GLUE Benchmark</h3>\n<ul>\n<li>The GLUE benchmark stands for <em>general language understanding evaluation</em></li>\n<li>The GLUE benchmark is one of the most popular benchmarks in NLP</li>\n<li>It is used to train, test, and analyze NLP tasks</li>\n<li>\n<p>It is a collection of benchmark tools consisting of:</p>\n<ul>\n<li>A benchmark of nine different language comprehension tasks</li>\n<li>An ancillary data set</li>\n<li>A platform for evaluating and comparing the models</li>\n</ul>\n</li>\n<li>\n<p>It is used for various types of NLP tasks:</p>\n<ul>\n<li>Verifying whether a sentence is grammatical</li>\n<li>Verifying the accuracy of sentiment predictions</li>\n<li>Verifying the accuracy of paraphrasing text</li>\n<li>Verifying the similarity between two texts</li>\n<li>Verifying whether two questions are duplicates</li>\n<li>Verifying whether a question is answerable</li>\n<li>Verifying whether a question is a contradiction</li>\n</ul>\n</li>\n<li>\n<p>Usually, it is used with a leaderboard</p>\n<ul>\n<li>This is so people can see how well their model performs compared to other models on a dataset</li>\n</ul>\n</li>\n<li>\n<p>The GLUE benchmark has the following advantages:</p>\n<ul>\n<li>\n<p>The GLUE benchmark is model-agnostic</p>\n<ul>\n<li>Doesn't matter if we're evaluating a transformer or LSTM</li>\n</ul>\n</li>\n<li>Makes use of transfer learning</li>\n<li>Most research uses the GLUE benchmark as a standard</li>\n</ul>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://cs224d.stanford.edu/lectures/\" target=\"_blank\" rel=\"nofollow\">Stanford Deep Learning Lectures</a></li>\n<li><a href=\"http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\" target=\"_blank\" rel=\"nofollow\">Stanford Lecture about LSTMs</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/qMRXX/transfer-learning-in-nlp\" target=\"_blank\" rel=\"nofollow\">Lecture about Types of Transfer Learning</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/iPUp8/elmo-gpt-bert-t5\" target=\"_blank\" rel=\"nofollow\">Lecture about the History of Neural Networks in NLP</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/lZX7F/bidirectional-encoder-representations-from-transformers-bert\" target=\"_blank\" rel=\"nofollow\">Lecture about Defining the BERT Model</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/1g8LM/bert-objective\" target=\"_blank\" rel=\"nofollow\">Lecture about Intuition of BERT Tasks</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/EMBvt/fine-tuning-bert\" target=\"_blank\" rel=\"nofollow\">Lecture about BERT Applications</a></li>\n<li><a href=\"https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html\" target=\"_blank\" rel=\"nofollow\">Textbook Chapter about BERT</a></li>\n<li><a href=\"https://d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html\" target=\"_blank\" rel=\"nofollow\">Textbook Chapter about Pre-Training BERT</a></li>\n<li><a href=\"https://stats.stackexchange.com/a/193451\" target=\"_blank\" rel=\"nofollow\">Post about Pre-Training and Fine-Tuning Networks</a></li>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about Alignment and Attention Models</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/bert","previousSlug":"ml/nlp/transfer","nextSlug":"ml/nlp/t5","previousTitle":"Transfer Learning","nextTitle":"The T5 Model"}},"staticQueryHashes":[]}