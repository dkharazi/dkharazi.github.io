{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/data_trax","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Data Pipelines using Trax"},"html":"<h3>Motivating Padding in Neural Networks</h3>\n<ul>\n<li>When training NLP neural networks, training sets typically are separated into batches</li>\n<li>\n<p>Observations within a batch all must be the same length</p>\n<ul>\n<li>However, observations in different batches can be different lengths</li>\n</ul>\n</li>\n<li>\n<p>To do this, zeroes are appended to the end of each tensor until each tensor equals the length of the largest tensor within its batch</p>\n<ul>\n<li>Here, a tensor represent an encoded tweet, sentence, or any other sequence of words</li>\n</ul>\n</li>\n</ul>\n<h3>Motivating Bucketing in Neural Networks</h3>\n<ul>\n<li>When training on batches with padded tensors, the zeroes are ignored when gradients are computed</li>\n<li>\n<p>However, forward propagating over tensors with many zeroes in an RNN can slow down learning</p>\n<ul>\n<li>Sometimes, we can into vanishing gradients</li>\n</ul>\n</li>\n<li>In other words, training on padded tensors can impact the accuracy and performance</li>\n<li>Therefore, our goal is to minimize zero padding</li>\n</ul>\n<h3>Minimizing Zero Padding with Bucketing</h3>\n<ul>\n<li>Normally, batches are initialized by grouping observations at random</li>\n<li>As a result, we'll group together tensors of all different lengths</li>\n<li>This isn't an efficient approach for minimizing zero padding</li>\n<li>Instead, we can use a technique called <em>bucketing</em> to minimize zero padding</li>\n</ul>\n<h3>Describing Bucketing in Detail</h3>\n<ul>\n<li>Bucketing is performed on a group of batches</li>\n<li>Specifically, each batch is associated with a bucket</li>\n<li>Each bucket represents a range of lengths of tensors</li>\n<li>\n<p>A training set using batching and bucketing works in the following way:</p>\n<ul>\n<li>Each batch is assigned a <strong>batch size</strong> and <strong>bucket dimension</strong></li>\n<li>Each training tensor is assigned to a suitable batch</li>\n</ul>\n</li>\n<li>Specifically, a training sensor is assigned to a batch that satisfies a bucket dimension</li>\n</ul>\n<h3>Defining Bucketing Dimensions and Batch Size</h3>\n<ul>\n<li>A bucketing dimension refers to a range of suitable tensor lengths</li>\n<li>A batch size refers to the number of tensors that must be included in the batch for further training</li>\n<li>The number of buckets and size of buckets are adjustable hyperparameters</li>\n<li>The number of batches and the size of batches are adjustable hyperparameters</li>\n</ul>\n<h3>Sample Data Pipeline in Trax</h3>\n<ul>\n<li>Before training our neural network, we typically need to run word embedding functions on the training set</li>\n<li>\n<p>In particular, data pipelines are used for the following general reasons:</p>\n<ul>\n<li>Tokenizing and encoding raw data to tensors</li>\n<li>Shuffling tensors</li>\n<li>Filtering out tensors that don't satisfy a length requirement</li>\n<li>Padding tensors</li>\n<li>Bucketing training tensors into batches</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># tweets.txt</span>\n<span class=\"token comment\">#</span>\n<span class=\"token comment\"># good morning, 1</span>\n<span class=\"token comment\"># hello world, 1</span>\n<span class=\"token comment\"># please stop that, 0</span>\n<span class=\"token comment\"># thank you, 1</span>\n<span class=\"token comment\"># a b c d e f g h i j, 0</span>\n<span class=\"token comment\">#</span>\n\n<span class=\"token comment\"># Read in tuples of tweets and sentiments</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'tweets.txt'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    tweets <span class=\"token operator\">=</span> <span class=\"token builtin\">iter</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">tuple</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">','</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">','</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Initialize data pipeline for preprocessing raw data</span>\ndata_pipeline <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Tokenize<span class=\"token punctuation\">(</span>vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">,</span> keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Shuffle<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>FilterByLength<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>BucketByLength<span class=\"token punctuation\">(</span>boundaries<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">30</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>batch_sizes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>AddLossWeights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Run pipeline</span>\ntrain_batches_stream <span class=\"token operator\">=</span> data_pipeline<span class=\"token punctuation\">(</span>tweets<span class=\"token punctuation\">)</span>\nexample_batch <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>train_batches_stream<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Describing <code class=\"language-text\">Tokenize</code> in Trax</h3>\n<ul>\n<li>In Trax, the <code class=\"language-text\">Tokenize</code> class is used for a handful of embedding functions</li>\n<li>Specifically, tokenization only involves encoding sequences of words (e.g. sentences or tweets) as tensors (i.e. numerical vectors)</li>\n<li>The <code class=\"language-text\">Tokenize</code> class doesn't perform preprocessing functions</li>\n<li>\n<p>As a result, any of the following preprocessing functions need to be run before tokenization:</p>\n<ul>\n<li>Lowercasing words</li>\n<li>Removing stop words</li>\n<li>Removing symbols</li>\n</ul>\n</li>\n<li>\n<p>The <code class=\"language-text\">Tokenize</code> determines word embeddings based on the hash values of indices of words (or subwords) within a given vocabulary</p>\n<ul>\n<li>Again, it doesn't remove any capitalization, stop words, etc.</li>\n<li>The Trax library provides a comprehensive vocabulary called <code class=\"language-text\">en_8k.subwords</code></li>\n<li>This vocabulary will extract subwords from words and assign hash values to those subwords</li>\n<li>It also accounts for both lowercase and uppercase words</li>\n<li>A customeor personalized vocabulary can also be used in the <code class=\"language-text\">Tokenize</code> class</li>\n</ul>\n</li>\n<li>\n<p>The following table includes:</p>\n<ul>\n<li>Raw tweets</li>\n<li>Those tweets matched in the <code class=\"language-text\">en_8k.subword</code> vocab</li>\n<li>Final tokenized tweets after running the <code class=\"language-text\">Tokenize</code> class</li>\n</ul>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Tweet</th>\n<th>Matched with Vocab</th>\n<th>Tokenized Tensors</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>the good evening</td>\n<td>[the, good, eve, ning]</td>\n<td>[205, 2730, 4750, 1264]</td>\n</tr>\n<tr>\n<td>the Good Evening</td>\n<td>[the, Goo, d, Even, ing]</td>\n<td>[205, 5901, 20, 3207, 561]</td>\n</tr>\n<tr>\n<td>the good Evening</td>\n<td>[the, good, Even, ing]</td>\n<td>[205, 2730, 3207, 561]</td>\n</tr>\n</tbody>\n</table>\n<h3>Describing <code class=\"language-text\">FilterByLength</code> in Trax</h3>\n<ul>\n<li><code class=\"language-text\">FilterByLength</code> filters out the original tweets that are shorter than a given value representing the maximum character length</li>\n<li>Specifically, only tweets with fewer characters than the maximum value are kept</li>\n<li>The following table includes the input and output of the <code class=\"language-text\">FilterByLength</code> call with <code class=\"language-text\">max_length=10</code>:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>All Raw Tweets</th>\n<th>Length</th>\n<th>Remaining Tweets</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>hello world</td>\n<td>11</td>\n<td><em>filtered</em></td>\n</tr>\n<tr>\n<td>goodbye</td>\n<td>7</td>\n<td>goodbye</td>\n</tr>\n<tr>\n<td>hi there</td>\n<td>8</td>\n<td>hi there</td>\n</tr>\n<tr>\n<td>goodnight</td>\n<td>9</td>\n<td>goodnight</td>\n</tr>\n<tr>\n<td>good afternoon</td>\n<td>14</td>\n<td><em>filtered</em></td>\n</tr>\n</tbody>\n</table>\n<h3>Describing <code class=\"language-text\">BucketByLength</code> in Trax</h3>\n<ul>\n<li>\n<p>The <code class=\"language-text\">BucketByLength</code> class achieves the following:</p>\n<ol>\n<li>\n<p>First, tensors are bucketed into batches</p>\n<ul>\n<li>For each batch, tensors have a length within the same range</li>\n<li>In this scenario, a tensor represents a tokenized tweet</li>\n</ul>\n</li>\n<li>\n<p>Second, tensors are padded to match the length of the largest tensor within its batch</p>\n<ul>\n<li>Thus, all of the tensors in batch are the same length (based on a bucket dimension)</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Sample Implementation for Example</span>\ntrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>BucketByLength<span class=\"token punctuation\">(</span>\n    boundaries<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    batch_sizes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<table>\n<thead>\n<tr>\n<th>All Raw Tweets</th>\n<th>Sentiment</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>good morning</td>\n<td>1</td>\n</tr>\n<tr>\n<td>hello world</td>\n<td>1</td>\n</tr>\n<tr>\n<td>please stop that</td>\n<td>0</td>\n</tr>\n<tr>\n<td>thank you</td>\n<td>1</td>\n</tr>\n<tr>\n<td>a b c d e f g h i j</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>↓</mo></mrow><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mrel\">↓</span></span></span></span></span>\n<table>\n<thead>\n<tr>\n<th>Tensors</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>[121, 270]</td>\n</tr>\n<tr>\n<td>[41, 3]</td>\n</tr>\n<tr>\n<td>[740, 12, 601]</td>\n</tr>\n<tr>\n<td>[4, 360]</td>\n</tr>\n<tr>\n<td>[17, 2, 111, 731, 6, 98, 99, 119, 555, 90]</td>\n</tr>\n</tbody>\n</table>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mo>↓</mo></mrow><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mrel\">↓</span></span></span></span></span>\n<table>\n<thead>\n<tr>\n<th>Batch Index</th>\n<th>Batch</th>\n<th>Bucket Dimension</th>\n<th>Batch Size</th>\n<th>Is Padded</th>\n<th>Is Filtered</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>[[121, 270, 0], [41, 3, 0], [740, 12, 601]]</td>\n<td>1-5</td>\n<td>3</td>\n<td>Yes</td>\n<td>No</td>\n</tr>\n<tr>\n<td>2</td>\n<td>[[4, 360]]</td>\n<td>1-5</td>\n<td>3</td>\n<td>No</td>\n<td>Yes</td>\n</tr>\n<tr>\n<td>3</td>\n<td>[[17, 2, 111, 731, 6, 98, 99, 119, 555, 90]]</td>\n<td>6-10</td>\n<td>1</td>\n<td>No</td>\n<td>No</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Notice, each batch only contains tensors that are no larger than its associated bucket dimension</li>\n<li>\n<p>Notice, the tensors within a batch may be smaller than its associated bucket dimension</p>\n<ul>\n<li>This can happen if none of the tensors reach the exact bucket dimension</li>\n</ul>\n</li>\n<li>\n<p>Notice, a batch will be considered incomplete if it doesn't have the maximum number of tensors needed for its batch</p>\n<ul>\n<li>As a result, its tensors will be removed from any training</li>\n</ul>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://trax-ml.readthedocs.io/en/latest/trax.data.html\" target=\"_blank\" rel=\"nofollow\">Trax Data Documentation</a></li>\n<li><a href=\"https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976\" target=\"_blank\" rel=\"nofollow\">Article about Bucketing and Batching in Neural Networks</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/data_trax","previousSlug":"ml/nlp/layers_trax","nextSlug":"ml/nlp/train_trax","previousTitle":"Layers in Trax","nextTitle":"Supervised Training using Trax"}},"staticQueryHashes":[]}