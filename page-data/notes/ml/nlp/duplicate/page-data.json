{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/duplicate","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Duplicate Checking"},"html":"<h3>Duplicate Checking with Siamese Networks</h3>\n<ol>\n<li>\n<p>Load in the data:</p>\n<ul>\n<li>Suppose our data consists of Quora questions</li>\n<li>Prepare the training set so each question is paired with a duplicate or non-duplicate:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Question 1</th>\n<th>Question 2</th>\n<th>Is Duplicate?</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>What is your age?</td>\n<td>How old are you?</td>\n<td>true</td>\n</tr>\n<tr>\n<td>Where are you from?</td>\n<td>Where are you going?</td>\n<td>false</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>Gather questions into batches:</p>\n<ul>\n<li>Each question in batch <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> should pair with its duplicate question in batch <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span></li>\n<li>Note, there shouldn't be any duplicate questions within a batch</li>\n<li>Meaning, each question in an individual batch should be unique</li>\n<li>The following table illustrates these pairings:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Batch 1</th>\n<th>Batch 2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>What is your age?</td>\n<td>How old are you?</td>\n</tr>\n<tr>\n<td>Where are you from?</td>\n<td>Where were you born?</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>\n<p>Preprocess questions</p>\n<ul>\n<li>Maybe convert uppercase to lowercase</li>\n<li>Maybe remove symbols</li>\n<li>Maybe remove numbers</li>\n<li>Maybe remove stop words</li>\n<li>Maybe remove usernames</li>\n</ul>\n</li>\n<li>\n<p>Build a vocabulary:</p>\n<ul>\n<li>Parse each question by adding unique words to the vocabulary</li>\n<li>Again, preprocess the quesstion before building a vocabulary</li>\n<li>The vocabulary is built on training data</li>\n<li>There are many broad vocabularies already built for us</li>\n<li>These pre-built vocabularies may work for our specific use-case</li>\n<li>However, we may be more interested in building our own if our project requires a more individualized vocabulary</li>\n</ul>\n</li>\n<li>\n<p>Convert preprocessed questions to tensors:</p>\n<ul>\n<li>After preprocessing data, we tokenize questions into tensors</li>\n<li>These tensors may have padding based on the length of the longest question within the batch</li>\n<li>Usually, the tensor values are hashes of each word's index in the vocabulary</li>\n</ul>\n</li>\n<li>\n<p>Build supervised model with embedding layer</p>\n<ul>\n<li>\n<p>A typical duplicate checking system includes an initial word embedding layer</p>\n<ul>\n<li>An embedding layer is a trainable layer with weights</li>\n<li>Embedding layers are commonly used to map discrete vectors representing an individual question to smaller vectors</li>\n<li>The values in word embeddings usually represent some learned contextual value</li>\n<li>These values become the weights in the embedding layer</li>\n</ul>\n</li>\n<li>\n<p>The output value can be:</p>\n<ul>\n<li>A normalized vector of the LSTM network</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Train the model</p>\n<ul>\n<li>\n<p>Train the model using:</p>\n<ul>\n<li>Optimizers like Adam</li>\n<li>Batching techniques like bucketing</li>\n<li>etc.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Pass the two questions into the same LSTM model:</li>\n<li>Compute a cosine similarity score on the normalized output of the two questions</li>\n<li>\n<p>Test the model</p>\n<ul>\n<li>Convert each input into an array of numbers</li>\n<li>Feed the inputs into our model</li>\n<li>Compare the outputs <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">v_{1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">v_{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> using cosine similarity</li>\n<li>Test against a threshold <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>τ</mi></mrow><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.1132em;\">τ</span></span></span></span></li>\n</ul>\n</li>\n</ol>\n<h3>General Implementation of Duplicate Checking</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 1. Split data into training set</span>\ntrain_x<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> test_x<span class=\"token punctuation\">,</span> test_y <span class=\"token operator\">=</span> load_questions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 2. Pair questions into duplicates</span>\ntrain_x<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> test_x<span class=\"token punctuation\">,</span> test_y <span class=\"token operator\">=</span> pair_questions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 3. Preprocess data</span>\ntrain_x<span class=\"token punctuation\">,</span> test_x <span class=\"token operator\">=</span> preprocess_questions<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 4. Create Vocabulary</span>\nvocab <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">'__PAD__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'__&lt;/e>__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'__UNK__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">for</span> question <span class=\"token keyword\">in</span> train_x<span class=\"token punctuation\">:</span>\n\tprocessed_question <span class=\"token operator\">=</span> process_question<span class=\"token punctuation\">(</span>question<span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> processed_question<span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">if</span> word <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> vocab<span class=\"token punctuation\">:</span>\n\t\t\tvocab<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 5a. Initialize tokenizer for quesstions</span>\ndata_pipeline <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Tokenize<span class=\"token punctuation\">(</span>vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">,</span> keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Shuffle<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>FilterByLength<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>BucketByLength<span class=\"token punctuation\">(</span>boundaries<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">30</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>batch_sizes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>AddLossWeights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\t<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 5b. Tokenize and batch tweets, then output as generator</span>\nstreamed_batches <span class=\"token operator\">=</span> data_pipeline<span class=\"token punctuation\">(</span>questions<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 6a. Initiailize normalization function for output layer in LSTM</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">normalize</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>  <span class=\"token comment\"># normalizes the vectors to have L2 norm 1</span>\n    <span class=\"token keyword\">return</span> x <span class=\"token operator\">/</span> fastnp<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>fastnp<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">*</span> x<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keepdims<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 6b. Initialize model</span>\nmodel <span class=\"token operator\">=</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n    tl<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token operator\">=</span><span class=\"token number\">8192</span><span class=\"token punctuation\">,</span> d_feature<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    tl<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>d_model<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># LSTM layes</span>\n    tl<span class=\"token punctuation\">.</span>Mean<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>    <span class=\"token comment\"># Average on axis 1 (length of sentence)</span>\n    tl<span class=\"token punctuation\">.</span>Fn<span class=\"token punctuation\">(</span><span class=\"token string\">'Normalize'</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">lambda</span> x<span class=\"token punctuation\">:</span> normalize<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Apply normalization</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7a. Initialize triplet loss function</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">TripletLoss</span><span class=\"token punctuation\">(</span>margin<span class=\"token operator\">=</span><span class=\"token number\">0.25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    triplet_loss_fn <span class=\"token operator\">=</span> partial<span class=\"token punctuation\">(</span>TripletLossFn<span class=\"token punctuation\">,</span> margin<span class=\"token operator\">=</span>margin<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> tl<span class=\"token punctuation\">.</span>Fn<span class=\"token punctuation\">(</span><span class=\"token string\">'TripletLoss'</span><span class=\"token punctuation\">,</span> triplet_loss_fn<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7b. Compute cosine similarity for triplet loss</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">TripletLossFn</span><span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">,</span> v2<span class=\"token punctuation\">,</span> margin<span class=\"token operator\">=</span><span class=\"token number\">0.25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    scores <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">,</span>v2<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># pariwise cosine similarity</span>\n\n    batch_size <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>scores<span class=\"token punctuation\">)</span>\n    positive <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>diagonal<span class=\"token punctuation\">(</span>scores<span class=\"token punctuation\">)</span>\n    negative_without_positive <span class=\"token operator\">=</span> scores <span class=\"token operator\">-</span> fastnp<span class=\"token punctuation\">.</span>eye<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2.0</span> \n    closest_negative <span class=\"token operator\">=</span> negative_without_positive<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>axis <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    negative_zero_on_duplicate <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1.0</span> <span class=\"token operator\">-</span> fastnp<span class=\"token punctuation\">.</span>eye<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> scores\n    mean_negative <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span>negative_zero_on_duplicate<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span>batch_size <span class=\"token operator\">-</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n\n    triplet_loss1 <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>maximum<span class=\"token punctuation\">(</span>margin <span class=\"token operator\">-</span> positive <span class=\"token operator\">+</span> closest_negative<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token punctuation\">)</span>\n    triplet_loss2 <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>maximum<span class=\"token punctuation\">(</span>margin <span class=\"token operator\">-</span> positive <span class=\"token operator\">+</span> mean_negative<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token punctuation\">)</span>\n    triplet_loss <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span>triplet_loss1 <span class=\"token operator\">+</span> triplet_loss2<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">return</span> triplet_loss\n\n<span class=\"token comment\"># 7c. Initialize training task with TripletLoss</span>\ntrain_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>TrainTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_batches<span class=\"token punctuation\">,</span>\n    loss_layer<span class=\"token operator\">=</span>tl<span class=\"token punctuation\">.</span>TripletLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    optimizer<span class=\"token operator\">=</span>trax<span class=\"token punctuation\">.</span>optimizers<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    n_steps_per_checkpoint<span class=\"token operator\">=</span><span class=\"token number\">500</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7d. Initialize evaluation task</span>\neval_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>EvalTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_eval_batches<span class=\"token punctuation\">,</span>\n    metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>tl<span class=\"token punctuation\">.</span>WeightedCategoryCrossEntropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>WeightedCategoryAccuracy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    n_eval_batches<span class=\"token operator\">=</span><span class=\"token number\">20</span>  <span class=\"token comment\"># For less variance in eval numbers</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7e. Prepare training loop and saving checkpoints to output_dir</span>\ntraining_loop <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>Loop<span class=\"token punctuation\">(</span>\n    model<span class=\"token punctuation\">,</span>\n    train_task<span class=\"token punctuation\">,</span>\n    eval_tasks<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>eval_task<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    output_dir<span class=\"token operator\">=</span><span class=\"token string\">'~/output_file.txt'</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7f. Run 2000 steps (batches)</span>\ntraining_loop<span class=\"token punctuation\">.</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">2000</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 8a. Initialize prediction function for duplicates</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">predict</span><span class=\"token punctuation\">(</span>question1<span class=\"token punctuation\">,</span> question2<span class=\"token punctuation\">,</span> threshold<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> vocab<span class=\"token punctuation\">,</span> data_generator<span class=\"token punctuation\">,</span> verbose<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    q1 <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>question1<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># tokenize</span>\n    q2 <span class=\"token operator\">=</span> nltk<span class=\"token punctuation\">.</span>word_tokenize<span class=\"token punctuation\">(</span>question2<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># tokenize</span>\n    Q1<span class=\"token punctuation\">,</span> Q2 <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> q1<span class=\"token punctuation\">:</span>  <span class=\"token comment\"># encode q1</span>\n        Q1 <span class=\"token operator\">+=</span> <span class=\"token punctuation\">[</span>vocab<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> q2<span class=\"token punctuation\">:</span>  <span class=\"token comment\"># encode q2</span>\n        Q2 <span class=\"token operator\">+=</span> <span class=\"token punctuation\">[</span>vocab<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n        \n    <span class=\"token comment\"># Call the data generator (built in Ex 01) using next()</span>\n    Q1<span class=\"token punctuation\">,</span> Q2 <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>data_generator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>Q1<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>Q2<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> vocab<span class=\"token punctuation\">[</span><span class=\"token string\">'&lt;PAD>'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    v1<span class=\"token punctuation\">,</span> v2 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>Q1<span class=\"token punctuation\">,</span>Q2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># compute cosine similarity</span>\n    d <span class=\"token operator\">=</span> fastnp<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>v1<span class=\"token punctuation\">,</span> v2<span class=\"token punctuation\">.</span>T<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># is d greater than the threshold?</span>\n    res <span class=\"token operator\">=</span> d <span class=\"token operator\">></span> threshold\n    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>verbose<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Q1  = \"</span><span class=\"token punctuation\">,</span> Q1<span class=\"token punctuation\">,</span> <span class=\"token string\">\"\\nQ2  = \"</span><span class=\"token punctuation\">,</span> Q2<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"d   = \"</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"res = \"</span><span class=\"token punctuation\">,</span> res<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> res\n\n<span class=\"token comment\"># 8b. Predict duplicates on test data</span>\nmodel <span class=\"token operator\">=</span> Siamese<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nquestion1 <span class=\"token operator\">=</span> <span class=\"token string\">\"Do they enjoy eating the dessert?\"</span>\nquestion2 <span class=\"token operator\">=</span> <span class=\"token string\">\"Do they like hiking in the desert?\"</span>\n<span class=\"token comment\"># 1 means it is duplicated, 0 otherwise</span>\npredict<span class=\"token punctuation\">(</span>question1 <span class=\"token punctuation\">,</span> question2<span class=\"token punctuation\">,</span> <span class=\"token number\">0.7</span><span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> vocab<span class=\"token punctuation\">,</span> data_generator<span class=\"token punctuation\">,</span> verbose<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://cs224d.stanford.edu/lectures/\" target=\"_blank\" rel=\"nofollow\">Stanford Deep Learning Lectures</a></li>\n<li><a href=\"http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\" target=\"_blank\" rel=\"nofollow\">Stanford Lecture about LSTMs</a></li>\n<li><a href=\"https://www.coursera.org/learn/sequence-models-in-nlp/lecture/KDqML/training-testing\" target=\"_blank\" rel=\"nofollow\">Lecture about Training Siamese Networks</a></li>\n<li><a href=\"https://zhangruochi.com/Question-duplicates/2020/08/23/\" target=\"_blank\" rel=\"nofollow\">Implementation of a Siamese Network in Trax</a></li>\n<li><a href=\"https://github.com/ThinamXx/DuplicateQuestions__Recognition\" target=\"_blank\" rel=\"nofollow\">Another Implementation of a Siamese Network in Trax</a></li>\n<li><a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about Siamese Networks used in Image Recognition</a></li>\n<li><a href=\"mathworks.com/help/deeplearning/ug/train-a-siamese-network-to-compare-images.html\">Applications of Siamese Networks</a></li>\n<li><a href=\"https://bdtechtalks.com/2020/08/12/what-is-one-shot-learning/\" target=\"_blank\" rel=\"nofollow\">Article about One-Shot Learning</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/duplicate","previousSlug":"ml/nlp/sentiment","nextSlug":"ml/nlp/translation","previousTitle":"Sentiment Analysis","nextTitle":"Neural Machine Translation"}},"staticQueryHashes":[]}