{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/embedding","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Word Embeddings"},"html":"<h3>Introducing Word Embeddings</h3>\n<ul>\n<li>A word embedding is a vectorial representation of a word</li>\n<li>A word embedding is made up of trainable similarity values</li>\n<li>The length of a word embedding is the number of words in our vocabulary</li>\n<li>\n<p>The number of word embeddings is an adjustable hyperparameter</p>\n<ul>\n<li>It refers to the number of contextual traits we're interested in learning about our words</li>\n</ul>\n</li>\n<li>In the following example, we only learn <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span> word embeddings:</li>\n</ul>\n<p><img src=\"/ecc71bb7c9e227b292dd909b02dbf4e8/embedding.svg\" alt=\"embeddings\"></p>\n<ul>\n<li>\n<p>A word embedding can have any of the following interpretations:</p>\n<ul>\n<li>It uncovers a semantic meaning of the words in our vocabulary</li>\n<li>It approximates the similarity of certain words in our vocabulary</li>\n<li>It represents a hidden semantic relationship</li>\n</ul>\n</li>\n<li>\n<p>For example, a word embedding may have similarly large values for:</p>\n<ul>\n<li>Kitten and Cat</li>\n<li>Pizza and Sandwich</li>\n<li>House and home</li>\n</ul>\n</li>\n<li>\n<p>Specifically, a word embedding has the following properties:</p>\n<ul>\n<li>It is a vector of (non-binary) numeric values</li>\n<li>It has a size equal to the number of words in the vocabulary</li>\n</ul>\n</li>\n</ul>\n<h3>Motivating Word Embeddings</h3>\n<ul>\n<li>Machine learning algorithms require words to be in vector format</li>\n<li>So, we must find a way to convert free-text words into numeric values</li>\n<li>One of the simplest conversion techniques is one-hot encoding:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Corpus</mtext><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>banana</mtext><mo separator=\"true\">,</mo><mtext>orange</mtext><mo separator=\"true\">,</mo><mtext>yellow</mtext><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Corpus} = [\\text{banana}, \\text{orange}, \\text{yellow}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">Corpus</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">banana</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">orange</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">yellow</span></span><span class=\"mclose\">]</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Vocab</mtext><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>banana</mtext><mo separator=\"true\">,</mo><mtext>orange</mtext><mo separator=\"true\">,</mo><mtext>yellow</mtext><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Vocab} = [\\text{banana}, \\text{orange}, \\text{yellow}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord text\"><span class=\"mord\">Vocab</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">banana</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">orange</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">yellow</span></span><span class=\"mclose\">]</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>e</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>d</mi><mi>e</mi><mo stretchy=\"false\">(</mo><mtext>Vocab</mtext><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">encode(\\text{Vocab}) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">e</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">Vocab</span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.6010299999999997em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.0510099999999998em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎣</span></span></span><span style=\"top:-2.8099900000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎢</span></span></span><span style=\"top:-4.05101em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎡</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.0510099999999998em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎦</span></span></span><span style=\"top:-2.8099900000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎥</span></span></span><span style=\"top:-4.05101em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎤</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span></span></span></span></span></span>\n<ul>\n<li>\n<p>The one-hot encoding method is inefficient for two related reasons:</p>\n<ul>\n<li>The vectors become sparse</li>\n<li>The number of dimensions grows very large</li>\n<li>They lack any semantic meaning</li>\n</ul>\n</li>\n<li>Instead, word embeddings can be learned</li>\n<li>\n<p>Then, they can maintain the following traits:</p>\n<ul>\n<li>They aren't sparse</li>\n<li>The number of dimensions are adjustable</li>\n<li>They carry some semantic meaning</li>\n</ul>\n</li>\n<li>Then, a word embedding becomes an efficient representation of the words in a vocabulary</li>\n<li>The following is an example of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span> word embeddings:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Corpus</mtext><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>banana</mtext><mo separator=\"true\">,</mo><mtext>orange</mtext><mo separator=\"true\">,</mo><mtext>yellow</mtext><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Corpus} = [\\text{banana}, \\text{orange}, \\text{yellow}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">Corpus</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">banana</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">orange</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">yellow</span></span><span class=\"mclose\">]</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Vocab</mtext><mo>=</mo><mo stretchy=\"false\">[</mo><mtext>banana</mtext><mo separator=\"true\">,</mo><mtext>orange</mtext><mo separator=\"true\">,</mo><mtext>yellow</mtext><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Vocab} = [\\text{banana}, \\text{orange}, \\text{yellow}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord text\"><span class=\"mord\">Vocab</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord text\"><span class=\"mord\">banana</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">orange</span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord text\"><span class=\"mord\">yellow</span></span><span class=\"mclose\">]</span></span></span></span></span>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>e</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mo stretchy=\"false\">(</mo><mtext>Vocab</mtext><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.9</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.7</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.8</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.2</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0.8</mn></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">embed(\\text{Vocab}) = \\begin{bmatrix} 0.9 &amp; 0.7 \\\\ 0.8 &amp; 0.2 \\\\ 0.1 &amp; 0.8 \\end{bmatrix}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">m</span><span class=\"mord mathnormal\">b</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">Vocab</span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.6010299999999997em;vertical-align:-1.55002em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.0510099999999998em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎣</span></span></span><span style=\"top:-2.8099900000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎢</span></span></span><span style=\"top:-4.05101em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎡</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">9</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">8</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.21em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">7</span></span></span><span style=\"top:-3.0099999999999993em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">2</span></span></span><span style=\"top:-1.8099999999999994em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"mord\">.</span><span class=\"mord\">8</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5500000000000007em;\"><span></span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.0510099999999998em;\"><span style=\"top:-2.2500000000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎦</span></span></span><span style=\"top:-2.8099900000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎥</span></span></span><span style=\"top:-4.05101em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎤</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55002em;\"><span></span></span></span></span></span></span></span></span></span></span></span>\n<h3>Describing Word Embeddings</h3>\n<ul>\n<li>As stated already, the size of a word embedding equals the number of words in the vocabulary</li>\n<li>The number of word embeddings is an adjustable hyperparameter</li>\n<li>Adding additional word embeddings to a model will capture more context about the words</li>\n<li>However, adding additional word embeddings will slow down training</li>\n<li>Contextual information is useful for learning meaning and relationships of words</li>\n<li>This is because similar words typically appear in similar context</li>\n<li>\n<p>In general, there are two approaches for learning word embedding:</p>\n<ul>\n<li>Count-Based Models</li>\n<li>Context-Based Models</li>\n</ul>\n</li>\n</ul>\n<h3>Describing Count-Based Models</h3>\n<ul>\n<li>These models rely on a co-occurrence (or word frequency) matrix</li>\n<li>It assumes that words in the same contexts share similar or related semantic meanings</li>\n<li>The model maps co-occurrences between neighboring words down to small and dense word vectors</li>\n<li>\n<p>The following are examples of count-based models:</p>\n<ul>\n<li>PCA</li>\n<li>Topic models</li>\n<li>Neural probabilistic language models</li>\n</ul>\n</li>\n</ul>\n<h3>Describing Context-Based Models</h3>\n<ul>\n<li>Context-based models predict a word given its neighbors</li>\n<li>The weights in the word vectors are model parameters</li>\n<li>As a result, the best vector representation of each word is learned during the model training process</li>\n</ul>\n<h3>Defining a Skip-Gram Context-Based Model</h3>\n<ul>\n<li>\n<p>There is a sliding window of fixed size moving along a sentence</p>\n<ul>\n<li>Where, the word in the middle is the <em>target</em></li>\n<li>Where those words on its left and right within the sliding window are the <em>context words</em></li>\n</ul>\n</li>\n<li>The skip-gram model is trained to predict the probabilities of a word being a context word for a given target</li>\n<li>\n<p>The following example includes observations with:</p>\n<ul>\n<li>A target word</li>\n<li>The associated context words as training samples</li>\n<li>The associated 5-word sliding window along the sentence</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>The man who passes the sentence should swing the sword.</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Sliding Window (n=5)</th>\n<th>Target</th>\n<th>Context</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>[the, man, who]</td>\n<td>[the]</td>\n<td>[man, who]</td>\n</tr>\n<tr>\n<td>[the, man, who, passes]</td>\n<td>[man]</td>\n<td>[the, who, passes]</td>\n</tr>\n<tr>\n<td>[the, man, who, passes, the]</td>\n<td>[who]</td>\n<td>[the, man, passes, the]</td>\n</tr>\n<tr>\n<td>[man, who, passes, the, sentence]</td>\n<td>[passes]</td>\n<td>[the, man, the, sentence]</td>\n</tr>\n<tr>\n<td>...</td>\n<td>...</td>\n<td>...</td>\n</tr>\n<tr>\n<td>[sentence, should, swing, the, sword]</td>\n<td>[swing]</td>\n<td>[sentence, should, the, sword]</td>\n</tr>\n<tr>\n<td>[should, swing, the, sword]</td>\n<td>[the]</td>\n<td>[should, swing, sword]</td>\n</tr>\n<tr>\n<td>[swing, the, sword]</td>\n<td>[sword]</td>\n<td>[swing, the]</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>Each context-target pair is treated as a new observation in the data</li>\n<li>\n<p>For example, the target word <em>swing</em> produces four training samples:</p>\n<ul>\n<li>(<em>swing</em>, <em>sentence</em>)</li>\n<li>(<em>swing</em>, <em>should</em>)</li>\n<li>(<em>swing</em>, <em>the</em>)</li>\n<li>(<em>swing</em>, <em>sword</em>)</li>\n</ul>\n</li>\n</ul>\n<h3>Architecture of Skip-Gram Context-Based Model</h3>\n<ul>\n<li>We're given a vocabulary size <strong>V</strong></li>\n<li>We learn word embedding vectors of size <strong>N</strong></li>\n<li>The model learns to predict one context word (output)</li>\n<li>To do this, the model uses one target word (input) at a time</li>\n</ul>\n<p><img src=\"/200be2d4811abd39974f113db9e8047f/skipgram.png\" alt=\"word2vec\"></p>\n<ul>\n<li>Both input word <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> and the output word <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{j}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> are one-hot encoded into binary vectors <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>y</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{j}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> of size <strong>V</strong></li>\n<li>Here, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> represents the word embedding matrix of size <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">V \\times N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></li>\n<li>\n<p>Multiplying <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> outputs the embedding vector of the input word <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></p>\n<ul>\n<li>Located in the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">i^{th}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.849108em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">i</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.849108em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mord mathnormal mtight\">h</span></span></span></span></span></span></span></span></span></span></span></span> row of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></li>\n<li>The goal of this weight matrix is to squeeze the one-hot encoded vectors into a lower dimensional representation</li>\n</ul>\n</li>\n<li>\n<p>This newly discovered embedding vector forms the hidden layer</p>\n<ul>\n<li>This embedding vector has <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> dimensions</li>\n</ul>\n</li>\n<li>\n<p>Now, we have another word context matrix <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">W&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> of size <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">N \\times V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.76666em;vertical-align:-0.08333em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></p>\n<ul>\n<li>This matrix encodes the meanings of words as context</li>\n<li>The goal of this weight matrix is to provide context to the word embeddings</li>\n</ul>\n</li>\n<li>Multiplying the hidden layer and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">W&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> outputs the one-hot encoded vector <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span></span></span></span></li>\n<li>Note, the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">W&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> try to achieve different things</li>\n</ul>\n<h3>Defining a Continuous Bag-of-Words Context-Based Model</h3>\n<ul>\n<li>The Continuous Bag-of-Words (CBOW) also learns word vectors</li>\n<li>Recall, the skip-gram model predicts context words given a target</li>\n<li>Inversely, the CBOW model predicts a target word given context words</li>\n</ul>\n<p><img src=\"/b804d3b78d206c9d47250808a84f953c/cbow.png\" alt=\"cbow\"></p>\n<ul>\n<li>Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer</li>\n<li>In other words, we average the word vectors, since there are multiple contextual words <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span></li>\n<li>Specifically, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02691em;\">w</span></span></span></span> is the input vector and the matrix <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span> multiplied together</li>\n<li>Because the averaging stage smoothes over a lot of the distributional information, some people believe the CBOW model is can perform as well on a small dataset</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\" target=\"_blank\" rel=\"nofollow\">Learning Word Embedding Blog Post</a></li>\n<li><a href=\"https://arxiv.org/pdf/1301.3781.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about Word Representations in Vector Space</a></li>\n<li><a href=\"https://datascience.stackexchange.com/a/29161/93566\" target=\"_blank\" rel=\"nofollow\">Difference between Weight Matrices in Word2Vec</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/embedding","previousSlug":"ml/nlp/train_trax","nextSlug":"ml/nlp/rnn","previousTitle":"Supervised Training using Trax","nextTitle":"Recurrent Neural Networks"}},"staticQueryHashes":[]}