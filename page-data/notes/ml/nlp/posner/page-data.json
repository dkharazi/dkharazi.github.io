{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/posner","result":{"data":{"markdownRemark":{"frontmatter":{"title":"POS Tagging and NER"},"html":"<h3>Describing Part-of-Speech Tagging</h3>\n<ul>\n<li>Part-of-Speeach (or POS) tagging is used for determining whether or not a given word belongs to part-of-speech (or some grammatical group)</li>\n<li>Specifically, POS tagging typically refers to determining whether a word is a noun, verb, adjective, etc.</li>\n<li>For example, the sentance <code class=\"language-text\">Manchester United is looking to sign Harry Kane for $90 million dollars</code> could return the following using POS tagging</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Word</th>\n<th>POS</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Manchester</td>\n<td>PRON</td>\n</tr>\n<tr>\n<td>United</td>\n<td>ADJ</td>\n</tr>\n<tr>\n<td>is</td>\n<td>VERB</td>\n</tr>\n<tr>\n<td>looking</td>\n<td>VERB</td>\n</tr>\n<tr>\n<td>to</td>\n<td>PREP</td>\n</tr>\n<tr>\n<td>sign</td>\n<td>VERB</td>\n</tr>\n<tr>\n<td>Harry</td>\n<td>PRON</td>\n</tr>\n<tr>\n<td>Kane</td>\n<td>PRON</td>\n</tr>\n<tr>\n<td>for</td>\n<td>PREP</td>\n</tr>\n<tr>\n<td>$</td>\n<td>PUNCT</td>\n</tr>\n<tr>\n<td>90</td>\n<td>NUM</td>\n</tr>\n<tr>\n<td>million</td>\n<td>NUM</td>\n</tr>\n<tr>\n<td>dollars</td>\n<td>NOUN</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>We can see that POS tagging determines our results by taking in the context of the entire sentance</li>\n<li>Therefore, POS is more of a global problem, since there can be relationships between the first and last word of a sentance</li>\n</ul>\n<h3>Describing Named Entity Recognition</h3>\n<ul>\n<li>Named Entity Recognition (or NER) is used for determining whether or not a given word belongs to a named entity</li>\n<li>Named entities refer to people, locations, organizations, time expressions, etc.</li>\n<li>Specifically, NER tagging typically refers to determining whether a word is in some general group</li>\n<li>For example, the sentance <code class=\"language-text\">Manchester United is looking to sign Harry Kane for $90 million dollars</code> could return the following using NER:</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Phrase</th>\n<th>NE</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Manchester United</td>\n<td>ORG</td>\n</tr>\n<tr>\n<td>Harry Kane</td>\n<td>PERSON</td>\n</tr>\n<tr>\n<td>$90 million dollars</td>\n<td>MONEY</td>\n</tr>\n</tbody>\n</table>\n<ul>\n<li>\n<p>This problem can be broken down into the following steps:</p>\n<ul>\n<li>Detect of names (i.e. distinguish between two different words Manchester and United versus one word Manchester United)</li>\n<li>Classify names into the corresponding categories (i.e. need to manually determine which words belong to which category beforehand)</li>\n</ul>\n</li>\n<li>Therefore, NER is more of a local problem, since we only really need to look at surrounding words when recognizing a named entity</li>\n</ul>\n<h3>Applications of NER Systems</h3>\n<ul>\n<li>Customer service</li>\n<li>\n<p>Search engine efficiency</p>\n<ul>\n<li>Web scrapes millions of web sites</li>\n<li>Using NER to extract and store these entities</li>\n<li>These tags are compared to the search tags</li>\n</ul>\n</li>\n<li>\n<p>Recommendation engines</p>\n<ul>\n<li>Using NER to extract tags from your history</li>\n<li>Compare these tags to tags of interest</li>\n</ul>\n</li>\n<li>\n<p>Automatic trading</p>\n<ul>\n<li>Web scraping for information about publicly traded companies</li>\n<li>Using NER to extract text about the CEO of each company</li>\n<li>Performing sentiment analysis on the text about CEOs</li>\n</ul>\n</li>\n</ul>\n<h3>Named Entity Recognition with LSTM</h3>\n<ol>\n<li>\n<p>Initialize a dictionary of entities:</p>\n<ul>\n<li>This involves assigning entity classes with a unique number</li>\n<li>For example, a filler may be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, a personal name may be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>, a geographical location may be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span>, and a time indicator may be <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span></li>\n</ul>\n</li>\n<li>\n<p>Load in the data:</p>\n<ul>\n<li>Suppose each observation is a tweet</li>\n</ul>\n</li>\n<li>\n<p>Preprocess tweets:</p>\n<ul>\n<li>Maybe convert uppercase to lowercase</li>\n<li>Maybe remove symbols</li>\n<li>Maybe remove numbers</li>\n<li>Maybe remove stop words</li>\n<li>Maybe remove usernames</li>\n</ul>\n</li>\n<li>\n<p>Build a vocabulary:</p>\n<ul>\n<li>Parse each tweet by adding unique words to the vocabulary</li>\n<li>Again, preprocess the tweet before building a vocabulary</li>\n<li>The vocabulary is built on training data</li>\n<li>There are many broad vocabularies already built for us</li>\n<li>These pre-built vocabularies may work for our specific use-case</li>\n<li>However, we may be more interested in building our own if our project requires a more individualized vocabulary</li>\n</ul>\n</li>\n<li>\n<p>Convert preprocessed tweets to tensors:</p>\n<ul>\n<li>After preprocessing data, we tokenize tweets into tensors</li>\n<li>These tensors may have padding based on the length of the longest tweet within the batch</li>\n<li>Usually, the tensor values are hashes of each word's index in the vocabulary</li>\n<li>The following tweet maps to a tensor:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>i flew to boston last monday</mtext><mo>→</mo><mo stretchy=\"false\">[</mo><mn>362</mn><mo separator=\"true\">,</mo><mn>89</mn><mo separator=\"true\">,</mo><mn>103</mn><mo separator=\"true\">,</mo><mn>4406</mn><mo separator=\"true\">,</mo><mn>12</mn><mo separator=\"true\">,</mo><mn>908</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{i flew to boston last monday} \\to [362, 89, 103, 4406, 12, 908]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">i flew to boston last monday</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">3</span><span class=\"mord\">6</span><span class=\"mord\">2</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">8</span><span class=\"mord\">9</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">0</span><span class=\"mord\">3</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">4</span><span class=\"mord\">4</span><span class=\"mord\">0</span><span class=\"mord\">6</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mord\">2</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">9</span><span class=\"mord\">0</span><span class=\"mord\">8</span><span class=\"mclose\">]</span></span></span></span></span>\n</li>\n<li>\n<p>Build supervised model with embedding layer:</p>\n<ul>\n<li>\n<p>A typical NER system includes an initial word embedding layer</p>\n<ul>\n<li>An embedding layer is a trainable layer with weights</li>\n<li>Embedding layers are commonly used to map discrete vectors representing an individual tweet to smaller vectors</li>\n<li>The values in word embeddings usually represent some learned contextual value</li>\n<li>These valuess become the weights in the embedding layer</li>\n</ul>\n</li>\n<li>\n<p>The output value is a softmax value:</p>\n<ul>\n<li>This predicts the probability a word is associated with each of the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> number of classes</li>\n<li>\n<p>The <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span> number of classes is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">4</span></span></span></span>:</p>\n<ul>\n<li>Filler</li>\n<li>Personal name</li>\n<li>Geographical location</li>\n<li>Time indicator</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Train the model</p>\n<ul>\n<li>\n<p>Train the model using:</p>\n<ul>\n<li>Optimizers like Adam</li>\n<li>Batching techniques like bucketing</li>\n<li>etc.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Test the model</p>\n<ul>\n<li>Using new data, predict the probabilities of a word in a sentence being associated to each class</li>\n</ul>\n</li>\n</ol>\n<h3>General Implementation of NER</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 1. Initialize a dictionary of entities</span>\nentities <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n\t<span class=\"token number\">0</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'filler'</span><span class=\"token punctuation\">,</span>\n\t<span class=\"token number\">1</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'personal name'</span><span class=\"token punctuation\">,</span>\n\t<span class=\"token number\">2</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'geographical location'</span><span class=\"token punctuation\">,</span>\n\t<span class=\"token number\">3</span><span class=\"token punctuation\">:</span> <span class=\"token string\">'time indicator'</span>\n\t<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\"># 2. Split data into training set</span>\ntrain_x<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">,</span> test_x<span class=\"token punctuation\">,</span> test_y <span class=\"token operator\">=</span> load_tweets<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 3. Preprocess data</span>\ntrain_x<span class=\"token punctuation\">,</span> test_x <span class=\"token operator\">=</span> preprocess_tweets<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 4. Create Vocabulary</span>\nvocab <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span><span class=\"token string\">'__PAD__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'__&lt;/e>__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'__UNK__'</span><span class=\"token punctuation\">:</span> <span class=\"token number\">2</span><span class=\"token punctuation\">}</span>\n<span class=\"token keyword\">for</span> tweet <span class=\"token keyword\">in</span> train_x<span class=\"token punctuation\">:</span>\n\tprocessed_tweet <span class=\"token operator\">=</span> process_tweet<span class=\"token punctuation\">(</span>tweet<span class=\"token punctuation\">)</span>\n\t<span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> processed_tweet<span class=\"token punctuation\">:</span>\n\t\t<span class=\"token keyword\">if</span> word <span class=\"token keyword\">not</span> <span class=\"token keyword\">in</span> vocab<span class=\"token punctuation\">:</span>\n\t\t\tvocab<span class=\"token punctuation\">[</span>word<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>vocab<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 5a. Initialize tokenizer for tweets</span>\ndata_pipeline <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Tokenize<span class=\"token punctuation\">(</span>vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">,</span> keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Shuffle<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>FilterByLength<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>BucketByLength<span class=\"token punctuation\">(</span>boundaries<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">30</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>batch_sizes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\ttrax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>AddLossWeights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\t<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 5b. Tokenize and batch tweets, then output as generator</span>\nstreamed_batches <span class=\"token operator\">=</span> data_pipeline<span class=\"token punctuation\">(</span>tweets<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 6. Initialize model</span>\nmodel <span class=\"token operator\">=</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n    tl<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token operator\">=</span><span class=\"token number\">8192</span><span class=\"token punctuation\">,</span> d_feature<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    tl<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>        <span class=\"token comment\"># LSTM cell</span>\n    tl<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\"># Classify 2 classes</span>\n    tl<span class=\"token punctuation\">.</span>LogSoftmax<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># Produce log-probabilities</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7a. Initialize training task</span>\ntrain_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>TrainTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_batches<span class=\"token punctuation\">,</span>\n    loss_layer<span class=\"token operator\">=</span>tl<span class=\"token punctuation\">.</span>WeightedCategoryCrossEntropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    optimizer<span class=\"token operator\">=</span>trax<span class=\"token punctuation\">.</span>optimizers<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    n_steps_per_checkpoint<span class=\"token operator\">=</span><span class=\"token number\">500</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7b. Initialize evaluation task</span>\neval_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>EvalTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_eval_batches<span class=\"token punctuation\">,</span>\n    metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>tl<span class=\"token punctuation\">.</span>WeightedCategoryCrossEntropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>WeightedCategoryAccuracy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    n_eval_batches<span class=\"token operator\">=</span><span class=\"token number\">20</span>  <span class=\"token comment\"># For less variance in eval numbers</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7c. Prepare training loop and saving checkpoints to output_dir</span>\ntraining_loop <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>Loop<span class=\"token punctuation\">(</span>\n    model<span class=\"token punctuation\">,</span>\n    train_task<span class=\"token punctuation\">,</span>\n    eval_tasks<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>eval_task<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    output_dir<span class=\"token operator\">=</span><span class=\"token string\">'~/output_file.txt'</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 7d. Run 2000 steps (batches)</span>\ntraining_loop<span class=\"token punctuation\">.</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">2000</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 8. Predict sentiment on test data</span>\nexample_input <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>eval_batches_stream<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\nexample_input_str <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>detokenize<span class=\"token punctuation\">(</span>example_input<span class=\"token punctuation\">,</span> vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">)</span>\nsentiment_log_probs <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>example_input<span class=\"token punctuation\">[</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Add batch dimension</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'example input_str: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>example_input_str<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Model returned sentiment probabilities: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>sentiment_log_probs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.coursera.org/learn/sequence-models-in-nlp/lecture/vUAlY/training-ners-data-processing\" target=\"_blank\" rel=\"nofollow\">Lecture about Training NER Networks</a></li>\n<li><a href=\"https://www.quora.com/What-is-the-difference-between-POS-Tag-and-Named-Entity-Recognition\" target=\"_blank\" rel=\"nofollow\">Difference between POS Tagging and NER Recognition</a></li>\n<li><a href=\"https://stackabuse.com/python-for-nlp-parts-of-speech-tagging-and-named-entity-recognition/\" target=\"_blank\" rel=\"nofollow\">Example of POS Tagging and NER Recognition</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/posner","previousSlug":"ml/nlp/lda","nextSlug":"ml/nlp/sentiment","previousTitle":"Latent Dirichlet Allocation","nextTitle":"Sentiment Analysis"}},"staticQueryHashes":[]}