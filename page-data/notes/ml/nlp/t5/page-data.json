{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/t5","result":{"data":{"markdownRemark":{"frontmatter":{"title":"The T5 Model"},"html":"<h3>History of BERT and Other NLP Models</h3>\n<ul>\n<li>In the early stages of NLP, we simply wanted to predict the next word in a sentence</li>\n<li>\n<p>To do this, we used a continuous bag of words (CBOW) model</p>\n<ul>\n<li>This model is limited to classification based on the input words within a fixed-length sliding window</li>\n<li>Unfortunately, this model excludes the use of many useful context words and relationships with other words in the sentence</li>\n</ul>\n</li>\n<li>\n<p>Then, ELMo was created in 2018 by researchers at the Allen Institute</p>\n<ul>\n<li>This model is a bidirectional LSTM</li>\n<li>Implying, words from the left and right are considered</li>\n<li>This model is able to entirely capture context words and relationships</li>\n<li>However, it still suffered from capturing context in longer sentences</li>\n</ul>\n</li>\n<li>\n<p>Later in 2018, OpenAI introduced the GPT model</p>\n<ul>\n<li>There are <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">3</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">3</span></span></span></span> versions of GPT: GPT-1, GPT-2, and GPT-3</li>\n<li>All three models are a transformer model</li>\n<li>This model only includes a decoder (no encoders included)</li>\n<li>This model only uses causal attention</li>\n<li>Unfortunately, each GPT model is only unidirectional</li>\n<li>Thus, we can't capture context both leftward and rightward of our target word in a sentences</li>\n</ul>\n</li>\n<li>\n<p>In 2019, Google released the BERT model</p>\n<ul>\n<li>This model is a bidirectional transformer</li>\n<li>Implying, words from the left and right are considered</li>\n<li>This model is able to entirely capture context words and relationships</li>\n<li>This model only includes an encoder (no decoders included)</li>\n<li>This model doesn't suffer from capturing context in longer sentences</li>\n<li>\n<p>This model can do the following tasks:</p>\n<ul>\n<li>Next sentence prediction</li>\n<li>Multi-mask language modeling:</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>... on the _ side _ history ... </mtext><mo>→</mo><menclose notation=\"box\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mi>M</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></mstyle></mstyle></mstyle></menclose><mo>→</mo><mtext> right, of</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{... on the \\_ side \\_ history ... } \\to \\boxed{Model} \\to \\text{ right, of}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.00444em;vertical-align:-0.31em;\"></span><span class=\"mord text\"><span class=\"mord\">... on the _ side _ history ... </span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.3744399999999999em;vertical-align:-0.3400000000000001em;\"></span><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0344399999999998em;\"><span style=\"top:-3.37444em;\"><span class=\"pstrut\" style=\"height:3.37444em;\"></span><span class=\"boxpad\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\">d</span><span class=\"mord mathnormal\">e</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span style=\"top:-3.03444em;\"><span class=\"pstrut\" style=\"height:3.37444em;\"></span><span class=\"stretchy fbox\" style=\"height:1.3744399999999999em;border-style:solid;border-width:0.04em;\"></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3400000000000001em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">→</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\"> right, of</span></span></span></span></span></span>\n<h3>Introducing the T5 Model</h3>\n<ul>\n<li>The T5 model could be used for several NLP tasks</li>\n<li>Its model (i.e. transformer) is similar to BERT</li>\n<li>Its pre-training (and training) strategy is similar to BERT</li>\n<li>\n<p>Similar to BERT, it makes use of two general concepts:</p>\n<ul>\n<li>Transfer learning</li>\n<li>Mask language modeling</li>\n</ul>\n</li>\n<li>T5 proposes a unified framework attempting to combine many NLP tasks into a text-to-text format</li>\n<li>To do this, the T5 model utilizes transfer learning</li>\n<li>\n<p>The T5 model was trained on the <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>C</mtext><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\text{C}4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord text\"><span class=\"mord\">C</span></span><span class=\"mord\">4</span></span></span></span> dataset</p>\n<ul>\n<li>This data set has been open-sourced by the authors</li>\n<li>It contains <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>750</mn><mtext>GB</mtext></mrow><annotation encoding=\"application/x-tex\">750\\text{GB}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">7</span><span class=\"mord\">5</span><span class=\"mord\">0</span><span class=\"mord text\"><span class=\"mord\">GB</span></span></span></span></span> of cleaned data scraped from the internet</li>\n</ul>\n</li>\n</ul>\n<h3>Applications of the T5 Model</h3>\n<ul>\n<li>Text classification</li>\n<li>Question answering</li>\n<li>Machine translation</li>\n<li>Text summarization</li>\n<li>Sentiment analysis</li>\n<li>And many other NLP tasks</li>\n</ul>\n<h3>Formatting the Inputs of the T5 Model</h3>\n<ul>\n<li>Again, the T5 model is a unified framework combining many NLP tasks into a text-to-text format</li>\n<li>\n<p>This style of architecture differs from BERT</p>\n<ul>\n<li>\n<p>BERT is pre-trained base on the following two tasks:</p>\n<ul>\n<li>Masked language modeling</li>\n<li>Next sentence prediction</li>\n</ul>\n</li>\n<li>Meaning, it must be fine-tuned for other tasks</li>\n<li>For example, we must slightly tweak the architecture if we're performing text classification or question answering</li>\n</ul>\n</li>\n<li>\n<p>Contrarily, the text-to-text framework suggests using the same model, loss function, and hyperparameters for every NLP task </p>\n<ul>\n<li>Thus, the goal of T5 is to perform any NLP task without fine-tuning</li>\n</ul>\n</li>\n<li>\n<p>This approach requires the inputs to be modeled so the model can recognize the task of interest</p>\n<ul>\n<li>As a result, each input must have its task included as a prefix</li>\n</ul>\n</li>\n<li>\n<p>The following are examples of inputs formatted with task-prefixes:</p>\n<ul>\n<li>\n<p>Machine translation:</p>\n<ul>\n<li><code class=\"language-text\">Input:</code> 'translate English to Spanish: Hello!'</li>\n<li><code class=\"language-text\">Ouput:</code> 'Hola!'</li>\n</ul>\n</li>\n<li>\n<p>CoLA (grammar checking):</p>\n<ul>\n<li><code class=\"language-text\">Input:</code> 'cola sentence: I had cat time!'</li>\n<li><code class=\"language-text\">Ouput:</code> 'unacceptable'</li>\n</ul>\n</li>\n<li>\n<p>Sentiment analysis:</p>\n<ul>\n<li><code class=\"language-text\">Input:</code> 'sst2 sentence: I had a great time!'</li>\n<li><code class=\"language-text\">Ouput:</code> 'positive'</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Then, the output is simply the text version of the expected outcome</li>\n<li>The following diagram illustrates the format of the input and output: </li>\n</ul>\n<p><img src=\"/eb4839e08d6609f28a62fdf04e07eaf0/t5.gif\" alt=\"t5\"></p>\n<h3>Describing the Pre-Training Strategy for T5</h3>\n<ul>\n<li>\n<p>The T5 paper experimented with using three different architectures for pre-training:</p>\n<ul>\n<li>Encoder-Decoder architecture</li>\n<li>Language model architecture</li>\n<li>Prefix language model architecture</li>\n</ul>\n</li>\n<li>\n<p>The encoder-decoder architecture is an ordinary encoder-decoder transformer</p>\n<ul>\n<li>The encoder uses a <em>fully-visible</em> attention mask</li>\n<li>This masking technique is also used in BERT</li>\n<li>Thus, every input token contributes to the attention computation of every other input token in the input sequence</li>\n<li>Then, the decoder is trained using causal attention</li>\n<li>Meaning, only the previous output tokens contribute to attention computation of the current output token in the output sequence</li>\n</ul>\n</li>\n<li>\n<p>The language model architecture uses the attention mechanism</p>\n<ul>\n<li>It is an autoregressive modeling approach</li>\n<li>It is a mix between the the BERT architecture and language modeling approaches</li>\n</ul>\n</li>\n<li>Based on various benchmarks, the best architecture is the encoder-decoder architecture</li>\n</ul>\n<p><img src=\"/88459ae93dd4af11a69cab297fec5dbd/t5training.png\" alt=\"t5training\"></p>\n<h3>Defining the GLUE Benchmark</h3>\n<ul>\n<li>The GLUE benchmark stands for <em>general language understanding evaluation</em></li>\n<li>The GLUE benchmark is one of the most popular benchmarks in NLP</li>\n<li>It is used to train, test, and analyze NLP tasks</li>\n<li>\n<p>It is a collection of benchmark tools consisting of:</p>\n<ul>\n<li>A benchmark of nine different language comprehension tasks</li>\n<li>An ancillary data set</li>\n<li>A platform for evaluating and comparing the models</li>\n</ul>\n</li>\n<li>\n<p>It is used for various types of NLP tasks:</p>\n<ul>\n<li>Verifying whether a sentence is grammatical</li>\n<li>Verifying the accuracy of sentiment predictions</li>\n<li>Verifying the accuracy of paraphrasing text</li>\n<li>Verifying the similarity between two texts</li>\n<li>Verifying whether two questions are duplicates</li>\n<li>Verifying whether a question is answerable</li>\n<li>Verifying whether a question is a contradiction</li>\n</ul>\n</li>\n<li>\n<p>Usually, it is used with a leaderboard</p>\n<ul>\n<li>This is so people can see how well their model performs compared to other models on a dataset</li>\n</ul>\n</li>\n<li>\n<p>The GLUE benchmark has the following advantages:</p>\n<ul>\n<li>\n<p>The GLUE benchmark is model-agnostic</p>\n<ul>\n<li>Doesn't matter if we're evaluating a transformer or LSTM</li>\n</ul>\n</li>\n<li>Makes use of transfer learning</li>\n<li>Most research uses the GLUE benchmark as a standard</li>\n</ul>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://cs224d.stanford.edu/lectures/\" target=\"_blank\" rel=\"nofollow\">Stanford Deep Learning Lectures</a></li>\n<li><a href=\"http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\" target=\"_blank\" rel=\"nofollow\">Stanford Lecture about LSTMs</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/dDSZk/transformer-t5\" target=\"_blank\" rel=\"nofollow\">Lecture about the T5 Transformer</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/vhRkb/multi-task-training-strategy\" target=\"_blank\" rel=\"nofollow\">Lecture about Multi-Trask Training</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/h2IJz/glue-benchmark\" target=\"_blank\" rel=\"nofollow\">Lecture about the GLUE Benchmark</a></li>\n<li><a href=\"https://towardsdatascience.com/t5-text-to-text-transfer-transformer-643f89e8905e\" target=\"_blank\" rel=\"nofollow\">Article about the T5 Transformer Model</a></li>\n<li><a href=\"https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\" target=\"_blank\" rel=\"nofollow\">Description of T5 from Google AI Blog</a></li>\n<li><a href=\"https://d2l.ai/chapter_natural-language-processing-pretraining/bert.html\" target=\"_blank\" rel=\"nofollow\">Textbook Chapter about Pre-Training Strategy</a></li>\n<li><a href=\"https://github.com/huggingface/transformers/issues/3704\" target=\"_blank\" rel=\"nofollow\">Details about Preparing Inputs for Sentiment Analysis</a></li>\n<li><a href=\"https://colab.research.google.com/github/PytorchLightning/pytorch-lightning/blob/master/notebooks/04-transformers-text-classification.ipynb\" target=\"_blank\" rel=\"nofollow\">List of Tasks used in T5</a></li>\n<li><a href=\"https://arxiv.org/pdf/1910.10683.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about the T5 Transformer Model</a></li>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about Alignment and Attention Models</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/t5","previousSlug":"ml/nlp/bert","nextSlug":"ml/nlp/reformer","previousTitle":"BERT","nextTitle":"Reformers"}},"staticQueryHashes":[]}