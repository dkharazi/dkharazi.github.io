{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/train_trax","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Supervised Training using Trax"},"html":"<h3>Sample Data Pipeline for Training</h3>\n<ul>\n<li>In the previous section, we implemented a data pipeline in Trax</li>\n<li>There isn't any preprocessing run on the raw tweets</li>\n<li>Meaning, our example will train on uppercased words and symbols</li>\n<li>Before creating a data pipeline, the tweets are read in using standard Python</li>\n<li>Then, a tokenizer is implemented in Trax</li>\n<li>Finally, the tokenizer is applied to our tweets</li>\n<li>As a reminder, the code looks like the following:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Read in raw tweets</span>\n<span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'tweets.txt'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n    tweets <span class=\"token operator\">=</span> <span class=\"token builtin\">iter</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">tuple</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">','</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">','</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> f<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Initialize data pipeline for preprocessing raw data</span>\ndata_pipeline <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Tokenize<span class=\"token punctuation\">(</span>vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">,</span> keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Shuffle<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>FilterByLength<span class=\"token punctuation\">(</span>max_length<span class=\"token operator\">=</span><span class=\"token number\">30</span><span class=\"token punctuation\">,</span> length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>BucketByLength<span class=\"token punctuation\">(</span>boundaries<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">30</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>batch_sizes<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>length_keys<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>AddLossWeights<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Return generator</span>\nstreamed_batches <span class=\"token operator\">=</span> data_pipeline<span class=\"token punctuation\">(</span>tweets<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Sample Model for Training</h3>\n<ul>\n<li>In an earlier section, we introduced layers</li>\n<li>Activation layers are wrapped in a <code class=\"language-text\">Serial</code> layer</li>\n<li>The <code class=\"language-text\">Serial</code> layer represents our neural network</li>\n<li>Other sub-layers can be added for creating word embeddings</li>\n<li>The following model can be used for sentiment analysis:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Initialize model</span>\nmodel <span class=\"token operator\">=</span> tl<span class=\"token punctuation\">.</span>Serial<span class=\"token punctuation\">(</span>\n    tl<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span>vocab_size<span class=\"token operator\">=</span><span class=\"token number\">8192</span><span class=\"token punctuation\">,</span> d_feature<span class=\"token operator\">=</span><span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    tl<span class=\"token punctuation\">.</span>Mean<span class=\"token punctuation\">(</span>axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># Average on axis 1 (length of sentence).</span>\n    tl<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\"># Classify 2 classes.</span>\n    tl<span class=\"token punctuation\">.</span>LogSoftmax<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># Produce log-probabilities.</span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Introducing the Training <code class=\"language-text\">Task</code></h3>\n<ul>\n<li>The <code class=\"language-text\">TrainTask</code> class is used for defining the training architecture</li>\n<li>\n<p>Specifically, it's used for defining the strategy behind:</p>\n<ul>\n<li>Loss function</li>\n<li>Any gradient optimizers, such as Adam</li>\n<li>Logging checkpoints for parameter and accuracy evaluations after an <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span> number of steps have been taken</li>\n</ul>\n</li>\n<li>\n<p>A training step refers to one gradient update:</p>\n<ul>\n<li>A single step implies <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span> number of observations are processed</li>\n<li>Here, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">m</span></span></span></span> is the number of observations within a single batch</li>\n<li>Thus, a step is an iteration over a single batch</li>\n<li><strong>In other words, parameter updates happen after all of the observations within a batch have been forward propagated</strong></li>\n<li>As a reminder, parameter updates refer to backward propagation</li>\n</ul>\n</li>\n<li>The following is a sample training task:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Initialize training task</span>\ntrain_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>TrainTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_batches<span class=\"token punctuation\">,</span>\n    loss_layer<span class=\"token operator\">=</span>tl<span class=\"token punctuation\">.</span>WeightedCategoryCrossEntropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    optimizer<span class=\"token operator\">=</span>trax<span class=\"token punctuation\">.</span>optimizers<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    n_steps_per_checkpoint<span class=\"token operator\">=</span><span class=\"token number\">500</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Introducing the Testing <code class=\"language-text\">Task</code></h3>\n<ul>\n<li>The <code class=\"language-text\">EvalTask</code> class is used for defining the testing architecture</li>\n<li>\n<p>Similar to <code class=\"language-text\">TrainTask</code>, it defines:</p>\n<ul>\n<li>How to measure model performance as a function of steps</li>\n<li>When to measure model performance as a function of steps</li>\n<li>Determining which data to use</li>\n<li>Determining which metrics to report</li>\n</ul>\n</li>\n<li>The following is a sample testing task:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Initialize evaluation task</span>\neval_task <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>EvalTask<span class=\"token punctuation\">(</span>\n    labeled_data<span class=\"token operator\">=</span>streamed_eval_batches<span class=\"token punctuation\">,</span>\n    metrics<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>tl<span class=\"token punctuation\">.</span>WeightedCategoryCrossEntropy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tl<span class=\"token punctuation\">.</span>WeightedCategoryAccuracy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    n_eval_batches<span class=\"token operator\">=</span><span class=\"token number\">20</span>  <span class=\"token comment\"># For less variance in eval numbers</span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Introducing the Training <code class=\"language-text\">Loop</code></h3>\n<ul>\n<li>The <code class=\"language-text\">Loop</code> classs is used for running and performing the core training loop</li>\n<li>The number of steps taken by the training is given in the training task</li>\n<li>The training parameters run by <code class=\"language-text\">Loop</code> are initialized randomly</li>\n<li>First, we define the directory <code class=\"language-text\">output_dir</code> to which the output file will be written</li>\n<li>\n<p>Next, we implement a <code class=\"language-text\">Loop</code> object that does the following:</p>\n<ul>\n<li>\n<p>Trains a given model on training data</p>\n<ul>\n<li>The training data is given in the training task</li>\n</ul>\n</li>\n<li>Outlines the training architecture with a training task</li>\n<li>Outlines the testing architecture with an evaluation task</li>\n</ul>\n</li>\n<li>The following is a sample training loop:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Run training loop and save checkpoints to output_dir</span>\ntraining_loop <span class=\"token operator\">=</span> training<span class=\"token punctuation\">.</span>Loop<span class=\"token punctuation\">(</span>\n    model<span class=\"token punctuation\">,</span>\n    train_task<span class=\"token punctuation\">,</span>\n    eval_tasks<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>eval_task<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    output_dir<span class=\"token operator\">=</span><span class=\"token string\">'~/output_file.txt'</span>\n    <span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Run 2000 steps (batches)</span>\ntraining_loop<span class=\"token punctuation\">.</span>run<span class=\"token punctuation\">(</span><span class=\"token number\">2000</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Sample Training Output of Running <code class=\"language-text\">Loop</code></h3>\n<ul>\n<li>In the <code class=\"language-text\">Loop</code> class, we can write reporting output to a file</li>\n<li>\n<p>Based on the arguments defined in our <code class=\"language-text\">TrainTask</code> and <code class=\"language-text\">EvalTask</code>, we can define the following:</p>\n<ul>\n<li>Which metrics are reported to this file</li>\n<li>How frequenty the metrics are reported to this file</li>\n</ul>\n</li>\n<li>The following is an example of the reporting streamed to this file:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Step      1: Ran 1 train steps in 0.78 secs\nStep      1: train WeightedCategoryCrossEntropy |  1.33800304\nStep      1: eval  WeightedCategoryCrossEntropy |  0.71843582\nStep      1: eval      WeightedCategoryAccuracy |  0.56562500\n\nStep    500: Ran 499 train steps in 5.77 secs\nStep    500: train WeightedCategoryCrossEntropy |  0.62914723\nStep    500: eval  WeightedCategoryCrossEntropy |  0.49253047\nStep    500: eval      WeightedCategoryAccuracy |  0.74062500\n\nStep   1000: Ran 500 train steps in 5.03 secs\nStep   1000: train WeightedCategoryCrossEntropy |  0.42949259\nStep   1000: eval  WeightedCategoryCrossEntropy |  0.35451687\nStep   1000: eval      WeightedCategoryAccuracy |  0.83750000\n\nStep   1500: Ran 500 train steps in 4.80 secs\nStep   1500: train WeightedCategoryCrossEntropy |  0.41843575\nStep   1500: eval  WeightedCategoryCrossEntropy |  0.35207348\nStep   1500: eval      WeightedCategoryAccuracy |  0.82109375\n\nStep   2000: Ran 500 train steps in 5.35 secs\nStep   2000: train WeightedCategoryCrossEntropy |  0.38129005\nStep   2000: eval  WeightedCategoryCrossEntropy |  0.33760912\nStep   2000: eval      WeightedCategoryAccuracy |  0.85312500</code></pre></div>\n<h3>Sample Predictions and Testing After Training</h3>\n<ul>\n<li>After training our model, we can test individual tweets by running them through our model</li>\n<li>The following is an example of testing a tweet:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Retrieve a single tokenized test from our evaluation set</span>\nexample_input <span class=\"token operator\">=</span> <span class=\"token builtin\">next</span><span class=\"token punctuation\">(</span>eval_batches_stream<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># Detokenize the test based on our vocab</span>\nexample_input_str <span class=\"token operator\">=</span> trax<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>detokenize<span class=\"token punctuation\">(</span>example_input<span class=\"token punctuation\">,</span> vocab_file<span class=\"token operator\">=</span><span class=\"token string\">'en_8k.subword'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'example input_str: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>example_input_str<span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token string\">\"example input_str: I first saw this when I was a teen in my last year of Junior High.&lt;pad>&lt;pad>&lt;pad>\"</span>\n\n<span class=\"token comment\"># Predict using our trained model</span>\nsentiment_log_probs <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>example_input<span class=\"token punctuation\">[</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Add batch dimension</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f'Model returned sentiment probabilities: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>np<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>sentiment_log_probs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">'</span></span><span class=\"token punctuation\">)</span>\n<span class=\"token string\">\"Model returned sentiment probabilities: [[3.984500e-04 9.996014e-01]]\"</span></code></pre></div>\n<h3>Resources</h3>\n<ul>\n<li><a href=\"https://github.com/google/trax\" target=\"_blank\" rel=\"nofollow\">Training Walkthrough on GitHub</a></li>\n<li><a href=\"https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb#scrollTo=djTiSLcaNFGa\" target=\"_blank\" rel=\"nofollow\">Trax Quick Intro</a></li>\n<li><a href=\"https://stackoverflow.com/a/44416034/12777044\" target=\"_blank\" rel=\"nofollow\">Defining Epochs and Training Steps</a></li>\n<li><a href=\"https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop\" target=\"_blank\" rel=\"nofollow\">Documentation of Training Loop</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/train_trax","previousSlug":"ml/nlp/data_trax","nextSlug":"ml/nlp/embedding","previousTitle":"Data Pipelines using Trax","nextTitle":"Word Embeddings"}},"staticQueryHashes":[]}