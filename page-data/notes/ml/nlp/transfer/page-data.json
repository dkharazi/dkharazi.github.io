{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/nlp/transfer","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Transfer Learning"},"html":"<h3>Introducing Transfer Learning</h3>\n<ul>\n<li>Suppose someone has trained a deep network on an NLP task</li>\n<li>\n<p>We may be interested in using the network if:</p>\n<ul>\n<li>The network was trained on very large dataset</li>\n<li>The network provides a very good accuracy</li>\n</ul>\n</li>\n<li>\n<p>If we're interested in a similar NLP task, then we may want to use this pre-trained network for our particular task</p>\n<ul>\n<li>Then, we'll only need to train this network on a smaller set of data</li>\n</ul>\n</li>\n<li>\n<p>At a high level, we're essentially taking a network with parameters that are close enough to those parameters in our optimal network</p>\n<ul>\n<li>To focus the network on our particular task, we just slightly tune the parameters by training the model on a small set of relevent data</li>\n</ul>\n</li>\n<li>\n<p>We may be interested in using a pre-trained network if:</p>\n<ul>\n<li>The network was pre-trained for a task similar to ours</li>\n<li>The network was pre-trained on a very large dataset</li>\n<li>The network is very accurate</li>\n</ul>\n</li>\n<li>\n<p>The potential benefits of transfer learning are the following:</p>\n<ul>\n<li>Reduced training time</li>\n<li>Improvement in predictions</li>\n<li>Training only needs a small dataset</li>\n</ul>\n</li>\n<li>\n<p>In general, the following rules of thumb hold true in transfer learning:</p>\n<ul>\n<li>As our model grows larger, the accuracy improves</li>\n<li>As our data grows larger, the accuracy improves</li>\n</ul>\n</li>\n<li>The process of using a pre-trained network is called <em>transfer learning</em></li>\n</ul>\n<h3>Different Options and Types of Transfer Learning</h3>\n<ul>\n<li>\n<p>In general, there are two types of transfer learning</p>\n<ul>\n<li>Feature-based transfer learning</li>\n<li>Fine-tuning transfer learning</li>\n</ul>\n</li>\n<li>\n<p><em>Feature-based</em> transfer learning takes the weights from the pre-trained model, then uses these weights as input into our own (new) model</p>\n<ul>\n<li>Impying, we typically train the data on an entirely new model</li>\n</ul>\n</li>\n<li>\n<p><em>Fine-tuning</em> transfer learning inputs our own data set into the pre-trained model</p>\n<ul>\n<li>Implying, we rarely make changes to the pre-trained model</li>\n<li>\n<p>Sometimes, we may adjust the output layer</p>\n<ul>\n<li>e.g. adding a softmax layer</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"http://cs224d.stanford.edu/lectures/\" target=\"_blank\" rel=\"nofollow\">Stanford Deep Learning Lectures</a></li>\n<li><a href=\"http://cs224d.stanford.edu/lectures/CS224d-Lecture9.pdf\" target=\"_blank\" rel=\"nofollow\">Stanford Lecture about LSTMs</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/qMRXX/transfer-learning-in-nlp\" target=\"_blank\" rel=\"nofollow\">Lecture about Types of Transfer Learning</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/iPUp8/elmo-gpt-bert-t5\" target=\"_blank\" rel=\"nofollow\">Lecture about the History of Neural Networks in NLP</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/lZX7F/bidirectional-encoder-representations-from-transformers-bert\" target=\"_blank\" rel=\"nofollow\">Lecture about Defining the BERT Model</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/1g8LM/bert-objective\" target=\"_blank\" rel=\"nofollow\">Lecture about Intuition of BERT Tasks</a></li>\n<li><a href=\"https://www.coursera.org/learn/attention-models-in-nlp/lecture/EMBvt/fine-tuning-bert\" target=\"_blank\" rel=\"nofollow\">Lecture about BERT Applications</a></li>\n<li><a href=\"https://d2l.ai/chapter_natural-language-processing-pretraining/bert-pretraining.html\" target=\"_blank\" rel=\"nofollow\">Defining and Pre-Training BERT</a></li>\n<li><a href=\"https://stats.stackexchange.com/a/193451\" target=\"_blank\" rel=\"nofollow\">Post about Pre-Training and Fine-Tuning Networks</a></li>\n<li><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" target=\"_blank\" rel=\"nofollow\">Paper about Alignment and Attention Models</a></li>\n</ul>"}},"pageContext":{"slug":"ml/nlp/transfer","previousSlug":"ml/nlp/transformer","nextSlug":"ml/nlp/bert","previousTitle":"Transformer Models","nextTitle":"BERT"}},"staticQueryHashes":[]}