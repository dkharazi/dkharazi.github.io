{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/probability_theory/bayes","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Bayesian and Frequentist Estimation"},"html":"<h3>An Analogy of the Data Generation Process</h3>\n<ul>\n<li>A data generating process is the true, underlying phenomenon that is creating the data</li>\n<li>A mathematical model is the (often imperfect) attempt to describe and emulate this phenomenon</li>\n<li>A mathematical model is represented as a function with adjustable parameters</li>\n<li>We can think of a mathematical model as a shower head that is controlled by a bunch of knobs, which are the model's parameters</li>\n<li>For example, the angle of the shower head is one parameter that controls the <em>location</em> of droplets on the floor</li>\n<li>There is another knob that controls the <em>spread</em> of the spray</li>\n<li>This shower head can be thought of as our normal probability density function, where the location know is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> and the spread know is <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span></li>\n<li>The bathroom shower head is just one device for generating a pattern of random water droplets</li>\n<li>There are others, such as different types of lawn sprinklers</li>\n<li>Each type of sprinkler generates a different pattern of droplets, and each type of sprinkler has different control knobs</li>\n<li>Different mathematical models of data are like different shower heads or sprinklers, each with their own control knobs</li>\n<li>Each mathematical model generates a particular type of data, and each mathematical model has particular knobs – called parameters – that control the specific details of the pattern of data</li>\n</ul>\n<h3>Motivating Parameter Estimation</h3>\n<ul>\n<li>Previously, we estimated probabilities using frequentist and bayesian methods</li>\n<li>A probability is just considered a parameter of a mathematical model</li>\n<li>Therefore, these frequentist and bayesian methods used for estimating probabilities can also be used to estimate other parameters</li>\n<li>In other words, we can use those same frequentist and bayesian parameter estimation techniques, such as MLE and simulations, to estimate the parameters <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span> for a normal distribution</li>\n</ul>\n<h3>More on Frequentist Estimation</h3>\n<ul>\n<li>There are lots of settings for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span> that may make the normal distribution mimick the data reasonably well, but what values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span> are the <em>best</em>?</li>\n<li>The classic answer to this question in the frequentist framework is the values of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span></span></span></span> that maximize the probability of the data</li>\n<li>Another technical term for probability is <em>likelihood</em>, and so the values that maximize the probability of the data are called the maximum likelihood estimate or MLE</li>\n<li>It turns out that for a normal distribution, the MLE value for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">μ</span></span></span></span> is just the sample mean, and the MLE value for <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">σ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span> is just the sample variance</li>\n</ul>\n<h3>More on Bayesian Estimation</h3>\n<ul>\n<li>\n<p>In Bayesian statistics, we typically follow a general process when estimating parameters:</p>\n<ol>\n<li>Start with a set of possible parameter values in a model, with initial credibilities of those parameter values</li>\n<li>Gather data that makes some parameter values more or less credible</li>\n<li>Re-allocate credibility to the parameter values that are more consistent with the data, and re-allocate credibility away from parameter values that are less consistent with the data</li>\n</ol>\n</li>\n<li>One attraction of Bayesian methods is that the posterior distribution inherently reveals the uncertainty of the estimated parameter value</li>\n<li>When the posterior distribution is wide, the estimate is uncertain</li>\n<li>When the posterior distribution is narrower, the posterior estimate is more certain</li>\n<li>In the Bayesian framework, uncertainty is inherently represented by the posterior distribution over the parameters</li>\n<li>In the frequentist framework, there is no such representation, and so confidence intervals must be used to represent uncertainty</li>\n</ul>\n<h3>Which Analysis and When?</h3>\n<ul>\n<li>We should use Bayesian analysis if we're asking what parameter values and models are most credible given the data</li>\n<li>We should use frequentist analysis if we're asking about error rates for imaginary data from hypothetical worlds</li>\n</ul>\n<h3>Bayesian versus Frequentist Estimation</h3>\n<ul>\n<li>It is often said incorrectly that parameters are treated as fixed by frequentists but random by bayesians</li>\n<li>However, frequentists and bayesians both believe a parameter may have been fixed from the start or may have been generated from a physically random mechanism</li>\n<li>In either case, both suppose it has taken on some fixed value</li>\n<li>The bayesian uses formal probability models to express personal uncertainty about that value, whereas the frequentist uses confidence interval to express uncertainty about that value</li>\n<li>Randomness in our model creates personal uncertainty about our parameter estimates in our model</li>\n<li>Randomness is not a property of the parameter, although we hope it accurately reflects properties of the mechanisms that produced the parameter</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://jkkweb.sitehost.iu.edu/KruschkeFreqAndBayesAppTutorial.html#analysis_model\" target=\"_blank\" rel=\"nofollow\">Bayesian and Frequentist Differences</a></li>\n<li><a href=\"https://analyticsconsultores.com.mx/wp-content/uploads/2019/04/Innovations-in-Bayesian-Networks.-Theory-and-Applications.pdf\" target=\"_blank\" rel=\"nofollow\">Representations of Uncertainty in Bayesianism and Frequentism</a></li>\n<li><a href=\"https://cnl.salk.edu/~schraudo/teach/ml03/ML_Class2.pdf\" target=\"_blank\" rel=\"nofollow\">Contrasts of Bayesianism and Frequentism Example</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/83731/would-a-bayesian-admit-that-there-is-one-fixed-parameter-value\" target=\"_blank\" rel=\"nofollow\">Fixed Parameters in Bayesianism and Frequentism</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english\" target=\"_blank\" rel=\"nofollow\">Bayesian and Frequentist Reasoning in Plain English</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/1525/whats-the-difference-between-a-probability-and-a-proportion/4850#4850\" target=\"_blank\" rel=\"nofollow\">Differences between a Probability and a Proportion</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/173056/how-exactly-do-bayesians-define-or-interpret-probability\" target=\"_blank\" rel=\"nofollow\">Bayesians Defining and Interpreting Probability</a></li>\n</ul>"}},"pageContext":{"slug":"ml/probability_theory/bayes","previousSlug":"ml/probability_theory/probability_philosophy","nextSlug":"ml/probability_theory/random_variables","previousTitle":"Philosophy behind Probability","nextTitle":"Random Variables"}},"staticQueryHashes":[]}