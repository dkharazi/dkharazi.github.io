{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/regularization/l1","result":{"data":{"markdownRemark":{"frontmatter":{"title":"L1 Regularization"},"html":"<h3>Describing L1 Regularization</h3>\n<ul>\n<li>L1 regularization is regularization technique used for variable selection</li>\n<li>L1 regularization achieves variable selection by shrinking the OLS coefficients down to zero</li>\n<li>L1 regularization quickly shrinks the OLS coefficients by applying a penalty term to the OLS objective function, and minimizing this modified cost function instead of the original OLS objective function</li>\n<li>The penalty term associated with L1 regularization is the absolute value of the magnitude of OLS coefficients, which is the main difference between the L1 and L2 regularization methods</li>\n<li>L1 regularization is used in lasso regression</li>\n</ul>\n<h3>Mathematical Properties of L1 Regularization</h3>\n<ul>\n<li>L1 regularization involves a diamond-shaped constraint region</li>\n<li>The optimal point intersecting the elliptical contour plot (representing the objective function) and the constraint region lies on an axis where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta_{j}=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.980548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05278em;\">β</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span>, which provides L1 regularization with the function of variable selection</li>\n<li>L1 regularization uses convex optimization, and causes the beta coefficients to converge to 0 quickly</li>\n</ul>\n<h3>Use-cases for L1 Regularization</h3>\n<ul>\n<li>Coefficient estimation that is robust to outliers</li>\n<li>Variable selection</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when\" target=\"_blank\" rel=\"nofollow\">Differences between L1 and L2 Regularization</a></li>\n<li><a href=\"http://laid.delanover.com/difference-between-l1-and-l2-regularization-implementation-and-visualization-in-tensorflow/\" target=\"_blank\" rel=\"nofollow\">Visualizing Regularization</a></li>\n<li><a href=\"http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/17_RegularizedLinModel_KnockoffFilter.html\" target=\"_blank\" rel=\"nofollow\">Regularization Lecture Notes</a></li>\n</ul>"}},"pageContext":{"slug":"ml/regularization/l1","previousSlug":"ml/regularization/regularization_basics","nextSlug":"ml/regularization/l2","previousTitle":"Basics of Regularization","nextTitle":"L2 Regularization"}},"staticQueryHashes":[]}