{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/unsupervised_learning/basics_of_clustering","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Basics of Clustering"},"html":"<h3>Motivating Categorization</h3>\n<ul>\n<li>Dividing data into discrete categories is one of the most common kinds of data mining task</li>\n<li>Often these categories are things which are given to us in advance (i.e. based on historical data)</li>\n<li>\n<p>Some examples of categories that could be given to us:</p>\n<ul>\n<li>Cells: cancerous or not</li>\n<li>Credit applicant: pay back loan or not</li>\n<li>Text: political or religious</li>\n<li>Picture: flower, tiger, or ocean</li>\n</ul>\n</li>\n<li>\n<p>There are two basic forms of categorization:</p>\n<ol>\n<li>Classification</li>\n<li>Clustering</li>\n</ol>\n</li>\n<li>\n<p>Classification is the process of assigning new data to pre-existing categories</p>\n<ul>\n<li>These categories are also referred to as classes</li>\n<li>The goal of classification methods is to accurately assign new, unlabeled examples from the test data to these pre-determined classes</li>\n<li>This process is an example of supervised learning</li>\n</ul>\n</li>\n<li>\n<p>Clustering is the process of discovering new categories for our data</p>\n<ul>\n<li>These categories are also referred to as clusters</li>\n<li>This process is an example of unsupervised learning</li>\n</ul>\n</li>\n</ul>\n<h3>Clustering Use-Cases</h3>\n<ol>\n<li>\n<p>Testing classes against objective, deterministic features if the classes were generated from an biased (or subjective) source</p>\n<ul>\n<li>Even if our data comes to us with class labels attached, itâ€™s often wise to be skeptical of their use</li>\n<li>\n<p>Official classification schemes are often the end result of a mix of the following:</p>\n<ul>\n<li>Practical experience</li>\n<li>Intuition</li>\n<li>Theory</li>\n<li>Prejudice</li>\n<li>Ideas copied from somewhere else</li>\n<li>Compromises among groups which differ in interests, ideas, and clout</li>\n<li>People making stuff up because they need something by a deadline</li>\n</ul>\n</li>\n<li>In other words, classes can be typically generated by a human from somewhere in the world, and we shouldn't just blindly trust their class generation process</li>\n<li>Essentially, even when you have what you are told is a supervised learning problem with labeled data, it can be worth treating it as an unsupervised learning problem</li>\n<li>If the clusters you find do not match the official classes, then that could be a sign of an unreasonable or irrelevant official scheme, and we would then need to decide whether to trust the official story of our cluster-finding method or not</li>\n</ul>\n</li>\n<li>\n<p>Pattern detection</p>\n<ul>\n<li>This could include subsetting any pre-determined class</li>\n<li>This could also include supersetting any unknown class (but maybe expected class)</li>\n<li>For example, we may want to partition customers into categories, so we can recommend different products to each group</li>\n</ul>\n</li>\n<li>\n<p>Reducing data size</p>\n<ul>\n<li>We may have features that explain the same information as other features (in terms of our response variable), so we many want to merge those features together through clustering</li>\n<li>For example, we may try to detect and classify different objects in a 3D point cloud, and thus we may want to focus on geometrically close clusters of points, rather than dealing with overlapping points for every dimension in the dataset</li>\n</ul>\n</li>\n</ol>\n<h3>Properties of Good Clusters</h3>\n<ol>\n<li>Every possible point should belong to one and only one cluster</li>\n<li>\n<p>Clusters should be compact</p>\n<ul>\n<li>Said another way, it would be nice if knowing the cluster could tell us about our features</li>\n<li>Specifically, we would like to maximize our information value for each cluster</li>\n<li>A high information value for the clusters means that knowing the cluster reduces our uncertainty about the features</li>\n<li>Essentially, this means that the points in a cluster should be similar to each other</li>\n</ul>\n</li>\n<li>\n<p>Cluster should be separated</p>\n<ul>\n<li>This implies that different clusters should have different distributions of features</li>\n<li>Said another way, if one cluster is much more probable than the other clusters, learning which cluster a point belongs to doesn't reduce uncertainty about its features much</li>\n<li>Essentially, clusters should be well-separated</li>\n</ul>\n</li>\n<li>\n<p>Clusters should be balanced</p>\n<ul>\n<li>Balanced clusters imply each cluster is equally probable of being observed</li>\n<li>Essentially, this means clusters are split fairly evenly amongst each other, but we can't always ensure this</li>\n</ul>\n</li>\n<li>\n<p>Cluster should be parsimonious</p>\n<ul>\n<li>Essentially, this means we want as few clusters as possible</li>\n</ul>\n</li>\n<li>Our hope is to find clusters that satisfy these four properties</li>\n<li>Cluster that follow the above properties are known as parsimonious</li>\n<li>Parsimony and balance are fairly simple to measure (i.e. simply using percentages of clusters)</li>\n<li>Measuring compactness and separation depends on having a good measure of distance for our data to start with</li>\n</ol>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.stat.cmu.edu/~cshalizi/350/2008/lectures/07/lecture-07.pdf\" target=\"_blank\" rel=\"nofollow\">Types of Categorization and Basic Classifiers</a></li>\n<li><a href=\"https://www.quora.com/What-are-some-use-cases-of-clustering-in-machine-learning\" target=\"_blank\" rel=\"nofollow\">Use-Cases of Clustering</a></li>\n</ul>"}},"pageContext":{"slug":"ml/unsupervised_learning/basics_of_clustering","previousSlug":"ml/unsupervised_learning/learning_methods","nextSlug":"ml/unsupervised_learning/similarity_measures","previousTitle":"Basics of Learning Methods","nextTitle":"Similarity Measures"}},"staticQueryHashes":[]}