{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/unsupervised_learning/pca","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Principle Component Analysis"},"html":"<h3>Describing Principal Components Analysis</h3>\n<ul>\n<li>Principal Component Analysis (or PCA) is a method for maximizing the variance of the projected data</li>\n<li>The output of PCA is a set of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\">p</span></span></span></span> orthogonal vectors in the original p-dimensional feature space</li>\n<li>This output of PCA is also referred to as the principal components</li>\n<li>Specifically, PCA is a dimensionality reduction algorithm</li>\n<li>In other words, PCA performs a linear mapping of data from one vector space to a different, lower-dimensional vector space</li>\n<li>Said another way, PCA performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized</li>\n<li>We use PCA in regression by regressing on the components output by PCA</li>\n</ul>\n<h3>The Generalized PCA Algorithm</h3>\n<ul>\n<li>First, we need to construct a covariance (or correlation) matrix of the data</li>\n<li>Then, all of the eigenvalues and eigenvectors are calculated for this matrix</li>\n<li>Then, we can use those eigenvalues to linearly transform the data to a lower dimensional space</li>\n<li>The eigenvectors (i.e. principal components) that correspond to the largest eigenvalues represent the eigenvectors that make up the largest fraction of the variance in the original data</li>\n<li>We can use these components to regress on or to analyze multi-collinearity of our original predictors</li>\n<li>In other words, the dimensionality reduction part of the algorithm is a linear transformation</li>\n<li>However, the algorithm used to calculate eigenvectors is a non-linear transformation</li>\n</ul>\n<h3>Definition of the PCA Algorithm</h3>\n<ol>\n<li>Normalize the data</li>\n<li>Construct a covariance matrix of the data (which is actually the correlation matrix since it has been normalized)</li>\n<li>\n<p>Compute the eigenvectors (and therefore eigenvalues) which diagonalize the covariance matrix</p>\n<ul>\n<li>This computation is a non-linear transformation</li>\n<li>Computing the diagonal matrix of eigenvalues represents a change of basis</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>C</mi><mi>V</mi><mo>=</mo><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">V^{-1}CV = D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.864108em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span>\n<ul>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">C</span></span></span></span> is the covariance matrix</li>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span> is a matrix made up by the eigenvectors that diagonalize the covariance matrix</li>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V^{-1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span></span></span></span> is the inverse of the matrix of eigenvectors</li>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span> is the diagonal matrix of eigenvalues</li>\n</ul>\n</li>\n<li>\n<p>Use the diagonal matrix of eigenvalues to multiply eigenvectors, which will give us the loadings</p>\n<ul>\n<li>Loadings are the convariances or correlations between the original variables and the unit-scaled components</li>\n<li>This computation is a linear transformation</li>\n</ul>\n</li>\n</ol>\n<h3>Use-Cases of PCA</h3>\n<ul>\n<li>If we want to determine which features are correlated with each other, then we can use principal components to interpret the weighted combinations of the original features</li>\n<li>If we want to determine how correlated each feature is correlated with each other, then we can use principal components to interpret the weighted combinations of the original features</li>\n<li>\n<p>If we want to determine the fraction of variance that a related combination of original features captures in the data, then we can determine the fraction <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> of the variance that the ith component captures in the original data</p>\n<ul>\n<li>Where <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>λ</mi></mrow><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">λ</span></span></span></span> represents an eigenvalue</li>\n</ul>\n</li>\n<li>If we want to determine the related combination of original features that captures the greatest fraction of variance in the data, then we can determine the ith component that captures the greatest fraction <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\lambda_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">λ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> of the variance in the original data</li>\n</ul>\n<h3>How Eigenvectors relate to PCA</h3>\n<ul>\n<li>As stated previously, PCA finds the eigenvector that maximizes the variance</li>\n<li>The eigenvector that maximizes the variance is the same eigenvector that minimizes the error (between the data and the eigenvector)</li>\n<li>Therefore, PCA will find the eigenvector that maximizes the variance by finding the eigenvector that minimizes the variance</li>\n<li>The eigenvalues associated with the eigenvectors are represented as a diagonal covariance matrix</li>\n<li>Therefore, the covariance between each principal component is 0 and thus uncorrelated with each other, since this matrix is a diagonal matrix</li>\n<li>The sum of the eigenvalues should sum up to the total amount of variance for each variable</li>\n<li>Therefore, each eigenvalue represents the fraction (of that total variance made up by the original variables) that the eigenvector makes up</li>\n</ul>\n<h3>Interpreting the Principal Components</h3>\n<ul>\n<li>Principal components are ordered based on how much variance they make up in the data</li>\n<li>\n<p>The first principal component has the following properties:</p>\n<ul>\n<li>The first principal component is the eigenvector of the covariance matrix that makes up the most variance</li>\n</ul>\n</li>\n<li>\n<p>The second principal component has the following properties:</p>\n<ul>\n<li>The second principal component is the eigenvector orthogonal to the first component</li>\n<li>The second principal component is the eigenvector that makes up the second most variance</li>\n</ul>\n</li>\n<li>\n<p>The third principal component has the following properties:</p>\n<ul>\n<li>The third principal component is the eigenvector orthogonal to the first and second components</li>\n<li>The third principal component is the eigenvector that makes up the third most variance</li>\n</ul>\n</li>\n<li>Projections on to all principal components are all completely uncorrelated with each other</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://www.stat.cmu.edu/~cshalizi/350/2008/lectures/10/lecture-10.pdf\" target=\"_blank\" rel=\"nofollow\">Principal Components Analysis and Other Data Transformations</a></li>\n<li><a href=\"https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf\" target=\"_blank\" rel=\"nofollow\">Principal Components Analysis Lecture Notes</a></li>\n<li><a href=\"https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component\" target=\"_blank\" rel=\"nofollow\">Eigenvectors of a Covariance Matrix and PCA</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\" target=\"_blank\" rel=\"nofollow\">Making Sense of PCA with Eigenvectors and Eigenvalues</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another\" target=\"_blank\" rel=\"nofollow\">Difference between Loadings and Eigenvectors</a></li>\n<li><a href=\"https://stats.stackexchange.com/questions/290750/linearity-of-pca\" target=\"_blank\" rel=\"nofollow\">Linearity of PCA</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" target=\"_blank\" rel=\"nofollow\">Principal Components Analysis Wiki</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis_(PCA)\" target=\"_blank\" rel=\"nofollow\">Dimensionality Reduction Wiki</a></li>\n<li><a href=\"https://shankarmsy.github.io/posts/pca-vs-lr.html\" target=\"_blank\" rel=\"nofollow\">Principal Components Analysis and Linear Regression</a></li>\n</ul>"}},"pageContext":{"slug":"ml/unsupervised_learning/pca","previousSlug":"ml/unsupervised_learning/dbscan","nextSlug":"ml/unsupervised_learning/tsne","previousTitle":"DBSCAN","nextTitle":"t-SNE"}},"staticQueryHashes":[]}