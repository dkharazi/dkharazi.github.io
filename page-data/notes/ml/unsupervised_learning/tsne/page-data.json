{"componentChunkName":"component---src-templates-entry-js","path":"/notes/ml/unsupervised_learning/tsne","result":{"data":{"markdownRemark":{"frontmatter":{"title":"t-SNE"},"html":"<h3>Describing t-SNE</h3>\n<ul>\n<li>T-distributed Stochastic Neighbor Embedding (or t-SNE) is a non-linear dimensionality reduction algorithm</li>\n<li>Specifically, t-sne maps high-dimensional data to a lower-dimensional space, where similar data points are close together and dissimilar data are distant from each other</li>\n<li>T-sne involves non-linear transformations for its dimensionality reduction, as apposed to PCA (which involves linear transformations)</li>\n<li>Specifically, T-sne uses the local relationships between points to create a low-dimensional mapping, which allows it to capture non-linear relationships</li>\n<li>Within its dimensionality reduction step, t-sne creates a probability distribution using the Gaussian distribution that defines the relationships between the points in high-dimensional space</li>\n<li>\n<p>Then, t-sne tries to recreate the probability distribution in a low-dimensional space using a t-distribution</p>\n<ul>\n<li>This prevents the crowding problem, where points tend to get crowded in low-dimensional space due to the curse of dimensionality</li>\n</ul>\n</li>\n<li>\n<p>T-sne optimizes the embeddings directly using gradient descent</p>\n<ul>\n<li>The cost function is non-convex however, meaning there is the risk of getting stuck in a local minima</li>\n<li>However, t-sne uses multiple tricks to try to avoid this problem</li>\n</ul>\n</li>\n</ul>\n<h3>Why Do We Need t-SNE?</h3>\n<ul>\n<li>Although PCA is fast, simple, and intuitive, it can't capture non-linear dependencies</li>\n<li>T-sne can capture non-linear dependencies</li>\n<li>Although Kernel PCA and Isomap can be used to capture non-linear dependencies as well, these algorithms don't handle the crowding problem as well as t-sne</li>\n<li>The crowding problem typically happens when higher-dimensional data is mapped to lower-dimensional data, where there is generally a lot less room in the lower dimensional space in comparison</li>\n<li>So, all the points get squashed in the lower dimension, causing crowding</li>\n<li>T-sne deals with this by making the optimization spead out using a t-distribution</li>\n<li>Another major benefit of t-sne is that it uses stochastic neighbors, meaning there is no clear line between which points are neighbors of the other points</li>\n<li>This lack of clear borders can be a major advantage because it allows t-sne to naturally take both global and local structure into account</li>\n<li>This is also achieved using the t-distribution</li>\n</ul>\n<h3>The General t-SNE Algorithm</h3>\n<ol>\n<li>In the high-dimensional space, we create a Gaussian probability distribution for each point that dictates the relationships between neighboring points</li>\n<li>Then, we try to recreate a low dimensional space that follows that probability distribution for each point as best as possible</li>\n<li>The <em>t</em> in t-sne comes from the algorithm's use of the t-distribution in the second step</li>\n<li>The <em>s</em> and <em>n</em> (i.e. stochastic and neighbor) come from the use of probability distributions across neghboring points in both steps</li>\n</ol>\n<h3>Dissecting the T-SNE Algorithm</h3>\n<ol>\n<li>\n<p>Calculate the scaled similarities from a normal curve for the higher-dimensional data</p>\n<ol>\n<li>\n<p>For each point, calculate the distances between every other point</p>\n<ul>\n<li>When calculating the distances, we can use whatever similarity metric that is relevant to our problem</li>\n</ul>\n</li>\n<li>\n<p>Calculate the density of each of those distances on a normal curve</p>\n<ul>\n<li>Said another way, we plot each distance on a normal curve and then calculate its densities</li>\n<li>Specifically, this normal distribution (of distances for each point) has a mean of 0 (i.e. centered around the point of interest) and a variance that is the sample variance of the distances for that point</li>\n<li>Therefore, each variance will differ for each distribution (for each point) depending on the distances from that point and other points</li>\n<li>These densities can be thought of as our unscaled similarities</li>\n</ul>\n</li>\n<li>\n<p>Scale those unscaled similarities to account for differing cluster densities</p>\n<ul>\n<li>We do this to account for the differences in densities for each cluster</li>\n<li>We typically don't care too much about how dense each point is to each other in the cluster</li>\n<li>If we don't scale the distances, then the problem mentioned above will impact the densities of the curve</li>\n<li>So we want to make sure each point is on the same scale as the others</li>\n<li>Specifically, we scale the distances using the following formula:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><mrow><mo>∑</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>e</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{score_{i}}{\\sum score_{i}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.04357em;vertical-align:-0.93601em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1075599999999999em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93601em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<ul>\n<li>Therefore, the scaled similarities should add up to 1</li>\n</ul>\n</li>\n</ol>\n</li>\n<li>\n<p>Calculate the scaled similarities from a t-distribution for the lower-dimensional data</p>\n<ol>\n<li>\n<p>Randomly project the data on a lower-dimensional space</p>\n<ul>\n<li>We do this by randomly projecting the data onto the lower-dimensional space</li>\n</ul>\n</li>\n<li>\n<p>For each point, calculate the distances between every other point</p>\n<ul>\n<li>When calculating the distances, we can use whatever similarity metric that is relevant to our problem</li>\n</ul>\n</li>\n<li>\n<p>Calculate the density of each of those distances on a t-distribution</p>\n<ul>\n<li>Said another way, we plot each distance on a t-distribution and then calculate its densities</li>\n<li>Specifically, this t-distribution (of distances for each point) has a mean of 0 (i.e. centered around the point of interest) and a variance that is the sample variance of the distances for that point</li>\n<li>Therefore, each variance will differ for each distribution (for each point) depending on the distances from that point and other points</li>\n<li>These densities can be thought of as our unscaled similarities</li>\n<li>Specifically, the t-distribution solves the issue of crowding, which refers to data becoming clumped up together in the middle once we move data from a higher dimensional space to a lower dimensional space</li>\n<li>Essentially, crowding happens because we have less space to represent the data</li>\n</ul>\n</li>\n<li>\n<p>Scale those unscaled similarities to account for differing cluster densities</p>\n<ul>\n<li>We do this to account for the differences in densities for each cluster</li>\n<li>We typically don't care too much about how dense each point is to each other in the cluster</li>\n<li>If we don't scale the distances, then the problem mentioned above will impact the densities of the curve</li>\n<li>So we want to make sure each point is on the same scale as the others</li>\n<li>Specifically, we scale the distances using the following formula:</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mfrac><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><mrow><mo>∑</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>e</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{score_{i}}{\\sum score_{i}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.04357em;vertical-align:-0.93601em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.1075599999999999em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:-0.0000050000000000050004em;\">∑</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">s</span><span class=\"mord mathnormal\">c</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord\"><span class=\"mord mathnormal\">e</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93601em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<ul>\n<li>Therefore, the scaled similarities should add up to 1</li>\n</ul>\n</li>\n<li>\n<p>Adjust the t-sne distribution of scaled similarities to match the normal distribution of scaled similarities for each point</p>\n<ul>\n<li>We do this iteratively using gradient descent</li>\n<li>Intuitively, the gradient represents the strength and direction of attraction or reupultion between two points</li>\n<li>A positive gradient represents attraction, whereas a negative gradient represents a repulsion between the points</li>\n<li>This push-and-pull eventually makes the points settle down in the low-dimensional space and match the normal distribution of distances in the higher-dimensional space</li>\n</ul>\n</li>\n</ol>\n</li>\n</ol>\n<h3>More on the Gradient Descent Optimization</h3>\n<ul>\n<li>\n<p>An important consequence of the gradient descent optimization mentioned above is that it does not define a function to reduce dimensionality (unlike PCA)</p>\n<ul>\n<li>T-sne has no parameters and directly optimizes the embeddings itself</li>\n</ul>\n</li>\n<li>Therefore, we can train a t-sne model on some data, but we can't use that model to reduce the dimensionality of some new data point</li>\n<li>Thus, we would need to start the process all over again for a new data point</li>\n<li>\n<p>The gradient descent method used in t-sne uses the two following tricks:</p>\n<ol>\n<li>\n<p>Early compression</p>\n<ul>\n<li>This trick is used to prevent getting stuck in a local minima early on</li>\n<li>Gradient descent allows the points to move around freely</li>\n<li>So, there is a chance that unwanted cluster will form prematurely</li>\n<li>To prevent this, t-sne uses the early compression trick, which involves simply adding an L2 penalty to the cost function at the early stages of optimization</li>\n<li>This will make the distance between embedded points small early on</li>\n<li>The strength of this optimization is a hyperparameter, but t-sne performs fairly robustly regardless of how you set it</li>\n</ul>\n</li>\n<li>\n<p>Early exaggeration</p>\n<ul>\n<li>This trick is used to accelerate the pushing and pulling of points to their respective cluster early on</li>\n<li>Each scaled similarity is multiplied (i.e. exaggerated) at the early stages of optimization</li>\n<li>The effect of this is to force the values of the similarieis to become more focused on larger similarities (i.e. closer points)</li>\n<li>This makes early cluster more tightly knit, allowing them to move around more easily without getting in each others way</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n<h3>References</h3>\n<ul>\n<li><a href=\"https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne\" target=\"_blank\" rel=\"nofollow\">When to use t-SNE?</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=NEaUSP4YerM\" target=\"_blank\" rel=\"nofollow\">t-SNE StatQuest Video</a></li>\n<li><a href=\"https://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/\" target=\"_blank\" rel=\"nofollow\">Visualizing t-SNE</a></li>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\" target=\"_blank\" rel=\"nofollow\">Example of t-SNE Implementation</a></li>\n<li><a href=\"https://lvdmaaten.github.io/tsne/\" target=\"_blank\" rel=\"nofollow\">t-SNE Algorithm</a></li>\n<li><a href=\"https://medium.com/@layog/i-do-not-understand-t-sne-part-2-b2f997d177e3\" target=\"_blank\" rel=\"nofollow\">General Description of the t-SNE Algorithm</a></li>\n</ul>"}},"pageContext":{"slug":"ml/unsupervised_learning/tsne","previousSlug":"ml/unsupervised_learning/pca","nextSlug":"ml/unsupervised_learning/umap","previousTitle":"Principle Component Analysis","nextTitle":"UMAP"}},"staticQueryHashes":[]}